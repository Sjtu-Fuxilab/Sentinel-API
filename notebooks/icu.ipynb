{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn xgboost shap matplotlib seaborn plotly optuna imbalanced-learn duckdb pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-ICU Project Setup\n",
    "# Run this in your Jupyter notebook at D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\icu.ipynb\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Install packages (Choose ONE method)\n",
    "# ============================================\n",
    "\n",
    "# Method A: If running in Jupyter notebook (use this)\n",
    "# Uncomment these lines if packages aren't installed:\n",
    "\"\"\"\n",
    "!pip install pandas numpy scikit-learn xgboost shap matplotlib seaborn plotly\n",
    "!pip install optuna imbalanced-learn\n",
    "!pip install duckdb pyarrow tqdm\n",
    "\"\"\"\n",
    "\n",
    "# Method B: If packages still won't install, run this in Command Prompt:\n",
    "# pip install pandas numpy scikit-learn xgboost shap matplotlib seaborn plotly optuna imbalanced-learn duckdb pyarrow tqdm\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Import Libraries\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Set Data Paths\n",
    "# ============================================\n",
    "MIMIC_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\mimic'\n",
    "EICU_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\eicu'\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = [\n",
    "    'outputs',\n",
    "    'outputs/models',\n",
    "    'outputs/figures', \n",
    "    'outputs/features',\n",
    "    'outputs/cohorts'\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"MIMIC-IV path: {MIMIC_PATH}\")\n",
    "print(f\"eICU path: {EICU_PATH}\")\n",
    "print(\"Output directories created!\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Test Data Access (handles .csv and .csv.gz)\n",
    "# ============================================\n",
    "def find_file(base_path, filename):\n",
    "    \"\"\"Check for both .csv and .csv.gz versions\"\"\"\n",
    "    csv_path = os.path.join(base_path, f\"{filename}.csv\")\n",
    "    gz_path = os.path.join(base_path, f\"{filename}.csv.gz\")\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        return csv_path\n",
    "    elif os.path.exists(gz_path):\n",
    "        return gz_path\n",
    "    return None\n",
    "\n",
    "# Test MIMIC-IV critical files for MI cohort\n",
    "print(\"Checking MIMIC-IV files needed for MI cohort:\")\n",
    "mimic_required = {\n",
    "    'icustays': os.path.join(MIMIC_PATH, 'icu'),\n",
    "    'admissions': os.path.join(MIMIC_PATH, 'hosp'),\n",
    "    'diagnoses_icd': os.path.join(MIMIC_PATH, 'hosp')\n",
    "}\n",
    "\n",
    "mimic_files_found = {}\n",
    "for file_name, base_path in mimic_required.items():\n",
    "    file_path = find_file(base_path, file_name)\n",
    "    if file_path:\n",
    "        print(f\"  âœ“ {file_name}: {os.path.basename(file_path)}\")\n",
    "        # Quick test read\n",
    "        test_df = pd.read_csv(file_path, nrows=5, compression='gzip' if file_path.endswith('.gz') else None)\n",
    "        print(f\"    - {len(test_df.columns)} columns, sample cols: {list(test_df.columns[:3])}\")\n",
    "        mimic_files_found[file_name] = file_path\n",
    "    else:\n",
    "        print(f\"  âœ— {file_name} not found in {base_path}\")\n",
    "\n",
    "# Test eICU critical files for MI cohort\n",
    "print(\"\\nChecking eICU files needed for MI cohort:\")\n",
    "eicu_required = {\n",
    "    'patient': EICU_PATH,\n",
    "    'diagnosis': EICU_PATH\n",
    "}\n",
    "\n",
    "eicu_files_found = {}\n",
    "for file_name, base_path in eicu_required.items():\n",
    "    file_path = find_file(base_path, file_name)\n",
    "    if file_path:\n",
    "        print(f\"  âœ“ {file_name}: {os.path.basename(file_path)}\")\n",
    "        # Quick test read\n",
    "        test_df = pd.read_csv(file_path, nrows=5, compression='gzip' if file_path.endswith('.gz') else None)\n",
    "        print(f\"    - {len(test_df.columns)} columns, sample cols: {list(test_df.columns[:3])}\")\n",
    "        eicu_files_found[file_name] = file_path\n",
    "    else:\n",
    "        print(f\"  âœ— {file_name} not found in {base_path}\")\n",
    "\n",
    "# Store file paths for later use\n",
    "FILE_PATHS = {\n",
    "    'mimic': mimic_files_found,\n",
    "    'eicu': eicu_files_found\n",
    "}\n",
    "\n",
    "if len(mimic_files_found) == 3 and len(eicu_files_found) == 2:\n",
    "    print(\"\\nâœ… All required files found! Ready to proceed with cohort extraction.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some required files missing. Check paths and file extensions (.csv or .csv.gz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COHORT EXTRACTION FROM MIMIC-IV AND eICU\n",
    "# For Sentinel-ICU Project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths from your setup\n",
    "MIMIC_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\mimic'\n",
    "EICU_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\eicu'\n",
    "\n",
    "# ============================================\n",
    "# PART 1: MIMIC-IV MI Cohort Extraction\n",
    "# ============================================\n",
    "print(\"=\"*50)\n",
    "print(\"EXTRACTING MIMIC-IV MI COHORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load MIMIC-IV files needed for MI identification\n",
    "print(\"Loading MIMIC-IV data files...\")\n",
    "\n",
    "# 1. Load diagnoses to find MI patients\n",
    "print(\"  Loading diagnoses_icd...\")\n",
    "mimic_diagnoses = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'diagnoses_icd.csv.gz'),\n",
    "    compression='gzip',\n",
    "    dtype={'icd_code': str, 'icd_version': str}\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_diagnoses):,} diagnosis records\")\n",
    "\n",
    "# 2. Load admissions for mortality outcome\n",
    "print(\"  Loading admissions...\")\n",
    "mimic_admissions = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'admissions.csv.gz'),\n",
    "    compression='gzip',\n",
    "    parse_dates=['admittime', 'dischtime', 'deathtime']\n",
    ")\n",
    "# Create hospital_expire_flag from deathtime (MIMIC-IV doesn't have this flag directly)\n",
    "mimic_admissions['hospital_expire_flag'] = mimic_admissions['deathtime'].notna().astype(int)\n",
    "print(f\"    Loaded {len(mimic_admissions):,} admissions\")\n",
    "\n",
    "# 3. Load ICU stays\n",
    "print(\"  Loading icustays...\")\n",
    "mimic_icustays = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'icu', 'icustays.csv.gz'),\n",
    "    compression='gzip',\n",
    "    parse_dates=['intime', 'outtime']\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_icustays):,} ICU stays\")\n",
    "\n",
    "# 4. Load patients for age calculation\n",
    "print(\"  Loading patients...\")\n",
    "mimic_patients = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'patients.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_patients):,} patients\")\n",
    "\n",
    "# Identify diagnoses\n",
    "print(\"\\nIdentifying patients by ICD codes...\")\n",
    "\n",
    "# ICD-10 codes\n",
    "mi_icd10 = mimic_diagnoses[\n",
    "    (mimic_diagnoses['icd_version'] == '10') & \n",
    "    (mimic_diagnoses['icd_code'].str.match(r'^(I21|I22)', na=False))\n",
    "].copy()\n",
    "print(f\"  ICD-10 codes (I21.*, I22.*): {len(mi_icd10):,} diagnoses\")\n",
    "\n",
    "# ICD-9 codes\n",
    "mi_icd9 = mimic_diagnoses[\n",
    "    (mimic_diagnoses['icd_version'] == '9') & \n",
    "    (mimic_diagnoses['icd_code'].str.match(r'^410', na=False))\n",
    "].copy()\n",
    "print(f\"  ICD-9 codes (410.*): {len(mi_icd9):,} diagnoses\")\n",
    "\n",
    "# Combine diagnoses\n",
    "all_mi_diagnoses = pd.concat([mi_icd10, mi_icd9], ignore_index=True)\n",
    "mi_hadm_ids = all_mi_diagnoses['hadm_id'].unique()\n",
    "print(f\"  Total unique hospital admissions: {len(mi_hadm_ids):,}\")\n",
    "\n",
    "# Get ICU stays for patients\n",
    "mi_icu_stays = mimic_icustays[mimic_icustays['hadm_id'].isin(mi_hadm_ids)].copy()\n",
    "print(f\" patients with ICU stays: {len(mi_icu_stays):,}\")\n",
    "\n",
    "# Merge with admission data for outcomes\n",
    "mi_icu_stays = mi_icu_stays.merge(\n",
    "    mimic_admissions[['hadm_id', 'hospital_expire_flag', 'deathtime']],\n",
    "    on='hadm_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with patient data for demographics\n",
    "mi_icu_stays = mi_icu_stays.merge(\n",
    "    mimic_patients[['subject_id', 'anchor_age', 'anchor_year', 'gender']],\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "mi_icu_stays['age'] = mi_icu_stays['anchor_age'] + (\n",
    "    mi_icu_stays['intime'].dt.year - mi_icu_stays['anchor_year']\n",
    ")\n",
    "\n",
    "# Calculate ICU length of stay in hours\n",
    "mi_icu_stays['icu_los_hours'] = (\n",
    "    mi_icu_stays['outtime'] - mi_icu_stays['intime']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Apply inclusion/exclusion criteria\n",
    "print(\"\\nApplying inclusion criteria...\")\n",
    "print(f\"  Starting cohort: {len(mi_icu_stays):,} ICU stays\")\n",
    "\n",
    "# Filters\n",
    "mimic_cohort = mi_icu_stays[\n",
    "    (mi_icu_stays['age'] >= 18) &  # Adult patients\n",
    "    (mi_icu_stays['icu_los_hours'] >= 6) &  # Min 6 hours in ICU\n",
    "    (mi_icu_stays['icu_los_hours'] <= 720)  # Max 30 days (exclude outliers)\n",
    "].copy()\n",
    "\n",
    "# Keep only first ICU stay per admission\n",
    "mimic_cohort = mimic_cohort.sort_values('intime').groupby('hadm_id').first().reset_index()\n",
    "\n",
    "print(f\"  Final MIMIC cohort: {len(mimic_cohort):,} patients\")\n",
    "print(f\"  In-hospital mortality: {mimic_cohort['hospital_expire_flag'].sum():,} ({mimic_cohort['hospital_expire_flag'].mean():.1%})\")\n",
    "\n",
    "# Select key columns for saving\n",
    "mimic_cohort_final = mimic_cohort[['subject_id', 'hadm_id', 'stay_id', 'age', 'gender',\n",
    "                                     'intime', 'outtime', 'icu_los_hours', \n",
    "                                     'hospital_expire_flag', 'deathtime']].copy()\n",
    "\n",
    "# ============================================\n",
    "# PART 2: eICU MI Cohort Extraction  \n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTRACTING eICU MI COHORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load eICU files\n",
    "print(\"Loading eICU data files...\")\n",
    "\n",
    "# 1. Load patient table\n",
    "print(\"  Loading patient table...\")\n",
    "eicu_patient = pd.read_csv(\n",
    "    os.path.join(EICU_PATH, 'patient.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(eicu_patient):,} patients\")\n",
    "\n",
    "# 2. Load diagnosis table\n",
    "print(\"  Loading diagnosis table...\")\n",
    "eicu_diagnosis = pd.read_csv(\n",
    "    os.path.join(EICU_PATH, 'diagnosis.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(eicu_diagnosis):,} diagnoses\")\n",
    "\n",
    "# Identify MI patients in eICU\n",
    "print(\"\\nIdentifying MI patients by diagnosis strings...\")\n",
    "\n",
    "# Related search terms\n",
    "mi_keywords = [\n",
    "    'myocardial infarction',\n",
    "    'STEMI',\n",
    "    'NSTEMI', \n",
    "    'ST elevation MI',\n",
    "    'non-ST elevation MI',\n",
    "    'acute MI'\n",
    "]\n",
    "\n",
    "# Create search pattern\n",
    "pattern = '|'.join(mi_keywords)\n",
    "mi_diagnoses_eicu = eicu_diagnosis[\n",
    "    eicu_diagnosis['diagnosisstring'].str.contains(pattern, case=False, na=False)\n",
    "]\n",
    "\n",
    "# Exclude historical (not acute cases)\n",
    "exclude_pattern = r'history of|old myocardial infarction|old mi|prior mi|previous mi'\n",
    "mi_diagnoses_eicu = mi_diagnoses_eicu[\n",
    "    ~mi_diagnoses_eicu['diagnosisstring'].str.contains(exclude_pattern, case=False, na=False)\n",
    "]\n",
    "\n",
    "mi_patient_ids = mi_diagnoses_eicu['patientunitstayid'].unique()\n",
    "print(f\"  Found {len(mi_patient_ids):,} acute patients (after excluding historical)\")\n",
    "\n",
    "# Get patient data for patients\n",
    "eicu_cohort = eicu_patient[eicu_patient['patientunitstayid'].isin(mi_patient_ids)].copy()\n",
    "\n",
    "# Filter out invalid discharge offsets (negative or missing)\n",
    "print(\"  Filtering invalid discharge offsets...\")\n",
    "before_filter = len(eicu_cohort)\n",
    "eicu_cohort = eicu_cohort[\n",
    "    eicu_cohort['unitdischargeoffset'].notna() & \n",
    "    (eicu_cohort['unitdischargeoffset'] >= 0)\n",
    "]\n",
    "print(f\"    Removed {before_filter - len(eicu_cohort):,} patients with invalid discharge times\")\n",
    "\n",
    "# Process age (handle \"> 89\" encoding and possible numeric values)\n",
    "eicu_cohort['age_numeric'] = pd.to_numeric(\n",
    "    eicu_cohort['age'].astype(str).str.replace('> 89', '90'),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Calculate ICU LOS in hours (from minutes)\n",
    "eicu_cohort['icu_los_hours'] = eicu_cohort['unitdischargeoffset'] / 60\n",
    "\n",
    "# Create mortality flag\n",
    "eicu_cohort['hospital_expire_flag'] = (\n",
    "    eicu_cohort['hospitaldischargestatus'] == 'Expired'\n",
    ").astype(int)\n",
    "\n",
    "# Apply inclusion criteria\n",
    "print(\"\\nApplying inclusion criteria...\")\n",
    "print(f\"  Starting cohort: {len(eicu_cohort):,} patients\")\n",
    "\n",
    "eicu_cohort_final = eicu_cohort[\n",
    "    (eicu_cohort['age_numeric'] >= 18) &  # Adult patients\n",
    "    (eicu_cohort['icu_los_hours'] >= 6) &  # Min 6 hours\n",
    "    (eicu_cohort['icu_los_hours'] <= 720)  # Max 30 days\n",
    "].copy()\n",
    "\n",
    "print(f\"  Final eICU cohort: {len(eicu_cohort_final):,} patients\")\n",
    "print(f\"  In-hospital mortality: {eicu_cohort_final['hospital_expire_flag'].sum():,} ({eicu_cohort_final['hospital_expire_flag'].mean():.1%})\")\n",
    "\n",
    "# Select key columns (use only available columns)\n",
    "eicu_columns_to_keep = ['patientunitstayid', 'patienthealthsystemstayid',\n",
    "                         'age_numeric', 'gender', 'icu_los_hours', 'hospital_expire_flag']\n",
    "\n",
    "# Add optional columns if they exist\n",
    "if 'ethnicity' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('ethnicity')\n",
    "if 'unitadmittime' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('unitadmittime')\n",
    "if 'unitdischargetime' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('unitdischargetime')\n",
    "\n",
    "eicu_cohort_final = eicu_cohort_final[eicu_columns_to_keep].copy()\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Save Cohorts and Generate Summary\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING COHORTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs('outputs/cohorts', exist_ok=True)\n",
    "\n",
    "# Save MIMIC cohort\n",
    "mimic_path = 'outputs/cohorts/mimic_mi_cohort.csv'\n",
    "mimic_cohort_final.to_csv(mimic_path, index=False)\n",
    "print(f\"âœ“ MIMIC cohort saved to: {mimic_path}\")\n",
    "\n",
    "# Save eICU cohort  \n",
    "eicu_path = 'outputs/cohorts/eicu_mi_cohort.csv'\n",
    "eicu_cohort_final.to_csv(eicu_path, index=False)\n",
    "print(f\"âœ“ eICU cohort saved to: {eicu_path}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 4: Summary Statistics\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COHORT SUMMARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ“Š MIMIC-IV MI Cohort:\")\n",
    "print(f\"  N = {len(mimic_cohort_final):,}\")\n",
    "print(f\"  Age = {mimic_cohort_final['age'].mean():.1f} Â± {mimic_cohort_final['age'].std():.1f} years\")\n",
    "print(f\"  Female = {(mimic_cohort_final['gender']=='F').sum():,} ({(mimic_cohort_final['gender']=='F').mean():.1%})\")\n",
    "print(f\"  ICU LOS = {mimic_cohort_final['icu_los_hours'].median():.1f} hours (IQR: {mimic_cohort_final['icu_los_hours'].quantile(0.25):.1f}-{mimic_cohort_final['icu_los_hours'].quantile(0.75):.1f})\")\n",
    "print(f\"  Mortality = {mimic_cohort_final['hospital_expire_flag'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nðŸ“Š eICU MI Cohort:\")\n",
    "print(f\"  N = {len(eicu_cohort_final):,}\")\n",
    "print(f\"  Age = {eicu_cohort_final['age_numeric'].mean():.1f} Â± {eicu_cohort_final['age_numeric'].std():.1f} years\")\n",
    "print(f\"  Female = {(eicu_cohort_final['gender']=='Female').sum():,} ({(eicu_cohort_final['gender']=='Female').mean():.1%})\")\n",
    "print(f\"  ICU LOS = {eicu_cohort_final['icu_los_hours'].median():.1f} hours (IQR: {eicu_cohort_final['icu_los_hours'].quantile(0.25):.1f}-{eicu_cohort_final['icu_los_hours'].quantile(0.75):.1f})\")\n",
    "print(f\"  Mortality = {eicu_cohort_final['hospital_expire_flag'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nâœ… cohort extraction complete!\")\n",
    "print(f\"Next step: Extract temporal features (vitals, labs, interventions) for these {len(mimic_cohort_final) + len(eicu_cohort_final):,} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COHORT EXTRACTION FROM MIMIC-IV AND eICU\n",
    "# For ICU Project - Correct Version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths from your setup\n",
    "MIMIC_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\mimic'\n",
    "EICU_PATH = r'D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\eicu'\n",
    "\n",
    "# ============================================\n",
    "# PART 1: MIMIC-IV MI Cohort Extraction\n",
    "# ============================================\n",
    "print(\"=\"*50)\n",
    "print(\"EXTRACTING MIMIC-IV MI COHORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load MIMIC-IV files needed for MI identification\n",
    "print(\"Loading MIMIC-IV data files...\")\n",
    "\n",
    "# 1. Load diagnoses to find MI patients\n",
    "print(\"  Loading diagnoses_icd...\")\n",
    "mimic_diagnoses = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'diagnoses_icd.csv.gz'),\n",
    "    compression='gzip',\n",
    "    dtype={'icd_code': str, 'icd_version': str}\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_diagnoses):,} diagnosis records\")\n",
    "\n",
    "# 2. Load admissions for mortality outcome\n",
    "print(\"  Loading admissions...\")\n",
    "mimic_admissions = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'admissions.csv.gz'),\n",
    "    compression='gzip',\n",
    "    parse_dates=['admittime', 'dischtime', 'deathtime']\n",
    ")\n",
    "# Create hospital_expire_flag from deathtime (MIMIC-IV doesn't have this flag directly)\n",
    "mimic_admissions['hospital_expire_flag'] = mimic_admissions['deathtime'].notna().astype(int)\n",
    "print(f\"    Loaded {len(mimic_admissions):,} admissions\")\n",
    "\n",
    "# 3. Load ICU stays\n",
    "print(\"  Loading icustays...\")\n",
    "mimic_icustays = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'icu', 'icustays.csv.gz'),\n",
    "    compression='gzip',\n",
    "    parse_dates=['intime', 'outtime']\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_icustays):,} ICU stays\")\n",
    "\n",
    "# 4. Load patients for age calculation\n",
    "print(\"  Loading patients...\")\n",
    "mimic_patients = pd.read_csv(\n",
    "    os.path.join(MIMIC_PATH, 'hosp', 'patients.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(mimic_patients):,} patients\")\n",
    "\n",
    "# Identify MI diagnoses\n",
    "print(\"\\nIdentifying MI patients by ICD codes...\")\n",
    "\n",
    "# Filter for PRIMARY diagnoses only (seq_num = 1) as specified in proposal\n",
    "primary_diagnoses = mimic_diagnoses[mimic_diagnoses['seq_num'] == 1].copy()\n",
    "print(f\"  Focusing on primary diagnoses only (seq_num = 1)\")\n",
    "\n",
    "# ICD-10 codes for MI\n",
    "mi_icd10 = primary_diagnoses[\n",
    "    (primary_diagnoses['icd_version'] == '10') & \n",
    "    (primary_diagnoses['icd_code'].str.match(r'^(I21|I22)', na=False))\n",
    "].copy()\n",
    "print(f\"  ICD-10 MI codes (I21.*, I22.*): {len(mi_icd10):,} primary diagnoses\")\n",
    "\n",
    "# ICD-9 codes for MI\n",
    "mi_icd9 = primary_diagnoses[\n",
    "    (primary_diagnoses['icd_version'] == '9') & \n",
    "    (primary_diagnoses['icd_code'].str.match(r'^410', na=False))\n",
    "].copy()\n",
    "print(f\"  ICD-9 MI codes (410.*): {len(mi_icd9):,} primary diagnoses\")\n",
    "\n",
    "# Combine MI diagnoses\n",
    "all_mi_diagnoses = pd.concat([mi_icd10, mi_icd9], ignore_index=True)\n",
    "mi_hadm_ids = all_mi_diagnoses['hadm_id'].unique()\n",
    "print(f\"  Total unique MI hospital admissions: {len(mi_hadm_ids):,}\")\n",
    "\n",
    "# Get ICU stays for MI patients\n",
    "mi_icu_stays = mimic_icustays[mimic_icustays['hadm_id'].isin(mi_hadm_ids)].copy()\n",
    "print(f\"  MI patients with ICU stays: {len(mi_icu_stays):,}\")\n",
    "\n",
    "# Merge with admission data for outcomes\n",
    "mi_icu_stays = mi_icu_stays.merge(\n",
    "    mimic_admissions[['hadm_id', 'hospital_expire_flag', 'deathtime']],\n",
    "    on='hadm_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with patient data for demographics\n",
    "mi_icu_stays = mi_icu_stays.merge(\n",
    "    mimic_patients[['subject_id', 'anchor_age', 'anchor_year', 'gender']],\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "mi_icu_stays['age'] = mi_icu_stays['anchor_age'] + (\n",
    "    mi_icu_stays['intime'].dt.year - mi_icu_stays['anchor_year']\n",
    ")\n",
    "\n",
    "# Clip implausible ages (handle de-identification edge cases)\n",
    "mi_icu_stays['age'] = mi_icu_stays['age'].clip(0, 115)\n",
    "\n",
    "# Calculate ICU length of stay in hours\n",
    "mi_icu_stays['icu_los_hours'] = (\n",
    "    mi_icu_stays['outtime'] - mi_icu_stays['intime']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Apply inclusion/exclusion criteria\n",
    "print(\"\\nApplying inclusion criteria...\")\n",
    "print(f\"  Starting cohort: {len(mi_icu_stays):,} ICU stays\")\n",
    "\n",
    "# Filters\n",
    "mimic_cohort = mi_icu_stays[\n",
    "    (mi_icu_stays['age'] >= 18) &  # Adult patients\n",
    "    (mi_icu_stays['icu_los_hours'] >= 6) &  # Min 6 hours in ICU\n",
    "    (mi_icu_stays['icu_los_hours'] <= 720)  # Max 30 days (exclude outliers)\n",
    "].copy()\n",
    "\n",
    "# Keep only first ICU stay per admission\n",
    "mimic_cohort = mimic_cohort.sort_values('intime').groupby('hadm_id').first().reset_index()\n",
    "\n",
    "# Additional sanity check: remove any stays with negative or zero LOS\n",
    "pre_filter = len(mimic_cohort)\n",
    "mimic_cohort = mimic_cohort[mimic_cohort['icu_los_hours'] > 0]\n",
    "if pre_filter > len(mimic_cohort):\n",
    "    print(f\"  Removed {pre_filter - len(mimic_cohort):,} stays with invalid LOS\")\n",
    "\n",
    "print(f\"  Final MIMIC cohort: {len(mimic_cohort):,} patients\")\n",
    "print(f\"  In-hospital mortality: {mimic_cohort['hospital_expire_flag'].sum():,} ({mimic_cohort['hospital_expire_flag'].mean():.1%})\")\n",
    "print(f\"  Note: Mortality defined as deathtime during hospitalization\")\n",
    "\n",
    "# Select key columns for saving\n",
    "mimic_cohort_final = mimic_cohort[['subject_id', 'hadm_id', 'stay_id', 'age', 'gender',\n",
    "                                     'intime', 'outtime', 'icu_los_hours', \n",
    "                                     'hospital_expire_flag', 'deathtime']].copy()\n",
    "\n",
    "# ============================================\n",
    "# PART 2: eICU MI Cohort Extraction  \n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTRACTING eICU MI COHORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load eICU files\n",
    "print(\"Loading eICU data files...\")\n",
    "\n",
    "# 1. Load patient table\n",
    "print(\"  Loading patient table...\")\n",
    "eicu_patient = pd.read_csv(\n",
    "    os.path.join(EICU_PATH, 'patient.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(eicu_patient):,} patients\")\n",
    "\n",
    "# 2. Load diagnosis table\n",
    "print(\"  Loading diagnosis table...\")\n",
    "eicu_diagnosis = pd.read_csv(\n",
    "    os.path.join(EICU_PATH, 'diagnosis.csv.gz'),\n",
    "    compression='gzip'\n",
    ")\n",
    "print(f\"    Loaded {len(eicu_diagnosis):,} diagnoses\")\n",
    "\n",
    "# Identify MI patients in eICU\n",
    "print(\"\\nIdentifying MI patients by diagnosis strings...\")\n",
    "\n",
    "# MI-related search terms\n",
    "mi_keywords = [\n",
    "    'myocardial infarction',\n",
    "    'STEMI',\n",
    "    'NSTEMI', \n",
    "    'ST elevation MI',\n",
    "    'non-ST elevation MI',\n",
    "    'acute MI'\n",
    "]\n",
    "\n",
    "# Create search pattern\n",
    "pattern = '|'.join(mi_keywords)\n",
    "mi_diagnoses_eicu = eicu_diagnosis[\n",
    "    eicu_diagnosis['diagnosisstring'].str.contains(pattern, case=False, na=False)\n",
    "]\n",
    "\n",
    "# Exclude historical MI and rule-out cases (not acute cases)\n",
    "# More comprehensive exclusion patterns for non-acute MI\n",
    "exclude_pattern = (r'history of|old myocardial infarction|old mi|prior mi|previous mi|'\n",
    "                  r'rule ?out|r/o|doubt|\\?troponin|'\n",
    "                  r'type ?(2|ii) ?(mi|myocardial infarction)|'\n",
    "                  r'demand ischemia|myocardial injury|'\n",
    "                  r'elevated troponin|troponinemia')\n",
    "mi_diagnoses_eicu = mi_diagnoses_eicu[\n",
    "    ~mi_diagnoses_eicu['diagnosisstring'].str.contains(exclude_pattern, case=False, na=False, regex=True)\n",
    "]\n",
    "\n",
    "mi_patient_ids = mi_diagnoses_eicu['patientunitstayid'].unique()\n",
    "print(f\"  Found {len(mi_patient_ids):,} acute MI patients (after excluding historical/rule-out/type 2 MI)\")\n",
    "\n",
    "# Get patient data for MI patients\n",
    "eicu_cohort = eicu_patient[eicu_patient['patientunitstayid'].isin(mi_patient_ids)].copy()\n",
    "\n",
    "# Check for required offset columns\n",
    "if 'unitadmitoffset' not in eicu_cohort.columns or 'unitdischargeoffset' not in eicu_cohort.columns:\n",
    "    print(\"  âš ï¸ Warning: unitadmitoffset or unitdischargeoffset not found, using alternative calculation\")\n",
    "    # Fallback if offsets missing (shouldn't happen in standard eICU)\n",
    "    eicu_cohort['icu_los_hours'] = eicu_cohort.get('unitdischargeoffset', 0) / 60\n",
    "else:\n",
    "    # Calculate ICU LOS in hours (offsets are from hospital admission, in minutes)\n",
    "    eicu_cohort['icu_los_hours'] = (eicu_cohort['unitdischargeoffset'] - eicu_cohort['unitadmitoffset']) / 60\n",
    "\n",
    "# Filter out invalid discharge offsets (negative or missing LOS)\n",
    "print(\"  Filtering invalid ICU stays...\")\n",
    "before_filter = len(eicu_cohort)\n",
    "eicu_cohort = eicu_cohort[\n",
    "    eicu_cohort['icu_los_hours'].notna() & \n",
    "    (eicu_cohort['icu_los_hours'] > 0)\n",
    "]\n",
    "print(f\"    Removed {before_filter - len(eicu_cohort):,} patients with invalid ICU LOS\")\n",
    "\n",
    "# Process age (handle \"> 89\" encoding and possible numeric values)\n",
    "eicu_cohort['age_numeric'] = pd.to_numeric(\n",
    "    eicu_cohort['age'].astype(str).str.replace('> 89', '90'),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Create mortality flag\n",
    "eicu_cohort['hospital_expire_flag'] = (\n",
    "    eicu_cohort['hospitaldischargestatus'] == 'Expired'\n",
    ").astype(int)\n",
    "\n",
    "# Apply inclusion criteria\n",
    "print(\"\\nApplying inclusion criteria...\")\n",
    "print(f\"  Starting cohort: {len(eicu_cohort):,} patients\")\n",
    "\n",
    "# Check for age edge cases\n",
    "age_missing = eicu_cohort['age_numeric'].isna().sum()\n",
    "if age_missing > 0:\n",
    "    print(f\"  Note: {age_missing:,} patients have non-numeric age values (will be excluded)\")\n",
    "\n",
    "eicu_cohort_final = eicu_cohort[\n",
    "    (eicu_cohort['age_numeric'] >= 18) &  # Adult patients (drops NaN ages)\n",
    "    (eicu_cohort['icu_los_hours'] >= 6) &  # Min 6 hours\n",
    "    (eicu_cohort['icu_los_hours'] <= 720) &  # Max 30 days\n",
    "    (eicu_cohort['icu_los_hours'] > 0)  # Sanity check for positive LOS\n",
    "].copy()\n",
    "\n",
    "# Keep only first ICU stay per hospital stay (to mirror MIMIC approach)\n",
    "pre_dedup = len(eicu_cohort_final)\n",
    "\n",
    "# Determine which column to use for sorting\n",
    "sort_col = None\n",
    "if 'unitadmitoffset' in eicu_cohort_final.columns:\n",
    "    sort_col = 'unitadmitoffset'\n",
    "elif 'unitadmittime' in eicu_cohort_final.columns:\n",
    "    # Ensure datetime for proper ordering\n",
    "    eicu_cohort_final['unitadmittime'] = pd.to_datetime(eicu_cohort_final['unitadmittime'], errors='coerce')\n",
    "    sort_col = 'unitadmittime'\n",
    "else:\n",
    "    # Fallback to patient unit stay ID (essentially arbitrary within hospital stay)\n",
    "    sort_col = 'patientunitstayid'\n",
    "    print(f\"  Note: Using patientunitstayid for sorting (no time columns available)\")\n",
    "\n",
    "# Remove duplicates, keeping first ICU stay per hospital admission\n",
    "eicu_cohort_final = (\n",
    "    eicu_cohort_final\n",
    "    .sort_values(sort_col)\n",
    "    .groupby('patienthealthsystemstayid', as_index=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "print(f\"  Removed {pre_dedup - len(eicu_cohort_final):,} duplicate ICU stays within same hospital stay\")\n",
    "\n",
    "print(f\"  Final eICU cohort: {len(eicu_cohort_final):,} patients\")\n",
    "print(f\"  In-hospital mortality: {eicu_cohort_final['hospital_expire_flag'].sum():,} ({eicu_cohort_final['hospital_expire_flag'].mean():.1%})\")\n",
    "print(f\"  Note: Mortality defined as hospitaldischargestatus=='Expired'\")\n",
    "\n",
    "# Select key columns (use only available columns)\n",
    "eicu_columns_to_keep = ['patientunitstayid', 'patienthealthsystemstayid',\n",
    "                         'age_numeric', 'gender', 'icu_los_hours', 'hospital_expire_flag']\n",
    "\n",
    "# Add optional columns if they exist\n",
    "if 'ethnicity' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('ethnicity')\n",
    "if 'unitadmittime' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('unitadmittime')\n",
    "if 'unitdischargetime' in eicu_cohort_final.columns:\n",
    "    eicu_columns_to_keep.append('unitdischargetime')\n",
    "\n",
    "eicu_cohort_final = eicu_cohort_final[eicu_columns_to_keep].copy()\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Save Cohorts and Generate Summary\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING COHORTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs('outputs/cohorts', exist_ok=True)\n",
    "\n",
    "# Save MIMIC cohort\n",
    "mimic_path = 'outputs/cohorts/mimic_mi_cohort.csv'\n",
    "mimic_cohort_final.to_csv(mimic_path, index=False)\n",
    "print(f\"âœ“ MIMIC cohort saved to: {mimic_path}\")\n",
    "\n",
    "# Save eICU cohort  \n",
    "eicu_path = 'outputs/cohorts/eicu_mi_cohort.csv'\n",
    "eicu_cohort_final.to_csv(eicu_path, index=False)\n",
    "print(f\"âœ“ eICU cohort saved to: {eicu_path}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 4: Summary Statistics\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COHORT SUMMARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ“Š MIMIC-IV MI Cohort:\")\n",
    "print(f\"  N = {len(mimic_cohort_final):,}\")\n",
    "print(f\"  Age = {mimic_cohort_final['age'].mean():.1f} Â± {mimic_cohort_final['age'].std():.1f} years\")\n",
    "print(f\"  Female = {(mimic_cohort_final['gender']=='F').sum():,} ({(mimic_cohort_final['gender']=='F').mean():.1%})\")\n",
    "print(f\"  ICU LOS = {mimic_cohort_final['icu_los_hours'].median():.1f} hours (IQR: {mimic_cohort_final['icu_los_hours'].quantile(0.25):.1f}-{mimic_cohort_final['icu_los_hours'].quantile(0.75):.1f})\")\n",
    "print(f\"  Mortality = {mimic_cohort_final['hospital_expire_flag'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nðŸ“Š eICU MI Cohort:\")\n",
    "print(f\"  N = {len(eicu_cohort_final):,}\")\n",
    "print(f\"  Age = {eicu_cohort_final['age_numeric'].mean():.1f} Â± {eicu_cohort_final['age_numeric'].std():.1f} years\")\n",
    "print(f\"  Female = {(eicu_cohort_final['gender']=='Female').sum():,} ({(eicu_cohort_final['gender']=='Female').mean():.1%})\")\n",
    "print(f\"  ICU LOS = {eicu_cohort_final['icu_los_hours'].median():.1f} hours (IQR: {eicu_cohort_final['icu_los_hours'].quantile(0.25):.1f}-{eicu_cohort_final['icu_los_hours'].quantile(0.75):.1f})\")\n",
    "print(f\"  Mortality = {eicu_cohort_final['hospital_expire_flag'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nðŸ“ Outcome Definitions:\")\n",
    "print(\"  - MIMIC: In-hospital mortality (deathtime during hospitalization)\")\n",
    "print(\"  - eICU: In-hospital mortality (hospitaldischargestatus == 'Expired')\")\n",
    "print(\"  - Both datasets: First ICU stay per hospital admission only\")\n",
    "\n",
    "print(\"\\nâœ… MI cohort extraction complete!\")\n",
    "print(f\"Next step: Extract temporal features (vitals, labs, interventions) for these {len(mimic_cohort_final) + len(eicu_cohort_final):,} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOYMENT HELPER FOR Sentinel-ICU MODELS\n",
    "# Production-ready functions to load models and make predictions\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------------------------------\n",
    "# Constants\n",
    "# -------------------------------\n",
    "ALLOWED_WINDOWS = {6, 12, 18, 24}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model dir resolver (portable)\n",
    "# -------------------------------\n",
    "def resolve_model_dir(window: int, default_dir: str = \"outputs/models\") -> str:\n",
    "    \"\"\"\n",
    "    Resolve the directory that actually contains the bundle for a given window.\n",
    "    Priority:\n",
    "      1) XMI_MODEL_DIR env var (if valid)\n",
    "      2) default_dir (if valid)\n",
    "      3) common absolute fallbacks on this machine\n",
    "    \"\"\"\n",
    "    env_dir = os.getenv(\"XMI_MODEL_DIR\")\n",
    "    if env_dir and os.path.isdir(env_dir):\n",
    "        return env_dir\n",
    "\n",
    "    def has_window(d: str) -> bool:\n",
    "        if not os.path.isdir(d):\n",
    "            return False\n",
    "        pkl = os.path.join(d, f\"xgboost_w{window}.pkl\")\n",
    "        jsn = os.path.join(d, f\"xgb_w{window}.json\")\n",
    "        imp = os.path.join(d, f\"imputer_w{window}.pkl\")\n",
    "        return (os.path.exists(pkl) or os.path.exists(jsn)) and os.path.exists(imp)\n",
    "\n",
    "    # 2) default_dir as given (relative to current working dir)\n",
    "    if has_window(default_dir):\n",
    "        return default_dir\n",
    "\n",
    "    # 3) likely absolute locations on your box (safe no-ops elsewhere)\n",
    "    fallbacks = [\n",
    "        r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\outputs\\models\",\n",
    "        r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\outputs\\external_validation\\models\",\n",
    "    ]\n",
    "    for d in fallbacks:\n",
    "        if has_window(d):\n",
    "            return d\n",
    "\n",
    "    # Last resort: return default (load will fail with clear message if missing)\n",
    "    return default_dir\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Bundle loading\n",
    "# -------------------------------\n",
    "def load_bundle(window: int, model_dir: str = \"outputs/models\"\n",
    "                ) -> Tuple[xgb.XGBClassifier, object, Optional[object], List[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Load the complete model bundle for a specific time window.\n",
    "    Returns (model, imputer, calibrator_or_None, feature_names, threshold_or_None)\n",
    "    \"\"\"\n",
    "    if window not in ALLOWED_WINDOWS:\n",
    "        raise ValueError(f\"window must be one of {sorted(ALLOWED_WINDOWS)}\")\n",
    "\n",
    "    # Resolve actual dir that contains this window\n",
    "    model_dir = resolve_model_dir(window, model_dir)\n",
    "\n",
    "    # Model (pickle â†’ JSON fallback)\n",
    "    model_path = os.path.join(model_dir, f\"xgboost_w{window}.pkl\")\n",
    "    json_path = os.path.join(model_dir, f\"xgb_w{window}.json\")\n",
    "\n",
    "    model: Optional[xgb.XGBClassifier] = None\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Warning: Failed to unpickle {os.path.basename(model_path)} ({e}). \"\n",
    "                  f\"Falling back to JSON model if available...\")\n",
    "\n",
    "    if model is None:\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\n",
    "                \"Model not found or could not be loaded for window \"\n",
    "                f\"{window} at '{model_dir}'.\\n\"\n",
    "                f\"  - Tried pickle: {model_path}\\n\"\n",
    "                f\"  - Tried JSON:   {json_path}\\n\"\n",
    "                f\"Hint: set XMI_MODEL_DIR to the folder with your bundles.\"\n",
    "            )\n",
    "        try:\n",
    "            model = xgb.XGBClassifier()\n",
    "            model.load_model(json_path)  # modern XGBoost (â‰¥1.6)\n",
    "        except Exception:\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(json_path)\n",
    "            model = xgb.XGBClassifier()\n",
    "            model._Booster = booster  # type: ignore[attr-defined]\n",
    "\n",
    "    # Imputer (required)\n",
    "    imputer_path = os.path.join(model_dir, f\"imputer_w{window}.pkl\")\n",
    "    if not os.path.exists(imputer_path):\n",
    "        raise FileNotFoundError(f\"Missing imputer file: {imputer_path}\")\n",
    "    with open(imputer_path, \"rb\") as f:\n",
    "        imputer = pickle.load(f)\n",
    "\n",
    "    # Calibrator (optional)\n",
    "    calibrator = None\n",
    "    calibrator_path = os.path.join(model_dir, f\"calibrator_w{window}.pkl\")\n",
    "    if os.path.exists(calibrator_path):\n",
    "        try:\n",
    "            with open(calibrator_path, \"rb\") as f:\n",
    "                calibrator = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Warning: Could not load calibrator ({e}). Using uncalibrated model.\")\n",
    "            calibrator = None\n",
    "\n",
    "    # Feature names (required; accept legacy filename; infer if missing)\n",
    "    feature_path = os.path.join(model_dir, f\"feature_names_w{window}.txt\")\n",
    "    alt_path = os.path.join(model_dir, f\"features_w{window}.txt\")  # legacy\n",
    "    feature_names: Optional[List[str]] = None\n",
    "\n",
    "    def _save_names(names: List[str], path: str) -> None:\n",
    "        try:\n",
    "            pd.Series(names).to_csv(path, index=False, header=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if os.path.exists(feature_path):\n",
    "        feature_names = pd.read_csv(feature_path, header=None)[0].astype(str).str.strip().tolist()\n",
    "    elif os.path.exists(alt_path):\n",
    "        feature_names = pd.read_csv(alt_path, header=None)[0].astype(str).str.strip().tolist()\n",
    "        _save_names(feature_names, feature_path)  # normalize name for future\n",
    "    else:\n",
    "        # Try to infer from imputer (best; preserves training order)\n",
    "        names = getattr(imputer, \"feature_names_in_\", None)\n",
    "        if names is not None:\n",
    "            feature_names = [str(n) for n in list(names)]\n",
    "            _save_names(feature_names, feature_path)\n",
    "            print(f\"â„¹ï¸ feature_names inferred from imputer and saved â†’ {feature_path}\")\n",
    "        else:\n",
    "            # Try model booster / sklearn-style names\n",
    "            try:\n",
    "                booster = model.get_booster()\n",
    "                bn = getattr(booster, \"feature_names\", None)\n",
    "            except Exception:\n",
    "                bn = None\n",
    "            nfi = getattr(model, \"feature_names_in_\", None)\n",
    "            if bn:\n",
    "                feature_names = [str(n) for n in list(bn)]\n",
    "                _save_names(feature_names, feature_path)\n",
    "                print(f\"â„¹ï¸ feature_names inferred from model booster and saved â†’ {feature_path}\")\n",
    "            elif nfi is not None:\n",
    "                feature_names = [str(n) for n in list(nfi)]\n",
    "                _save_names(feature_names, feature_path)\n",
    "                print(f\"â„¹ï¸ feature_names inferred from model and saved â†’ {feature_path}\")\n",
    "\n",
    "    if not feature_names:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing feature list for window {window} under '{model_dir}'. \"\n",
    "            f\"Expected {os.path.basename(feature_path)} (or legacy {os.path.basename(alt_path)}).\"\n",
    "        )\n",
    "\n",
    "    # Set n_features_in_ for sklearn compatibility (best effort)\n",
    "    try:\n",
    "        model.n_features_in_ = len(feature_names)  # type: ignore[attr-defined]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Threshold (optional)\n",
    "    threshold = None\n",
    "    threshold_path = os.path.join(model_dir, f\"threshold_w{window}.csv\")\n",
    "    if os.path.exists(threshold_path):\n",
    "        threshold_df = pd.read_csv(threshold_path)\n",
    "        if \"threshold\" in threshold_df.columns and not threshold_df.empty:\n",
    "            threshold = float(threshold_df[\"threshold\"].iloc[0])\n",
    "\n",
    "    return model, imputer, calibrator, feature_names, threshold\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Feature alignment\n",
    "# -------------------------------\n",
    "def _align_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature_names: List[str],\n",
    "    warn_all_missing: bool = True\n",
    ") -> Tuple[np.ndarray, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Align incoming dataframe to match training feature set.\n",
    "    Returns (aligned_matrix (np.ndarray), missing_features, extra_features)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cols = set(df.columns)\n",
    "    expected = set(feature_names)\n",
    "\n",
    "    # Identify missing and extra features\n",
    "    missing = [c for c in feature_names if c not in cols]\n",
    "    extra = sorted(list(cols - expected))\n",
    "\n",
    "    # Add missing as NaN, drop extras, order correctly\n",
    "    for c in missing:\n",
    "        df[c] = np.nan\n",
    "    df = df[feature_names]\n",
    "\n",
    "    # Ensure numeric types (coerce strings safely)\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Optional warning if everything is missing (imputer medians only)\n",
    "    if warn_all_missing and len(missing) == len(feature_names):\n",
    "        warnings.warn(\n",
    "            \"All features are missing! Prediction will be based on imputer medians only.\",\n",
    "            RuntimeWarning\n",
    "        )\n",
    "\n",
    "    return df.to_numpy(), missing, extra\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Core prediction functions\n",
    "# -------------------------------\n",
    "def predict_proba(\n",
    "    df: pd.DataFrame,\n",
    "    window: int,\n",
    "    calibrated: bool = True,\n",
    "    model_dir: str = \"outputs/models\",\n",
    "    return_diagnostics: bool = False\n",
    ") -> Union[np.ndarray, Tuple[np.ndarray, List[str], List[str]]]:\n",
    "    \"\"\"\n",
    "    Predict probabilities for a dataframe of patients.\n",
    "    If return_diagnostics=True, returns (probs, missing_features, extra_features)\n",
    "    \"\"\"\n",
    "    model, imputer, calibrator, feature_names, _ = load_bundle(window, model_dir)\n",
    "\n",
    "    X, missing, extra = _align_features(df, feature_names)\n",
    "    X_imputed = imputer.transform(X)\n",
    "\n",
    "    if calibrated and calibrator is not None:\n",
    "        probs = calibrator.predict_proba(X_imputed)[:, 1]\n",
    "    else:\n",
    "        probs = model.predict_proba(X_imputed)[:, 1]\n",
    "\n",
    "    if return_diagnostics:\n",
    "        return probs, missing, extra\n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_binary(\n",
    "    df: pd.DataFrame,\n",
    "    window: int,\n",
    "    threshold: Optional[float] = None,\n",
    "    model_dir: str = \"outputs/models\",\n",
    "    bundle: Optional[Tuple] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary predictions using an explicit or stored optimal threshold.\n",
    "    \"\"\"\n",
    "    if bundle is None:\n",
    "        _, _, _, _, optimal_thr = load_bundle(window, model_dir)\n",
    "    else:\n",
    "        _, _, _, _, optimal_thr = bundle\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = optimal_thr if optimal_thr is not None else 0.5\n",
    "\n",
    "    probs = predict_proba(df, window, calibrated=True, model_dir=model_dir)\n",
    "    return (probs >= float(threshold)).astype(int)\n",
    "\n",
    "\n",
    "def predict_single_patient(\n",
    "    patient_dict: Dict[str, float],\n",
    "    window: int,\n",
    "    calibrated: bool = True,\n",
    "    model_dir: str = \"outputs/models\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for a single patient dict.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([patient_dict])\n",
    "    prob = predict_proba(df, window, calibrated, model_dir)\n",
    "    return float(prob[0])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "def get_feature_importance(window: int, model_dir: str = \"outputs/models\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load feature importance CSV; if missing, derive from model.\n",
    "    \"\"\"\n",
    "    fi_path = os.path.join(model_dir, f\"feature_importance_w{window}.csv\")\n",
    "    if os.path.exists(fi_path):\n",
    "        return pd.read_csv(fi_path)\n",
    "\n",
    "    model, _, _, feature_names, _ = load_bundle(window, model_dir)\n",
    "    imp = getattr(model, \"feature_importances_\", None)\n",
    "    if imp is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Feature importance CSV not found and model has no feature_importances_.\"\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame({\"feature\": feature_names, \"importance\": imp}).sort_values(\n",
    "        \"importance\", ascending=False\n",
    "    )\n",
    "\n",
    "\n",
    "def get_risk_category(probability: float, thresholds: Optional[Dict[str, float]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Map probability to risk category.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {\"low\": 0.1, \"medium\": 0.3, \"high\": 0.5}\n",
    "\n",
    "    if probability < thresholds[\"low\"]:\n",
    "        return \"Low Risk\"\n",
    "    elif probability < thresholds[\"medium\"]:\n",
    "        return \"Moderate Risk\"\n",
    "    elif probability < thresholds[\"high\"]:\n",
    "        return \"High Risk\"\n",
    "    else:\n",
    "        return \"Very High Risk\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Batch prediction helper\n",
    "# -------------------------------\n",
    "def batch_predict_from_csv(\n",
    "    csv_path: str,\n",
    "    window: int,\n",
    "    output_path: Optional[str] = None,\n",
    "    calibrated: bool = True,\n",
    "    include_diagnostics: bool = False,\n",
    "    model_dir: str = \"outputs/models\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict for a CSV of patients; returns DataFrame with predictions appended.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if include_diagnostics:\n",
    "        probs, missing, extra = predict_proba(\n",
    "            df, window, calibrated, model_dir=model_dir, return_diagnostics=True\n",
    "        )\n",
    "        df[\"missing_features\"] = str(missing) if missing else \"\"\n",
    "        df[\"extra_features\"] = str(extra) if extra else \"\"\n",
    "    else:\n",
    "        probs = predict_proba(df, window, calibrated, model_dir=model_dir)\n",
    "\n",
    "    df[\"mortality_prob\"] = probs\n",
    "    df[\"risk_category\"] = df[\"mortality_prob\"].apply(get_risk_category)\n",
    "    df[\"binary_pred\"] = predict_binary(df, window, model_dir=model_dir)\n",
    "\n",
    "    if output_path:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Predictions saved to: {output_path}\")\n",
    "        print(f\"  Total patients: {len(df)}\")\n",
    "        print(f\"  High risk (>0.5): {(df['mortality_prob'] > 0.5).sum()}\")\n",
    "        print(f\"  Mean predicted mortality: {df['mortality_prob'].mean():.1%}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# API-ready wrapper (batch/single)\n",
    "# -------------------------------\n",
    "def predict_api(\n",
    "    patient_data: Union[Dict[str, float], pd.DataFrame],\n",
    "    window: int = 12,\n",
    "    return_details: bool = False,\n",
    "    model_dir: str = \"outputs/models\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    API-friendly prediction. Accepts a dict (single) or DataFrame (batch).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(patient_data, dict):\n",
    "            df = pd.DataFrame([patient_data])\n",
    "        else:\n",
    "            df = patient_data.copy()\n",
    "\n",
    "        model, imputer, calibrator, feature_names, threshold = load_bundle(window, model_dir)\n",
    "\n",
    "        X, missing, extra = _align_features(df, feature_names)\n",
    "        X_imputed = imputer.transform(X)\n",
    "\n",
    "        if calibrator is not None:\n",
    "            probs = calibrator.predict_proba(X_imputed)[:, 1]\n",
    "        else:\n",
    "            probs = model.predict_proba(X_imputed)[:, 1]\n",
    "\n",
    "        use_threshold = 0.5 if threshold is None else float(threshold)\n",
    "        binary = (probs >= use_threshold).astype(int)\n",
    "        risks = [get_risk_category(float(p)) for p in probs]\n",
    "\n",
    "        result: Dict = {\n",
    "            \"success\": True,\n",
    "            \"window\": window,\n",
    "            \"n_patients\": int(len(df)),\n",
    "            \"probabilities\": [float(p) for p in probs],\n",
    "            \"binary_predictions\": [int(b) for b in binary],\n",
    "            \"risk_categories\": risks,\n",
    "        }\n",
    "\n",
    "        if len(df) == 1:\n",
    "            result[\"probability\"] = float(probs[0])\n",
    "            result[\"binary_prediction\"] = int(binary[0])\n",
    "            result[\"risk_category\"] = risks[0]\n",
    "\n",
    "        if return_details:\n",
    "            result[\"threshold\"] = use_threshold\n",
    "            result[\"missing_features\"] = missing\n",
    "            result[\"extra_features\"] = extra\n",
    "            result[\"n_missing\"] = len(missing)\n",
    "            result[\"n_extra\"] = len(extra)\n",
    "            if len(missing) == len(feature_names):\n",
    "                result[\"warning\"] = (\n",
    "                    \"All features missing - prediction based on imputer medians only\"\n",
    "                )\n",
    "            try:\n",
    "                fi = get_feature_importance(window, model_dir=model_dir)\n",
    "                result[\"top_features\"] = fi.head(5)[[\"feature\", \"importance\"]].to_dict(\"records\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            result[\"calibrated\"] = calibrator is not None\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Test functions (without actual model files)\n",
    "# -------------------------------\n",
    "def test_functions_defined():\n",
    "    \"\"\"Test that all functions are properly defined\"\"\"\n",
    "    functions = [\n",
    "        'load_bundle',\n",
    "        '_align_features',\n",
    "        'predict_proba',\n",
    "        'predict_binary',\n",
    "        'predict_single_patient',\n",
    "        'get_feature_importance',\n",
    "        'get_risk_category',\n",
    "        'batch_predict_from_csv',\n",
    "        'predict_api'\n",
    "    ]\n",
    "\n",
    "    for func_name in functions:\n",
    "        assert func_name in globals(), f\"{func_name} not defined\"\n",
    "\n",
    "    print(\"âœ… All functions defined correctly\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main execution / Smoke test\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ICU DEPLOYMENT HELPER TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_functions_defined()\n",
    "\n",
    "    # Example single-patient dict (keys don't need to include all features)\n",
    "    sample = {\n",
    "        \"age_years\": 65,\n",
    "        \"hr_mean\": 85,\n",
    "        \"hr_max\": 95,\n",
    "        \"hr_min\": 75,\n",
    "        \"sbp_mean\": 120,\n",
    "        \"sbp_min\": 100,\n",
    "        \"sbp_max\": 140,\n",
    "        \"dbp_mean\": 70,\n",
    "        \"dbp_min\": 60,\n",
    "        \"dbp_max\": 80,\n",
    "        \"rr_mean\": 18,\n",
    "        \"rr_max\": 22,\n",
    "        \"rr_min\": 14,\n",
    "        \"spo2_mean\": 95,\n",
    "        \"spo2_min\": 92,\n",
    "        \"temp_mean\": 37.0,\n",
    "        \"temp_max\": 37.5,\n",
    "        \"lactate_max\": 2.1,\n",
    "        \"lactate_mean\": 1.8,\n",
    "        \"creat_max\": 1.3,\n",
    "        \"creat_mean\": 1.1,\n",
    "        \"troponin_max\": 5.2,\n",
    "        \"glucose_mean\": 120,\n",
    "        \"glucose_max\": 150,\n",
    "        \"hemoglobin_min\": 11.5,\n",
    "        \"wbc_max\": 12.5,\n",
    "        \"platelets_min\": 180\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Resolve model dir for window=12 without relying on cwd\n",
    "        md = resolve_model_dir(12, \"outputs/models\")\n",
    "        print(f\"\\nUsing model_dir: {md}\")\n",
    "\n",
    "        model_exists = (\n",
    "            os.path.exists(os.path.join(md, \"xgboost_w12.pkl\")) or\n",
    "            os.path.exists(os.path.join(md, \"xgb_w12.json\"))\n",
    "        )\n",
    "        if model_exists:\n",
    "            result = predict_api(sample, window=12, return_details=True, model_dir=md)\n",
    "            if result[\"success\"]:\n",
    "                print(\"\\nâœ… Smoke test PASSED!\")\n",
    "                print(f\"  Probability: {result.get('probability', float('nan')):.3f}\")\n",
    "                print(f\"  Risk category: {result.get('risk_category', 'N/A')}\")\n",
    "                print(f\"  Binary prediction: {result.get('binary_prediction', 'N/A')}\")\n",
    "                if 'n_missing' in result:\n",
    "                    print(f\"  Missing features: {result['n_missing']}\")\n",
    "                if 'calibrated' in result:\n",
    "                    print(f\"  Calibrated: {result['calibrated']}\")\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Prediction failed: {result.get('error', 'Unknown error')}\")\n",
    "                print(f\"  Error type: {result.get('error_type', 'Unknown')}\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ Model files not found.\")\n",
    "            print(\"  Expected at least one of:\")\n",
    "            print(f\"    - {os.path.join(md, 'xgboost_w12.pkl')}\")\n",
    "            print(f\"    - {os.path.join(md, 'xgb_w12.json')}\")\n",
    "            print(\"  Set XMI_MODEL_DIR or place bundles under outputs/models/\")\n",
    "            # Exercise error path:\n",
    "            result = predict_api(sample, window=12, return_details=True, model_dir=md)\n",
    "            assert result[\"success\"] is False\n",
    "            assert \"error\" in result\n",
    "            print(\"\\nâœ… Error handling works correctly\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Smoke test encountered error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Test complete. Functions are ready for use.\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ultra-minimal diagnostic using functions already defined in this notebook ---\n",
    "\n",
    "WINDOW = 12\n",
    "MODEL_DIR = \"outputs/models\"\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 1) Load bundle\n",
    "model, imputer, calibrator, feature_names, threshold = load_bundle(WINDOW, MODEL_DIR)\n",
    "print(f\"âœ“ Loaded bundle for {WINDOW}h: {len(feature_names)} features\")\n",
    "\n",
    "# 2) Imputer vs feature sanity check\n",
    "stats = getattr(imputer, \"statistics_\", None)\n",
    "assert stats is not None and len(stats) == len(feature_names), \\\n",
    "    f\"Imputer stats ({None if stats is None else len(stats)}) != feature count ({len(feature_names)})\"\n",
    "print(\"âœ“ Imputer statistics length matches features\")\n",
    "\n",
    "# 3) One prediction on the imputer-median row\n",
    "median_df = pd.DataFrame([imputer.statistics_], columns=feature_names)\n",
    "out = predict_api(median_df, window=WINDOW, return_details=True)\n",
    "assert out.get(\"success\", False), f\"Prediction failed: {out}\"\n",
    "\n",
    "prob = float(out[\"probabilities\"][0])\n",
    "thr  = out.get(\"threshold\", threshold if threshold is not None else 0.5)\n",
    "binp = int(prob >= float(thr))\n",
    "print(f\"âœ“ Prediction OK â€” prob={prob:.3f} | threshold={thr} | binary={binp} | \"\n",
    "      f\"calibrated={out.get('calibrated', calibrator is not None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell right after the deployment helper cell\n",
    "import numpy as np  # safe to import again\n",
    "MODEL_DIR = \"outputs/models\"\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "\n",
    "for w in WINDOWS:\n",
    "    try:\n",
    "        model, imputer, calibrator, feature_names, thr = load_bundle(w, MODEL_DIR)\n",
    "        # \"Typical\" dummy patient: use imputer medians aligned to feature order\n",
    "        dummy = dict(zip(feature_names, getattr(imputer, \"statistics_\", np.full(len(feature_names), np.nan))))\n",
    "        res = predict_api(dummy, window=w, return_details=True)\n",
    "        print(f\"w{w}: âœ“ prob={res['probability']:.3f} | thr={res['threshold']:.3f} | \"\n",
    "              f\"bin={res['binary_prediction']} | calibrated={res.get('calibrated', calibrator is not None)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"w{w}: âœ— {type(e).__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Show more precision (youâ€™ll see it isnâ€™t exactly zero)\n",
    "print(f\"{predict_proba(median_df, 12, calibrated=True,  model_dir=MODEL_DIR)[0]:.12f}\")\n",
    "\n",
    "# 2) See calibration effect on the SAME input\n",
    "pc = predict_proba(median_df, 12, calibrated=True,  model_dir=MODEL_DIR)[0]\n",
    "pr = predict_proba(median_df, 12, calibrated=False, model_dir=MODEL_DIR)[0]\n",
    "print(\"calibrated:\", pc, \" raw:\", pr)\n",
    "\n",
    "# 3) Compare median vs your original sample\n",
    "import pandas as pd\n",
    "pc_samp = predict_proba(pd.DataFrame([sample]), 12, calibrated=True, model_dir=MODEL_DIR)[0]\n",
    "print(\"sample (calibrated):\", pc_samp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "CSV_PATH = \"path/to/patients_sample.csv\"  # <-- replace with your CSV; keep same columns as training\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH).head(10)\n",
    "    res = predict_api(df, window=12, return_details=True)\n",
    "    print(\"n:\", res[\"n_patients\"])\n",
    "    print(\"mean prob:\", sum(res[\"probabilities\"]) / len(res[\"probabilities\"]))\n",
    "    print(\"â‰¥thr:\", sum(res[\"binary_predictions\"]))\n",
    "    print(\"first 3 risks:\", res[\"risk_categories\"][:3])\n",
    "else:\n",
    "    print(f\"CSV not found at {CSV_PATH} â€” skip batch check.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small test set using each window's imputer medians\n",
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "OUT_CSV = \"patients_sample.csv\"\n",
    "rows = []\n",
    "\n",
    "for w in [6, 12, 18, 24]:\n",
    "    model, imputer, calibrator, feature_names, thr = load_bundle(w, \"outputs/models\")\n",
    "    med = getattr(imputer, \"statistics_\", np.full(len(feature_names), np.nan))\n",
    "    # add a few slightly perturbed rows per window\n",
    "    for j in range(3):\n",
    "        row = dict(zip(feature_names, med))\n",
    "        # tiny jitter to avoid identical rows\n",
    "        for k in feature_names[: max(3, len(feature_names)//5) ]:\n",
    "            row[k] = float(row[k]) if pd.notna(row[k]) else 0.0\n",
    "            row[k] = row[k] * (1 + np.random.normal(0, 0.05))\n",
    "        row[\"__window__\"] = w  # helpful tag\n",
    "        rows.append(row)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(OUT_CSV, index=False)\n",
    "print(f\"Wrote {OUT_CSV} with {len(rows)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"patients_sample.csv\"\n",
    "if Path(CSV_PATH).exists():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    for w in [6, 12, 18, 24]:\n",
    "        df_w = df[df[\"__window__\"] == w].drop(columns=[\"__window__\"])\n",
    "        # Use the helper that aligns features & applies calibrator/threshold\n",
    "        res = predict_api(df_w, window=w, return_details=True)\n",
    "        print(f\"\\nw{w}: n={res['n_patients']}, mean_prob={np.mean(res['probabilities']):.3f}, \"\n",
    "              f\">=thr={sum(res['binary_predictions'])}/{res['n_patients']}, thr={res['threshold']:.3f}, \"\n",
    "              f\"calibrated={res.get('calibrated', True)}\")\n",
    "else:\n",
    "    print(\"Sample CSV not found â€” rerun the cell that creates patients_sample.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monotonic sanity check: bump a few risk features and confirm prob increases\n",
    "import numpy as np, pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "WINDOW = 12\n",
    "model, imputer, calibrator, feature_names, thr = load_bundle(WINDOW)\n",
    "\n",
    "# Start from median row\n",
    "base = dict(zip(feature_names, imputer.statistics_))\n",
    "base_df = pd.DataFrame([base])\n",
    "p0 = predict_api(base_df, window=WINDOW)[\"probability\"]\n",
    "\n",
    "# Make them sicker: older, higher lactate, lower SBP, lower SpO2\n",
    "sick = deepcopy(base)\n",
    "for k, v in [\n",
    "    (\"age_years\", base.get(\"age_years\", 60) + 20),\n",
    "    (\"lactate_max\", base.get(\"lactate_max\", 1.5) + 3.0),\n",
    "    (\"sbp_min\", max(40, base.get(\"sbp_min\", 110) - 30)),\n",
    "    (\"spo2_min\", max(50, base.get(\"spo2_min\", 96) - 10)),\n",
    "]:\n",
    "    sick[k] = v\n",
    "\n",
    "sick_df = pd.DataFrame([sick])\n",
    "p1 = predict_api(sick_df, window=WINDOW)[\"probability\"]\n",
    "\n",
    "print(f\"Baseline prob: {p0:.3f} â†’ Sick prob: {p1:.3f}  (Î”={p1-p0:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = predict_api(pd.DataFrame([base]), window=12, return_details=True)\n",
    "res1 = predict_api(pd.DataFrame([sick]), window=12, return_details=True)\n",
    "print(f\"thr={res1['threshold']:.3f} | base: prob={res0['probability']:.3f}, bin={res0['binary_prediction']}, {res0['risk_category']} \"\n",
    "      f\"â†’ sick: prob={res1['probability']:.3f}, bin={res1['binary_prediction']}, {res1['risk_category']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep a single feature and see how prob changes\n",
    "model, imputer, cal, feats, thr = load_bundle(12)\n",
    "base = dict(zip(feats, imputer.statistics_))\n",
    "\n",
    "def sweep(var, values):\n",
    "    rows = []\n",
    "    for v in values:\n",
    "        r = base.copy()\n",
    "        r[var] = v\n",
    "        rows.append(r)\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = predict_api(df, window=12)\n",
    "    return pd.DataFrame({var: values, \"prob\": out[\"probabilities\"]})\n",
    "\n",
    "s = sweep(\"lactate_max\", np.linspace(0.5, 6.0, 12))\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: create a tiny batch CSV and run batch prediction\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "WINDOW = 12\n",
    "model, imputer, calibrator, feature_names, thr = load_bundle(WINDOW)\n",
    "\n",
    "# Build a few rows from imputer medians and tweak a couple features\n",
    "base = dict(zip(feature_names, imputer.statistics_))\n",
    "rows = [\n",
    "    base,\n",
    "    {**base, \"lactate_max\": 4.0},\n",
    "    {**base, \"lactate_max\": 5.0, \"sbp_min\": base.get(\"sbp_min\", 100) - 15},\n",
    "]\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = Path(\"patients_sample.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Wrote {csv_path} with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "\n",
    "out = batch_predict_from_csv(str(csv_path), WINDOW, include_diagnostics=True)\n",
    "out[[\"mortality_prob\", \"binary_pred\", \"risk_category\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare calibrated vs uncalibrated probabilities on a sweep\n",
    "vals = np.linspace(0.5, 6.0, 12)\n",
    "rows = []\n",
    "for v in vals:\n",
    "    r = base.copy()\n",
    "    r[\"lactate_max\"] = v\n",
    "    rows.append(r)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "cal = predict_proba(df, window=12, calibrated=True)\n",
    "uncal = predict_proba(df, window=12, calibrated=False)\n",
    "pd.DataFrame({\"lactate_max\": vals, \"prob_cal\": cal, \"prob_uncal\": uncal})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "w = 12\n",
    "pkl = f\"outputs/models/xgboost_w{w}.pkl\"\n",
    "bak = pkl + \".bak\"\n",
    "if os.path.exists(pkl):\n",
    "    shutil.move(pkl, bak)\n",
    "try:\n",
    "    _ = load_bundle(w)  # should succeed via JSON\n",
    "    print(\"âœ“ JSON fallback works\")\n",
    "finally:\n",
    "    if os.path.exists(bak):\n",
    "        shutil.move(bak, pkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, imp, cal, feats, thr = load_bundle(12)\n",
    "assert len(imp.statistics_) == len(feats)\n",
    "print(\"âœ“ Imputer stats match feature count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE MIMIC TRAINING & eICU VALIDATION PIPELINE (Compat: no callbacks)\n",
    "# True External Validation: Train on MIMIC, Test on eICU\n",
    "# - Removes early stopping entirely for compatibility with your XGBoost wrapper\n",
    "# - Keeps: leak-free calibration, VAL threshold, schema alignment, SHAP, plots, saving\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML Libraries\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "# Advanced ML\n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "# Progress tracking\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRUE EXTERNAL VALIDATION PIPELINE\")\n",
    "print(\"Train on MIMIC-IV â†’ Test on eICU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "TIME_WINDOWS = [6, 12, 18, 24]\n",
    "RANDOM_STATE = 42\n",
    "VAL_SIZE = 0.2   # Validation split within MIMIC\n",
    "N_TRIALS  = 50   # Optuna trials\n",
    "N_FOLDS   = 5    # Cross-validation folds\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = [\n",
    "    'outputs/external_validation',\n",
    "    'outputs/external_validation/models',\n",
    "    'outputs/external_validation/figures',\n",
    "    'outputs/external_validation/shap',\n",
    "    'outputs/external_validation/results'\n",
    "]\n",
    "for d in output_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# DATA LOADING / PREP\n",
    "# ============================================\n",
    "\n",
    "def load_dataset_features(dataset: str) -> pd.DataFrame:\n",
    "    path = f'outputs/features/{dataset}_features.csv'\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Feature file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {dataset.upper()}: {len(df):,} samples\")\n",
    "    if dataset == 'eicu' and 'patientunitstayid' in df.columns:\n",
    "        df = df.rename(columns={'patientunitstayid': 'stay_id'})\n",
    "    return df\n",
    "\n",
    "def prepare_features(df: pd.DataFrame, window: int) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dfw = df[df['window'] == window].copy()\n",
    "    if len(dfw) == 0:\n",
    "        raise ValueError(f\"No data for window {window}h\")\n",
    "    drop_cols = ['stay_id', 'hospital_expire_flag', 'window', 'icu_los_hours', 'source']\n",
    "    drop_cols = [c for c in drop_cols if c in dfw.columns]\n",
    "    X = dfw.drop(columns=drop_cols, errors='ignore')\n",
    "    y = dfw['hospital_expire_flag']\n",
    "    print(f\"  Window {window}h: {len(X):,} samples, {X.shape[1]} features\")\n",
    "    print(f\"  Mortality rate: {y.mean():.1%}\")\n",
    "    return X, y\n",
    "\n",
    "def align_features(df: pd.DataFrame, feature_names: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_names:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df[feature_names]\n",
    "\n",
    "def youden_threshold(y_true, y_prob):\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    return float(thr[np.argmax(j)])\n",
    "\n",
    "# ============================================\n",
    "# MODELING HELPERS\n",
    "# ============================================\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, X_val, y_val, n_trials=30):\n",
    "    \"\"\"Optuna search WITHOUT early stopping (compat mode).\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'auc',\n",
    "            'verbosity': 0,\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    print(f\"  Best AUROC (val): {study.best_value:.3f}\")\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "def train_model_with_cv(X_train, y_train, best_params, n_folds=5):\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    for tr, va in skf.split(X_train, y_train):\n",
    "        Xt, Xv = X_train.iloc[tr], X_train.iloc[va]\n",
    "        yt, yv = y_train.iloc[tr], y_train.iloc[va]\n",
    "        model = xgb.XGBClassifier(\n",
    "            **best_params, random_state=RANDOM_STATE,\n",
    "            n_jobs=-1, eval_metric='auc', verbosity=0\n",
    "        )\n",
    "        model.fit(Xt, yt)\n",
    "        yhat = model.predict_proba(Xv)[:, 1]\n",
    "        scores.append(roc_auc_score(yv, yhat))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def calculate_metrics(y_true, y_proba, y_pred=None, dataset_name=\"\"):\n",
    "    if y_pred is None:\n",
    "        y_pred = (y_proba >= 0.5).astype(int)\n",
    "    tp = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    tn = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    fp = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "    fn = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    metrics = {\n",
    "        'dataset': dataset_name,\n",
    "        'n_samples': int(len(y_true)),\n",
    "        'n_positive': int(y_true.sum()),\n",
    "        'prevalence': float(y_true.mean()),\n",
    "        'auroc': roc_auc_score(y_true, y_proba),\n",
    "        'auprc': average_precision_score(y_true, y_proba),\n",
    "        'brier': brier_score_loss(y_true, y_proba),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'sensitivity': float(sensitivity),\n",
    "        'specificity': float(specificity),\n",
    "        'ppv': float(ppv),\n",
    "        'npv': float(npv),\n",
    "        'f1': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "    metrics['ece'] = float(np.mean(np.abs(frac_pos - mean_pred)))\n",
    "    return metrics\n",
    "\n",
    "# ============================================\n",
    "# SHAP\n",
    "# ============================================\n",
    "\n",
    "def perform_shap_analysis(model, X_train, X_test, feature_names, window, save_dir):\n",
    "    print(f\"  Performing SHAP analysis for {window}h window...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    n_explain = min(1000, len(X_test))\n",
    "    X_explain = X_test.iloc[:n_explain]\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    # Summary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_explain, feature_names=feature_names, show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/shap_summary_w{window}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Bar\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_explain, feature_names=feature_names, plot_type=\"bar\", show=False, max_display=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/shap_importance_w{window}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Waterfall (highest risk in X_explain)\n",
    "    high_risk_idx = int(np.argsort(model.predict_proba(X_explain)[:, 1])[-1])\n",
    "    expected_value = explainer.expected_value\n",
    "    if isinstance(expected_value, (list, np.ndarray)) and np.ndim(expected_value) > 0:\n",
    "        base_val = float(np.array(expected_value).reshape(-1)[-1])\n",
    "    else:\n",
    "        base_val = float(expected_value)\n",
    "    shap_explanation = shap.Explanation(\n",
    "        values=shap_values[high_risk_idx],\n",
    "        base_values=base_val,\n",
    "        data=X_explain.iloc[high_risk_idx].values,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.waterfall_plot(shap_explanation, show=False, max_display=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/shap_waterfall_high_risk_w{window}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Dependence for top features\n",
    "    feature_importance = np.abs(shap_values).mean(0)\n",
    "    top_idx = np.argsort(feature_importance)[-4:]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10)); axes = axes.flatten()\n",
    "    for i, fi in enumerate(top_idx):\n",
    "        shap.dependence_plot(int(fi), shap_values, X_explain, feature_names=feature_names, ax=axes[i], show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/shap_dependence_w{window}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save tabular importance\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance,\n",
    "        'importance_normalized': feature_importance / (feature_importance.sum() + 1e-12)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    fi_df.to_csv(f\"{save_dir}/feature_importance_w{window}.csv\", index=False)\n",
    "    return fi_df\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING\n",
    "# ============================================\n",
    "\n",
    "def plot_roc_curves(results_dict, save_path):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(TIME_WINDOWS)))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot([0,1],[0,1],'k--',alpha=0.5,label='Random')\n",
    "    for w, c in zip(TIME_WINDOWS, colors):\n",
    "        k = f\"mimic_{w}\"\n",
    "        if k in results_dict:\n",
    "            fpr = results_dict[k]['fpr']; tpr = results_dict[k]['tpr']; auc = results_dict[k]['metrics']['auroc']\n",
    "            ax.plot(fpr, tpr, color=c, lw=2, label=f'{w}h (AUC={auc:.3f})')\n",
    "    ax.set_title('ROC â€” MIMIC-IV (Internal)'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "    ax.legend(loc='lower right'); ax.grid(alpha=0.3); ax.set_aspect('equal')\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot([0,1],[0,1],'k--',alpha=0.5,label='Random')\n",
    "    for w, c in zip(TIME_WINDOWS, colors):\n",
    "        k = f\"eicu_{w}\"\n",
    "        if k in results_dict:\n",
    "            fpr = results_dict[k]['fpr']; tpr = results_dict[k]['tpr']; auc = results_dict[k]['metrics']['auroc']\n",
    "            ax.plot(fpr, tpr, color=c, lw=2, label=f'{w}h (AUC={auc:.3f})')\n",
    "    ax.set_title('ROC â€” eICU (External)'); ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n",
    "    ax.legend(loc='lower right'); ax.grid(alpha=0.3); ax.set_aspect('equal')\n",
    "\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_calibration_curves(results_dict, save_path):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8)); axes = axes.flatten()\n",
    "    for i, w in enumerate(TIME_WINDOWS):\n",
    "        for j, dataset in enumerate(['mimic-iv','eicu']):\n",
    "            ax = axes[i*2 + j]\n",
    "            key = f\"{dataset.split('-')[0]}_{w}\"\n",
    "            if key in results_dict:\n",
    "                y_true = results_dict[key]['y_true']; y_proba = results_dict[key]['y_proba']\n",
    "                frac_pos, mean_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "                ax.plot([0,1],[0,1],'k--',alpha=0.5,label='Perfect')\n",
    "                ece = results_dict[key]['metrics']['ece']\n",
    "                ax.plot(mean_pred, frac_pos, marker='o', lw=2, label=f'Model (ECE={ece:.3f})')\n",
    "                ax.set_title(f'{dataset.upper()} - {w}h'); ax.set_xlabel('Mean Predicted'); ax.set_ylabel('Fraction Pos')\n",
    "                ax.legend(loc='lower right', fontsize=9); ax.grid(alpha=0.3); ax.set_aspect('equal')\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=800, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_performance_comparison(metrics_df, save_path):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    metrics_to_plot = [\n",
    "        ('auroc', 'AUROC', [0.7, 1.0]),\n",
    "        ('auprc', 'AUPRC', [0.3, 0.8]),\n",
    "        ('brier', 'Brier', [0.0, 0.15]),\n",
    "        ('sensitivity', 'Sensitivity', [0.5, 1.0]),\n",
    "        ('specificity', 'Specificity', [0.5, 1.0]),\n",
    "        ('f1', 'F1 Score', [0.3, 0.8]),\n",
    "    ]\n",
    "    for ax, (m, title, ylim) in zip(axes.flatten(), metrics_to_plot):\n",
    "        m_vals = metrics_df[metrics_df['dataset']=='MIMIC-IV'][m].values\n",
    "        e_vals = metrics_df[metrics_df['dataset']=='eICU'][m].values\n",
    "        x = np.arange(len(TIME_WINDOWS)); w = 0.35\n",
    "        ax.bar(x-w/2, m_vals, w, label='MIMIC-IV', color='steelblue', alpha=0.8)\n",
    "        ax.bar(x+w/2, e_vals, w, label='eICU', color='coral', alpha=0.8)\n",
    "        ax.set_xticks(x); ax.set_xticklabels([f'{t}h' for t in TIME_WINDOWS])\n",
    "        ax.set_title(title); ax.set_ylim(ylim); ax.grid(alpha=0.3, axis='y'); ax.legend()\n",
    "        for i,(mv, ev) in enumerate(zip(m_vals, e_vals)):\n",
    "            ax.text(i-w/2, mv+0.01, f'{mv:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "            ax.text(i+w/2, ev+0.01, f'{ev:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=800, bbox_inches='tight'); plt.close()\n",
    "\n",
    "# ============================================\n",
    "# MAIN\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING EXTERNAL VALIDATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\nðŸ“Š Loading datasets...\")\n",
    "    try:\n",
    "        mimic_df = load_dataset_features('mimic')\n",
    "        eicu_df  = load_dataset_features('eicu')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ {e}\")\n",
    "        return\n",
    "\n",
    "    all_results = {}\n",
    "    metrics_list = []\n",
    "\n",
    "    for window in TIME_WINDOWS:\n",
    "        print(f\"\\n{'='*80}\\nPROCESSING {window}-HOUR WINDOW\\n{'='*80}\")\n",
    "        try:\n",
    "            # Prepare data\n",
    "            print(\"\\nðŸ“Š Preparing MIMIC-IV data...\")\n",
    "            X_mimic, y_mimic = prepare_features(mimic_df, window)\n",
    "            X_temp, X_mimic_test, y_temp, y_mimic_test = train_test_split(\n",
    "                X_mimic, y_mimic, test_size=0.2, random_state=RANDOM_STATE, stratify=y_mimic\n",
    "            )\n",
    "            X_mimic_train, X_mimic_val, y_mimic_train, y_mimic_val = train_test_split(\n",
    "                X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE, stratify=y_temp\n",
    "            )\n",
    "            print(f\"  MIMIC splits - Train: {len(X_mimic_train)}, Val: {len(X_mimic_val)}, Test: {len(X_mimic_test)}\")\n",
    "\n",
    "            print(\"\\nðŸ“Š Preparing eICU data...\")\n",
    "            X_eicu_test, y_eicu_test = prepare_features(eicu_df, window)\n",
    "            print(f\"  eICU test set: {len(X_eicu_test)} samples\")\n",
    "\n",
    "            # Align schemas\n",
    "            feature_names = X_mimic.columns.tolist()\n",
    "            X_mimic_train = align_features(X_mimic_train, feature_names)\n",
    "            X_mimic_val   = align_features(X_mimic_val,   feature_names)\n",
    "            X_mimic_test  = align_features(X_mimic_test,  feature_names)\n",
    "            X_eicu_test   = align_features(X_eicu_test,   feature_names)\n",
    "\n",
    "            # Impute (fit on TRAIN only)\n",
    "            print(\"\\nðŸ”§ Handling missing values...\")\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X_mimic_train_imp = pd.DataFrame(imputer.fit_transform(X_mimic_train), columns=feature_names, index=X_mimic_train.index)\n",
    "            X_mimic_val_imp   = pd.DataFrame(imputer.transform(X_mimic_val),     columns=feature_names, index=X_mimic_val.index)\n",
    "            X_mimic_test_imp  = pd.DataFrame(imputer.transform(X_mimic_test),    columns=feature_names, index=X_mimic_test.index)\n",
    "            X_eicu_test_imp   = pd.DataFrame(imputer.transform(X_eicu_test),     columns=feature_names, index=X_eicu_test.index)\n",
    "\n",
    "            # SMOTE (guarded)\n",
    "            if y_mimic_train.mean() < 0.15:\n",
    "                pos = int(y_mimic_train.sum())\n",
    "                if pos >= 2:\n",
    "                    print(\"  Applying SMOTE for class balance...\")\n",
    "                    k = max(1, min(5, pos-1))\n",
    "                    smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k)\n",
    "                    X_mimic_train_imp, y_mimic_train = smote.fit_resample(X_mimic_train_imp, y_mimic_train)\n",
    "                    print(f\"  After SMOTE: {len(X_mimic_train_imp)} samples (k={k})\")\n",
    "                else:\n",
    "                    print(\"  Skipping SMOTE (too few positives).\")\n",
    "\n",
    "            # Hyperparameter optimization (no early stopping)\n",
    "            print(\"\\nðŸŽ¯ Optimizing hyperparameters...\")\n",
    "            best_params, best_val_auroc = optimize_hyperparameters(\n",
    "                X_mimic_train_imp, y_mimic_train, X_mimic_val_imp, y_mimic_val, n_trials=N_TRIALS\n",
    "            )\n",
    "            print(f\"  Best validation AUROC: {best_val_auroc:.3f}\")\n",
    "\n",
    "            # CV on TRAIN+VAL (report only)\n",
    "            print(\"\\nðŸ“ˆ Cross-validation on MIMIC training set...\")\n",
    "            X_cv = pd.concat([X_mimic_train_imp, X_mimic_val_imp])\n",
    "            y_cv = pd.concat([y_mimic_train,     y_mimic_val])\n",
    "            cv_mean, cv_std = train_model_with_cv(X_cv, y_cv, best_params, n_folds=N_FOLDS)\n",
    "            print(f\"  {N_FOLDS}-fold CV AUROC: {cv_mean:.3f} Â± {cv_std:.3f}\")\n",
    "\n",
    "            # Final model on TRAIN only (no leakage)\n",
    "            print(\"\\nðŸ‹ï¸ Training final model on TRAIN only...\")\n",
    "            final_model = xgb.XGBClassifier(\n",
    "                **best_params, random_state=RANDOM_STATE, n_jobs=-1,\n",
    "                eval_metric='auc', verbosity=0\n",
    "            )\n",
    "            final_model.fit(X_mimic_train_imp, y_mimic_train)\n",
    "\n",
    "            # Calibrate on VAL\n",
    "            print(\"  Applying isotonic calibration on VAL...\")\n",
    "            calibrator = CalibratedClassifierCV(final_model, method='isotonic', cv='prefit')\n",
    "            calibrator.fit(X_mimic_val_imp, y_mimic_val)\n",
    "\n",
    "            # Threshold from VAL (calibrated)\n",
    "            print(\"  Learning decision threshold on VAL (Youden J)...\")\n",
    "            y_val_prob_cal = calibrator.predict_proba(X_mimic_val_imp)[:, 1]\n",
    "            thr = youden_threshold(y_mimic_val, y_val_prob_cal)\n",
    "            print(f\"  Chosen threshold: {thr:.3f}\")\n",
    "\n",
    "            # Internal validation (MIMIC TEST)\n",
    "            print(\"\\nðŸ“Š Internal validation (MIMIC test set)...\")\n",
    "            y_mimic_test_proba = calibrator.predict_proba(X_mimic_test_imp)[:, 1]\n",
    "            y_mimic_test_pred  = (y_mimic_test_proba >= thr).astype(int)\n",
    "            fpr_mimic, tpr_mimic, _ = roc_curve(y_mimic_test, y_mimic_test_proba)\n",
    "            mimic_metrics = calculate_metrics(y_mimic_test, y_mimic_test_proba, y_mimic_test_pred, \"MIMIC-IV\")\n",
    "            print(f\"  MIMIC Test AUROC: {mimic_metrics['auroc']:.3f}\")\n",
    "            print(f\"  MIMIC Test AUPRC: {mimic_metrics['auprc']:.3f}\")\n",
    "            print(f\"  MIMIC Test Brier: {mimic_metrics['brier']:.3f}\")\n",
    "\n",
    "            # External validation (eICU)\n",
    "            print(\"\\nðŸŒ External validation (eICU)...\")\n",
    "            y_eicu_test_proba = calibrator.predict_proba(X_eicu_test_imp)[:, 1]\n",
    "            y_eicu_test_pred  = (y_eicu_test_proba >= thr).astype(int)\n",
    "            fpr_eicu, tpr_eicu, _ = roc_curve(y_eicu_test, y_eicu_test_proba)\n",
    "            eicu_metrics = calculate_metrics(y_eicu_test, y_eicu_test_proba, y_eicu_test_pred, \"eICU\")\n",
    "            print(f\"  eICU Test AUROC: {eicu_metrics['auroc']:.3f}\")\n",
    "            print(f\"  eICU Test AUPRC: {eicu_metrics['auprc']:.3f}\")\n",
    "            print(f\"  eICU Test Brier: {eicu_metrics['brier']:.3f}\")\n",
    "            print(f\"  Performance drop: Î”AUROC = {mimic_metrics['auroc'] - eicu_metrics['auroc']:.3f}\")\n",
    "\n",
    "            # SHAP (TRAIN+VAL background, explain eICU)\n",
    "            print(\"\\nðŸ” SHAP Analysis...\")\n",
    "            X_mimic_full_train = pd.concat([X_mimic_train_imp, X_mimic_val_imp])\n",
    "            shap_importance = perform_shap_analysis(\n",
    "                final_model, X_mimic_full_train, X_eicu_test_imp,\n",
    "                feature_names, window, 'outputs/external_validation/shap'\n",
    "            )\n",
    "            print(\"  Top 5 features by SHAP importance:\")\n",
    "            for rank, (_, row) in enumerate(shap_importance.head(5).iterrows(), 1):\n",
    "                print(f\"    {rank}. {row['feature']}: {row['importance_normalized']:.3f}\")\n",
    "\n",
    "            # Store results\n",
    "            all_results[f'mimic_{window}'] = {\n",
    "                'metrics': mimic_metrics, 'fpr': fpr_mimic, 'tpr': tpr_mimic,\n",
    "                'y_true': y_mimic_test, 'y_proba': y_mimic_test_proba\n",
    "            }\n",
    "            all_results[f'eicu_{window}'] = {\n",
    "                'metrics': eicu_metrics, 'fpr': fpr_eicu, 'tpr': tpr_eicu,\n",
    "                'y_true': y_eicu_test, 'y_proba': y_eicu_test_proba\n",
    "            }\n",
    "            mimic_metrics['window'] = window; eicu_metrics['window'] = window\n",
    "            metrics_list += [mimic_metrics, eicu_metrics]\n",
    "\n",
    "            # Save artifacts\n",
    "            print(\"\\nðŸ’¾ Saving model artifacts...\")\n",
    "            with open(f'outputs/external_validation/models/xgboost_w{window}.pkl','wb') as f:\n",
    "                pickle.dump(final_model, f)\n",
    "            with open(f'outputs/external_validation/models/calibrator_w{window}.pkl','wb') as f:\n",
    "                pickle.dump(calibrator, f)\n",
    "            with open(f'outputs/external_validation/models/imputer_w{window}.pkl','wb') as f:\n",
    "                pickle.dump(imputer, f)\n",
    "            pd.DataFrame(feature_names).to_csv(f'outputs/external_validation/models/features_w{window}.txt', index=False, header=False)\n",
    "            with open(f'outputs/external_validation/models/params_w{window}.json','w') as f:\n",
    "                json.dump(best_params, f, indent=2)\n",
    "            pd.DataFrame({'threshold':[thr]}).to_csv(f'outputs/external_validation/models/threshold_w{window}.csv', index=False)\n",
    "            print(f\"  âœ… Window {window}h completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing window {window}h: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    # Summaries / Plots\n",
    "    if metrics_list:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERATING SUMMARY VISUALIZATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        metrics_df.to_csv('outputs/external_validation/results/all_metrics.csv', index=False)\n",
    "\n",
    "        summary_table = metrics_df.pivot_table(\n",
    "            index='window', columns='dataset',\n",
    "            values=['auroc','auprc','brier','sensitivity','specificity']\n",
    "        ).round(3)\n",
    "        print(\"\\nðŸ“Š SUMMARY TABLE:\"); print(summary_table)\n",
    "        summary_table.to_csv('outputs/external_validation/results/summary_table.csv')\n",
    "\n",
    "        print(\"\\nðŸ“ˆ Generating plots...\")\n",
    "        plot_roc_curves(all_results, 'outputs/external_validation/figures/roc_curves.png')\n",
    "        print(\"  âœ“ ROC curves saved\")\n",
    "        plot_calibration_curves(all_results, 'outputs/external_validation/figures/calibration_curves.png')\n",
    "        print(\"  âœ“ Calibration curves saved\")\n",
    "        plot_performance_comparison(metrics_df, 'outputs/external_validation/figures/performance_comparison.png')\n",
    "        print(\"  âœ“ Performance comparison saved\")\n",
    "\n",
    "        # Feature importance evolution\n",
    "        fig, axes = plt.subplots(1, len(TIME_WINDOWS), figsize=(20,6))\n",
    "        for i, w in enumerate(TIME_WINDOWS):\n",
    "            fi_path = f'outputs/external_validation/shap/feature_importance_w{w}.csv'\n",
    "            ax = axes[i]\n",
    "            if os.path.exists(fi_path):\n",
    "                fi_df = pd.read_csv(fi_path).head(10)\n",
    "                ax.barh(range(len(fi_df)), fi_df['importance_normalized'].values)\n",
    "                ax.set_yticks(range(len(fi_df))); ax.set_yticklabels(fi_df['feature'].values, fontsize=9)\n",
    "                ax.set_xlabel('Normalized Importance'); ax.set_title(f'{w}h'); ax.invert_yaxis(); ax.grid(alpha=0.3, axis='x')\n",
    "        plt.suptitle('Feature Importance Across Windows', fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout(); plt.savefig('outputs/external_validation/figures/feature_importance_evolution.png', dpi=300, bbox_inches='tight'); plt.close()\n",
    "        print(\"  âœ“ Feature importance evolution saved\")\n",
    "\n",
    "        # Approximate significance test\n",
    "        print(\"\\nðŸ“Š Statistical Testing (MIMIC vs eICU AUROC):\")\n",
    "        for w in TIME_WINDOWS:\n",
    "            mk, ek = f'mimic_{w}', f'eicu_{w}'\n",
    "            if mk in all_results and ek in all_results:\n",
    "                a_m, a_e = all_results[mk]['metrics']['auroc'], all_results[ek]['metrics']['auroc']\n",
    "                n_m, n_e = len(all_results[mk]['y_true']), len(all_results[ek]['y_true'])\n",
    "                se_m = np.sqrt(max(a_m*(1-a_m),1e-9)/max(n_m,1))\n",
    "                se_e = np.sqrt(max(a_e*(1-a_e),1e-9)/max(n_e,1))\n",
    "                z = (a_m - a_e) / max(np.sqrt(se_m**2 + se_e**2), 1e-9)\n",
    "                p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "                print(f\"  {w}h: Î”AUROC={a_m-a_e:.3f}, p={p:.3f} {'*' if p<0.05 else ''}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTERNAL VALIDATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ“ Output Directory Structure:\")\n",
    "    print(\"  outputs/external_validation/\")\n",
    "    print(\"  â”œâ”€â”€ models/          # Trained models, calibrators, imputers, thresholds, params\")\n",
    "    print(\"  â”œâ”€â”€ figures/         # ROC, calibration, performance plots\")\n",
    "    print(\"  â”œâ”€â”€ shap/            # SHAP analyses and feature importance\")\n",
    "    print(\"  â””â”€â”€ results/         # Metrics tables and summaries\")\n",
    "\n",
    "    print(\"\\nâœ… Key Achievements:\")\n",
    "    print(\"  â€¢ True external validation (MIMIC â†’ eICU)\")\n",
    "    print(\"  â€¢ Leak-free calibration (TRAIN â†’ calibrate on VAL)\")\n",
    "    print(\"  â€¢ Decision threshold learned on VAL and applied to TEST/eICU)\")\n",
    "    print(\"  â€¢ SHAP interpretability & saved plots/tables\")\n",
    "\n",
    "    return (pd.DataFrame(metrics_list) if metrics_list else None), all_results\n",
    "\n",
    "# ============================================\n",
    "# EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics_df, results = main()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUBGROUP ANALYSIS TEMPLATE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ“Š To perform subgroup analysis, merge predictions with original cohorts:\")\n",
    "    print(\"  1) Load mimic_mi_cohort.csv and eicu_mi_cohort.csv (with demographics).\")\n",
    "    print(\"  2) Merge with predictions using stay_id.\")\n",
    "    print(\"  3) Create age groups (<65, â‰¥65) and compute metrics per group.\")\n",
    "    print(\"  4) Compute metrics by gender and other clinically relevant strata.\")\n",
    "    print(\"  5) Test for interaction effects (e.g., logistic regression with interaction terms).\")\n",
    "    print(\"\\nAll results are saved in outputs/external_validation/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil, datetime as dt\n",
    "\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "SRC  = ROOT / \"outputs\" / \"external_validation\"\n",
    "DST  = ROOT / \"outputs\" / f\"external_validation_BACKUP_{dt.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "shutil.copytree(SRC, DST)\n",
    "print(\"Backed up â†’\", DST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETION: SUBGROUP ANALYSIS & CLINICAL INTERPRETATION\n",
    "# Subgroups, Interaction Effects (risk Ã— subgroup), Time-Resolved SHAP, Clinical Notes\n",
    "# - Uses calibrated XGB outputs from  artifacts\n",
    "# - Aligns eICU feature matrix to training schema (features_w{w}.txt / feature_names_w{w}.txt)\n",
    "# - Robust statsmodels p-values with a bootstrap fallback\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, accuracy_score, f1_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETION: SUBGROUP ANALYSIS & CLINICAL INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "TIME_WINDOWS = [6, 12, 18, 24]\n",
    "RANDOM_STATE = 42\n",
    "BASE_DIR    = Path(\"outputs/external_validation\")\n",
    "MODEL_DIR   = BASE_DIR / \"models\"\n",
    "FEAT_DIR    = Path(\"outputs/features\")\n",
    "COHORT_DIR  = Path(\"outputs/cohorts\")\n",
    "OUT_DIR     = BASE_DIR / \"interpretability\"\n",
    "(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ============================================\n",
    "# UTILITIES: LOAD BUNDLE & ALIGN FEATURES\n",
    "# ============================================\n",
    "\n",
    "def load_feature_names(window: int) -> List[str]:\n",
    "    cand = [\n",
    "        MODEL_DIR / f\"feature_names_w{window}.txt\",\n",
    "        MODEL_DIR / f\"features_w{window}.txt\",\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p, header=None)[0].astype(str).str.strip().tolist()\n",
    "    raise FileNotFoundError(f\"Missing feature names file for w{window}: {cand}\")\n",
    "\n",
    "def load_threshold(window: int) -> float:\n",
    "    thr_path = MODEL_DIR / f\"threshold_w{window}.csv\"\n",
    "    if thr_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(thr_path)\n",
    "            if \"threshold\" in df.columns:\n",
    "                return float(df[\"threshold\"].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return 0.5  # safe default if not found\n",
    "\n",
    "def load_bundle(window: int):\n",
    "    cal_path = MODEL_DIR / f\"calibrator_w{window}.pkl\"\n",
    "    imp_path = MODEL_DIR / f\"imputer_w{window}.pkl\"\n",
    "    if not cal_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing calibrator: {cal_path}\")\n",
    "    if not imp_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing imputer: {imp_path}\")\n",
    "    with open(cal_path, \"rb\") as f:\n",
    "        calibrator = pickle.load(f)\n",
    "    with open(imp_path, \"rb\") as f:\n",
    "        imputer = pickle.load(f)\n",
    "    feat_names = load_feature_names(window)\n",
    "    thr = load_threshold(window)\n",
    "    return calibrator, imputer, feat_names, thr\n",
    "\n",
    "def align_to_schema(df: pd.DataFrame, feature_names: List[str]) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    for c in feature_names:\n",
    "        if c not in X.columns:\n",
    "            X[c] = np.nan\n",
    "    X = X[feature_names]\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return X\n",
    "\n",
    "# ============================================\n",
    "# METRICS & HELPERS\n",
    "# ============================================\n",
    "\n",
    "def compute_core_metrics(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> Dict[str, float]:\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    # guard for degenerate cases\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auroc = np.nan\n",
    "    try:\n",
    "        auprc = average_precision_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auprc = np.nan\n",
    "    m = {\n",
    "        \"n\": int(len(y_true)),\n",
    "        \"pos\": int(y_true.sum()),\n",
    "        \"prevalence\": float(np.mean(y_true)),\n",
    "        \"auroc\": float(auroc),\n",
    "        \"auprc\": float(auprc),\n",
    "        \"brier\": float(brier_score_loss(y_true, y_prob)),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)) if (y_pred.sum() > 0 and y_true.sum() > 0) else np.nan,\n",
    "    }\n",
    "    try:\n",
    "        frac_pos, mean_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "        m[\"ece\"] = float(np.mean(np.abs(frac_pos - mean_pred)))\n",
    "    except Exception:\n",
    "        m[\"ece\"] = np.nan\n",
    "    return m\n",
    "\n",
    "def calibration_slope(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    \"\"\"Slope from logistic regression of y ~ logit(p), intercept included.\"\"\"\n",
    "    try:\n",
    "        p = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "        logit_p = np.log(p/(1-p)).reshape(-1, 1)\n",
    "        lr = LogisticRegression(penalty='none', solver='lbfgs', max_iter=200)\n",
    "        lr.fit(logit_p, y_true)\n",
    "        return float(lr.coef_.ravel()[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def normalize_sex(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    s = s.replace({\"m\":\"Male\",\"male\":\"Male\",\"f\":\"Female\",\"female\":\"Female\"})\n",
    "    return s.where(s.isin([\"Male\",\"Female\"]), \"Unknown\")\n",
    "\n",
    "def age_bin(series: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.where(series < 65, \"<65\", \"â‰¥65\"), index=series.index)\n",
    "\n",
    "def compress_ethnicity(series: pd.Series, min_frac: float = 0.05) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    keep = set(vc[vc >= min_frac].index)\n",
    "    return s.apply(lambda x: x if x in keep else \"Other/Minor\")\n",
    "\n",
    "# ============================================\n",
    "# SUBGROUP ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "def subgroup_metrics(df: pd.DataFrame, y_true: np.ndarray, y_prob: np.ndarray, thr: float,\n",
    "                     col: str, labels: List[str] = None) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Compute metrics per subgroup. If numeric age, bin at 65; if other numeric, split at median.\"\"\"\n",
    "    df = df.copy()\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        if col.lower() in (\"age\",\"age_years\",\"age_numeric\"):\n",
    "            df[\"_subgroup\"] = age_bin(df[col])\n",
    "        else:\n",
    "            med = df[col].median()\n",
    "            df[\"_subgroup\"] = np.where(df[col] >= med, \"High\", \"Low\")\n",
    "    else:\n",
    "        df[\"_subgroup\"] = df[col].astype(str)\n",
    "        if labels is not None:\n",
    "            df[\"_subgroup\"] = pd.Categorical(df[\"_subgroup\"], categories=labels).astype(str)\n",
    "\n",
    "    out = {}\n",
    "    for lvl in df[\"_subgroup\"].dropna().unique():\n",
    "        m = df[\"_subgroup\"] == lvl\n",
    "        if m.sum() < 30:  # avoid tiny groups\n",
    "            continue\n",
    "        yt, yp = y_true[m], y_prob[m]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            continue\n",
    "        met = compute_core_metrics(yt, yp, thr)\n",
    "        met[\"calibration_slope\"] = calibration_slope(yt, yp)\n",
    "        out[str(lvl)] = met\n",
    "    return out\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION: predicted risk Ã— subgroup\n",
    "# ============================================\n",
    "\n",
    "def interaction_test_pred_x_group(y_true: np.ndarray, y_prob: np.ndarray, group: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Test interaction between logit(pred) and binary subgroup. Prefers statsmodels; fallback bootstrap.\"\"\"\n",
    "    res = {\"method\": \"\", \"p_value\": np.nan, \"coef\": np.nan, \"note\": \"\"}\n",
    "    # binarize to two levels (drop Unknown)\n",
    "    g = group.astype(str).replace(\"Unknown\", np.nan).dropna()\n",
    "    idx = g.index\n",
    "    if len(g.unique()) != 2:\n",
    "        res[\"note\"] = \"interaction requires exactly 2 levels\"\n",
    "        return res\n",
    "    mask = group.index.isin(idx)\n",
    "    y = y_true[mask]\n",
    "    p = np.clip(y_prob[mask], 1e-6, 1-1e-6)\n",
    "    logit_p = np.log(p/(1-p))\n",
    "    gbin = (g == list(g.unique())[1]).astype(int).to_numpy()\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X = np.column_stack([logit_p, gbin, logit_p * gbin])\n",
    "        X = sm.add_constant(X)\n",
    "        fit = sm.Logit(y, X).fit(disp=0, maxiter=200)\n",
    "        # const, logit_p, gbin, interaction\n",
    "        p_int = float(fit.pvalues[-1])\n",
    "        coef = float(fit.params[-1])\n",
    "        res.update({\"method\":\"statsmodels_logit_interaction\", \"p_value\":p_int, \"coef\":coef})\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        # Bootstrap AUC-gap as a rough check\n",
    "        lvls = list(g.unique())\n",
    "        m0 = (g == lvls[0]).values\n",
    "        m1 = (g == lvls[1]).values\n",
    "        if m0.sum() < 40 or m1.sum() < 40:\n",
    "            res[\"note\"] = \"too few samples for bootstrap\"\n",
    "            return res\n",
    "        try:\n",
    "            auc0 = roc_auc_score(y[m0], p[m0])\n",
    "            auc1 = roc_auc_score(y[m1], p[m1])\n",
    "        except Exception:\n",
    "            res[\"note\"] = \"degenerate labels in a subgroup\"\n",
    "            return res\n",
    "        delta_obs = auc1 - auc0\n",
    "        B = 400\n",
    "        cnt = 0\n",
    "        n0, n1 = m0.sum(), m1.sum()\n",
    "        idx0 = np.where(m0)[0]; idx1 = np.where(m1)[0]\n",
    "        for _ in range(B):\n",
    "            b0 = np.random.choice(idx0, size=n0, replace=True)\n",
    "            b1 = np.random.choice(idx1, size=n1, replace=True)\n",
    "            try:\n",
    "                a0 = roc_auc_score(y[b0], p[b0]); a1 = roc_auc_score(y[b1], p[b1])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if (a1 - a0) >= delta_obs:\n",
    "                cnt += 1\n",
    "        p_boot = (cnt + 1)/(B + 1)\n",
    "        res.update({\"method\":\"bootstrap_auc_gap\", \"p_value\":float(p_boot), \"coef\":float(delta_obs)})\n",
    "        return res\n",
    "\n",
    "# ============================================\n",
    "# SHAP TIME-RESOLVED\n",
    "# ============================================\n",
    "\n",
    "def load_shap_tables() -> Dict[int, pd.DataFrame]:\n",
    "    d = {}\n",
    "    for w in TIME_WINDOWS:\n",
    "        p = BASE_DIR / f\"shap/feature_importance_w{w}.csv\"\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            # ensure sorted by importance desc\n",
    "            if \"importance\" in df.columns:\n",
    "                df = df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "            d[w] = df\n",
    "    return d\n",
    "\n",
    "def shap_time_heatmap(shap_dict: Dict[int, pd.DataFrame], top_n: int = 15):\n",
    "    # collect features in top_n across windows\n",
    "    feats = set()\n",
    "    for df in shap_dict.values():\n",
    "        feats.update(df.head(top_n)[\"feature\"].tolist())\n",
    "    feats = sorted(feats)\n",
    "    windows = [w for w in TIME_WINDOWS if w in shap_dict]\n",
    "\n",
    "    M = pd.DataFrame(0.0, index=feats, columns=windows)\n",
    "    for w in windows:\n",
    "        df = shap_dict[w]\n",
    "        for _, r in df.iterrows():\n",
    "            f = r[\"feature\"]\n",
    "            if f in M.index:\n",
    "                val = r[\"importance_normalized\"] if \"importance_normalized\" in df.columns else r[\"importance\"]\n",
    "                M.loc[f, w] = float(val)\n",
    "    fig, ax = plt.subplots(figsize=(max(10, len(feats)*0.5), 8))\n",
    "    sns.heatmap(M.T, cmap=\"YlOrRd\", annot=True, fmt=\".2f\",\n",
    "                cbar_kws={\"label\":\"Normalized SHAP Importance\"}, vmin=0, vmax=M.values.max(),\n",
    "                linewidths=0.5, linecolor=\"gray\", ax=ax)\n",
    "    ax.set_xlabel(\"Features\"); ax.set_ylabel(\"Time Window (h)\")\n",
    "    ax.set_title(\"Feature Importance Evolution Over Time\", fontweight=\"bold\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    return fig, M\n",
    "\n",
    "# ============================================\n",
    "# CLINICAL INTERPRETATION NOTES\n",
    "# ============================================\n",
    "\n",
    "def clinical_notes(shap_dict: Dict[int, pd.DataFrame], subgrp: Dict[int, Dict]) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"=== TEMPORAL PATTERNS ===\")\n",
    "    for w in [6, 24]:\n",
    "        if w in shap_dict:\n",
    "            top = \", \".join(shap_dict[w].head(5)[\"feature\"].tolist())\n",
    "            phase = \"acute phase\" if w == 6 else \"later/stabilization phase\"\n",
    "            lines.append(f\"{w}-hour window ({phase}) â€” Top predictors: {top}\")\n",
    "\n",
    "    # lactate & age shifts (best-effort)\n",
    "    def get_imp(w, key):\n",
    "        if w not in shap_dict: return 0.0\n",
    "        s = shap_dict[w]\n",
    "        s = s.copy()\n",
    "        s[\"lkey\"] = s[\"feature\"].str.lower()\n",
    "        hit = s[s[\"lkey\"].str.contains(key)]\n",
    "        if hit.empty: return 0.0\n",
    "        col = \"importance_normalized\" if \"importance_normalized\" in s.columns else \"importance\"\n",
    "        return float(hit.iloc[0][col])\n",
    "\n",
    "    lact6, lact24 = get_imp(6, \"lactate\"), get_imp(24, \"lactate\")\n",
    "    age6, age24   = get_imp(6, \"age\"),     get_imp(24, \"age\")\n",
    "    lines.append(\"\\n=== FEATURE TRANSITIONS ===\")\n",
    "    if lact6 > 1.5 * max(1e-12, lact24):\n",
    "        lines.append(\"â€¢ Lactate: higher importance early â†’ lower later (emphasizes early resuscitation/clearance).\")\n",
    "    if age24 > 1.5 * max(1e-12, age6):\n",
    "        lines.append(\"â€¢ Age: lower early â†’ higher later (physiologic reserve matters over time).\")\n",
    "\n",
    "    lines.append(\"\\n=== SUBGROUP INSIGHTS ===\")\n",
    "    if 12 in subgrp and \"age\" in subgrp[12]:\n",
    "        a = subgrp[12][\"age\"]\n",
    "        if \"<65\" in a and \"â‰¥65\" in a and a[\"<65\"][\"auroc\"] and a[\"â‰¥65\"][\"auroc\"]:\n",
    "            d = a[\"<65\"][\"auroc\"] - a[\"â‰¥65\"][\"auroc\"]\n",
    "            lines.append(f\"12h: AUC( <65 ) - AUC( â‰¥65 ) = {d:.03f} (review calibration if |Î”|>0.05).\")\n",
    "\n",
    "    lines.append(\"\\n=== MODEL RELIABILITY ===\")\n",
    "    if 12 in subgrp and \"age\" in subgrp[12]:\n",
    "        for g, m in subgrp[12][\"age\"].items():\n",
    "            slope = m.get(\"calibration_slope\", np.nan)\n",
    "            if not np.isnan(slope):\n",
    "                qual = \"Good\" if 0.8 < slope < 1.2 else (\"Moderate\" if 0.6 < slope < 1.4 else \"Poor\")\n",
    "                lines.append(f\"12h {g}: calibration slope {slope:.2f} ({qual}).\")\n",
    "\n",
    "    lines.append(\"\\n=== ACTIONABILITY (EARLY WINDOWS) ===\")\n",
    "    if 6 in shap_dict:\n",
    "        mods = []\n",
    "        for f in shap_dict[6].head(10)[\"feature\"]:\n",
    "            fl = f.lower()\n",
    "            if any(k in fl for k in [\"lactate\",\"hr_\",\"sbp_\",\"map\",\"rr_\",\"spo2\",\"urine\"]):\n",
    "                mods.append(f)\n",
    "        if mods:\n",
    "            lines.append(\"Focus modifiable signals at 6h: \" + \", \".join(mods[:3]))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ============================================\n",
    "# MAIN\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING DATA AND MODELS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load cohorts (for demographics) and features\n",
    "    mimic_cohort_path = COHORT_DIR / \"mimic_mi_cohort.csv\"\n",
    "    eicu_cohort_path  = COHORT_DIR / \"eicu_mi_cohort.csv\"\n",
    "    mimic_feat_path   = FEAT_DIR   / \"mimic_features.csv\"\n",
    "    eicu_feat_path    = FEAT_DIR   / \"eicu_features.csv\"\n",
    "\n",
    "    if not eicu_feat_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing eICU features: {eicu_feat_path}\")\n",
    "    if not eicu_cohort_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing eICU cohort (for demographics): {eicu_cohort_path}\")\n",
    "\n",
    "    mimic_cohort = pd.read_csv(mimic_cohort_path) if mimic_cohort_path.exists() else None\n",
    "    eicu_cohort  = pd.read_csv(eicu_cohort_path)\n",
    "    mimic_feat   = pd.read_csv(mimic_feat_path) if mimic_feat_path.exists() else None\n",
    "    eicu_feat    = pd.read_csv(eicu_feat_path)\n",
    "\n",
    "    # Harmonize keys & demographics\n",
    "    eicu_cohort = eicu_cohort.rename(columns={\"patientunitstayid\":\"stay_id\"})\n",
    "    # choose best-available age column name\n",
    "    if \"age\" not in eicu_cohort.columns:\n",
    "        if \"age_numeric\" in eicu_cohort.columns:\n",
    "            eicu_cohort[\"age\"] = eicu_cohort[\"age_numeric\"]\n",
    "        elif \"age_years\" in eicu_cohort.columns:\n",
    "            eicu_cohort[\"age\"] = eicu_cohort[\"age_years\"]\n",
    "\n",
    "    eicu = eicu_feat.merge(\n",
    "        eicu_cohort[[\"stay_id\",\"gender\",\"age\"]].copy(),\n",
    "        on=\"stay_id\", how=\"left\"\n",
    "    )\n",
    "    # Standardize gender to Male/Female/Unknown + binary\n",
    "    eicu[\"gender_norm\"] = normalize_sex(eicu[\"gender\"])\n",
    "    eicu[\"gender_binary\"] = (eicu[\"gender_norm\"] == \"Female\").astype(int)\n",
    "\n",
    "    print(f\"  eICU merged rows: {len(eicu):,}\")\n",
    "\n",
    "    # Storage\n",
    "    all_subgroup_results: Dict[int, Dict[str, Dict]] = {}\n",
    "    interaction_results: Dict[str, Dict[str, float]] = {}\n",
    "    shap_tables = load_shap_tables()\n",
    "\n",
    "    for w in TIME_WINDOWS:\n",
    "        print(f\"\\n{'='*80}\\nPROCESSING {w}-HOUR WINDOW\\n{'='*80}\")\n",
    "        dfw = eicu[eicu[\"window\"] == w].copy()\n",
    "        if dfw.empty:\n",
    "            print(f\"  âš ï¸ No eICU rows for {w}h; skipping\")\n",
    "            continue\n",
    "\n",
    "        # Load model bundle\n",
    "        try:\n",
    "            calibrator, imputer, feature_names, thr = load_bundle(w)\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Missing model bundle for w{w}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build X aligned to training schema (NO demographics/leakage columns)\n",
    "        drop_cols = [\"stay_id\",\"hospital_expire_flag\",\"window\",\"icu_los_hours\",\"source\",\n",
    "                     \"gender\",\"gender_norm\",\"gender_binary\",\"age\"]\n",
    "        X = dfw.drop(columns=[c for c in drop_cols if c in dfw.columns], errors=\"ignore\")\n",
    "        X = align_to_schema(X, feature_names)\n",
    "\n",
    "        # Predict calibrated probabilities\n",
    "        X_imp = imputer.transform(X)\n",
    "        y_prob = calibrator.predict_proba(X_imp)[:, 1]\n",
    "        y_true = dfw[\"hospital_expire_flag\"].values.astype(int)\n",
    "\n",
    "        # Overall metrics\n",
    "        overall = compute_core_metrics(y_true, y_prob, thr)\n",
    "        print(f\"  eICU overall â€” n={overall['n']}, AUROC={overall['auroc']:.3f}, \"\n",
    "              f\"AUPRC={overall['auprc']:.3f}, Brier={overall['brier']:.3f}, ECE={overall['ece']:.3f}\")\n",
    "\n",
    "        # Subgroups\n",
    "        sub = {}\n",
    "\n",
    "        # Age subgroups (if available)\n",
    "        if \"age\" in dfw.columns and dfw[\"age\"].notna().sum() > 0:\n",
    "            sub[\"age\"] = subgroup_metrics(dfw, y_true, y_prob, thr, \"age\")\n",
    "            if sub[\"age\"]:\n",
    "                a0 = sub[\"age\"].get(\"<65\",{}).get(\"auroc\", np.nan)\n",
    "                a1 = sub[\"age\"].get(\"â‰¥65\",{}).get(\"auroc\", np.nan)\n",
    "                if not np.isnan(a0) and not np.isnan(a1):\n",
    "                    print(f\"    Age <65 AUROC={a0:.3f} | â‰¥65 AUROC={a1:.3f}\")\n",
    "        else:\n",
    "            print(\"    â€¢ Age not available; skipping age subgroup.\")\n",
    "\n",
    "        # Sex/Gender subgroups\n",
    "        if \"gender_norm\" in dfw.columns:\n",
    "            sub[\"gender\"] = subgroup_metrics(dfw.assign(gender_label=dfw[\"gender_norm\"]),\n",
    "                                             y_true, y_prob, thr, \"gender_label\")\n",
    "            for g in [\"Male\",\"Female\"]:\n",
    "                if g in sub[\"gender\"]:\n",
    "                    print(f\"    {g}: AUROC={sub['gender'][g]['auroc']:.3f}\")\n",
    "        else:\n",
    "            print(\"    â€¢ Gender not available; skipping gender subgroup.\")\n",
    "\n",
    "        # Ethnicity (if present)\n",
    "        if \"ethnicity\" in dfw.columns:\n",
    "            dfw[\"_eth\"] = compress_ethnicity(dfw[\"ethnicity\"])\n",
    "            sub[\"ethnicity\"] = subgroup_metrics(dfw.assign(eth=dfw[\"_eth\"]), y_true, y_prob, thr, \"eth\")\n",
    "        else:\n",
    "            print(\"    â€¢ Ethnicity not available; skipping ethnicity subgroup.\")\n",
    "\n",
    "        all_subgroup_results[w] = sub\n",
    "\n",
    "        # Interaction tests: predicted risk Ã— subgroup\n",
    "        print(\"  Interaction tests (predicted risk Ã— subgroup):\")\n",
    "        # Age Ã— risk\n",
    "        if \"age\" in dfw.columns and dfw[\"age\"].notna().sum() > 0:\n",
    "            age_grp = age_bin(dfw[\"age\"])\n",
    "            age_inter = interaction_test_pred_x_group(y_true, y_prob, age_grp)\n",
    "            print(f\"    Risk Ã— Age: method={age_inter['method']}, p={age_inter['p_value']:.3f}\")\n",
    "            interaction_results[f\"{w}_risk_x_age\"] = age_inter\n",
    "        # Sex Ã— risk\n",
    "        if \"gender_norm\" in dfw.columns:\n",
    "            sex_inter = interaction_test_pred_x_group(y_true, y_prob, dfw[\"gender_norm\"])\n",
    "            print(f\"    Risk Ã— Sex: method={sex_inter['method']}, p={sex_inter['p_value']:.3f}\")\n",
    "            interaction_results[f\"{w}_risk_x_sex\"] = sex_inter\n",
    "\n",
    "    # ============================================\n",
    "    # VISUALIZATIONS\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1) SHAP time-resolved heatmap\n",
    "    if shap_tables:\n",
    "        fig, M = shap_time_heatmap(shap_tables, top_n=15)\n",
    "        fig.savefig(OUT_DIR / \"shap_time_evolution_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"  âœ“ Saved: shap_time_evolution_heatmap.png\")\n",
    "\n",
    "    # 2) Subgroup AUROC barplots (Age & Sex) across windows\n",
    "    def barplot_subgroup(metric_key: str, grp_name: str, levels: List[str], fname: str, ylim=(0.7, 0.95)):\n",
    "        data = {\"window\": [], \"group\": [], \"auroc\": []}\n",
    "        for w in TIME_WINDOWS:\n",
    "            if w in all_subgroup_results and grp_name in all_subgroup_results[w]:\n",
    "                for g in levels:\n",
    "                    if g in all_subgroup_results[w][grp_name]:\n",
    "                        data[\"window\"].append(f\"{w}h\")\n",
    "                        data[\"group\"].append(g)\n",
    "                        data[\"auroc\"].append(all_subgroup_results[w][grp_name][g][\"auroc\"])\n",
    "        dfp = pd.DataFrame(data)\n",
    "        if dfp.empty: return\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.barplot(data=dfp, x=\"window\", y=\"auroc\", hue=\"group\")\n",
    "        plt.ylim(*ylim); plt.title(f\"eICU â€” AUROC by {grp_name.capitalize()}\"); plt.xlabel(\"Window\"); plt.ylabel(\"AUROC\")\n",
    "        plt.tight_layout(); plt.savefig(OUT_DIR / f\"figures\" / fname, dpi=250, bbox_inches=\"tight\"); plt.close()\n",
    "        print(f\"  âœ“ Saved: figures/{fname}\")\n",
    "\n",
    "    barplot_subgroup(\"auroc\", \"age\", [\"<65\",\"â‰¥65\"], \"eicu_subgroup_auroc_age.png\")\n",
    "    barplot_subgroup(\"auroc\", \"gender\", [\"Male\",\"Female\"], \"eicu_subgroup_auroc_gender.png\")\n",
    "\n",
    "    # 3) Feature ranking evolution lines (top-10 per window)\n",
    "    if shap_tables:\n",
    "        feats = set()\n",
    "        for df in shap_tables.values():\n",
    "            feats.update(df.head(10)[\"feature\"].tolist())\n",
    "        feats = list(feats)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        for f in feats:\n",
    "            xs, ys = [], []\n",
    "            for w in TIME_WINDOWS:\n",
    "                if w in shap_tables:\n",
    "                    df = shap_tables[w].reset_index(drop=True)\n",
    "                    pos = df.index[df[\"feature\"] == f]\n",
    "                    if len(pos) > 0:\n",
    "                        xs.append(w); ys.append(int(pos[0]) + 1)\n",
    "            if len(xs) > 1:\n",
    "                plt.plot(xs, ys, marker=\"o\", linewidth=2, alpha=0.7, label=f)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xticks(TIME_WINDOWS, [f\"{w}h\" for w in TIME_WINDOWS])\n",
    "        plt.xlabel(\"Prediction Window (h)\"); plt.ylabel(\"Feature Rank (1 = most important)\")\n",
    "        plt.title(\"Feature Importance Ranking Evolution\", fontweight=\"bold\")\n",
    "        plt.legend(bbox_to_anchor=(1.03, 1), loc=\"upper left\", fontsize=8)\n",
    "        plt.tight_layout(); plt.savefig(OUT_DIR / \"feature_ranking_evolution.png\", dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "        print(\"  âœ“ Saved: feature_ranking_evolution.png\")\n",
    "\n",
    "    # ============================================\n",
    "    # CLINICAL INTERPRETATION + TABLES\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLINICAL INTERPRETATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    notes = clinical_notes(shap_tables, all_subgroup_results)\n",
    "    print(notes)\n",
    "    with open(OUT_DIR / \"clinical_interpretation.txt\", \"w\") as f:\n",
    "        f.write(notes)\n",
    "    print(\"  âœ“ Saved: clinical_interpretation.txt\")\n",
    "\n",
    "    # Subgroup performance table\n",
    "    rows = []\n",
    "    for w, d in all_subgroup_results.items():\n",
    "        for grp, res in d.items():\n",
    "            for lvl, met in res.items():\n",
    "                rows.append({\n",
    "                    \"Window\": f\"{w}h\",\n",
    "                    \"Subgroup Type\": grp,\n",
    "                    \"Subgroup\": lvl,\n",
    "                    \"N\": met[\"n\"],\n",
    "                    \"Prevalence\": met[\"prevalence\"],\n",
    "                    \"AUROC\": met[\"auroc\"],\n",
    "                    \"AUPRC\": met[\"auprc\"],\n",
    "                    \"Brier\": met[\"brier\"],\n",
    "                    \"ECE\": met.get(\"ece\", np.nan),\n",
    "                    \"Accuracy\": met[\"accuracy\"],\n",
    "                    \"F1\": met[\"f1\"],\n",
    "                    \"Calibration Slope\": met.get(\"calibration_slope\", np.nan),\n",
    "                })\n",
    "    if rows:\n",
    "        sub_df = pd.DataFrame(rows)\n",
    "        sub_df.to_csv(OUT_DIR / \"subgroup_performance.csv\", index=False)\n",
    "        print(\"  âœ“ Saved: subgroup_performance.csv\")\n",
    "        print(\"\\n  Subgroup Performance (12h preview):\")\n",
    "        print(sub_df[sub_df[\"Window\"] == \"12h\"].round(3))\n",
    "\n",
    "    # Interaction table\n",
    "    if interaction_results:\n",
    "        it = []\n",
    "        for k, v in interaction_results.items():\n",
    "            w, rest = k.split(\"_\", 1)\n",
    "            it.append({\"Window\": f\"{w}h\", \"Interaction\": rest.replace(\"risk_x_\", \"risk Ã— \"),\n",
    "                       \"Method\": v.get(\"method\",\"\"), \"Coef/Î”\": v.get(\"coef\", np.nan),\n",
    "                       \"p_value\": v.get(\"p_value\", np.nan)})\n",
    "        it_df = pd.DataFrame(it)\n",
    "        it_df.to_csv(OUT_DIR / \"interaction_effects.csv\", index=False)\n",
    "        print(\"  âœ“ Saved: interaction_effects.csv\")\n",
    "        print(\"\\n  Interaction Effects (preview):\")\n",
    "        print(it_df.round(3))\n",
    "\n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Best subgroup by AUROC\n",
    "    best = None\n",
    "    if rows:\n",
    "        tmp = pd.DataFrame(rows).dropna(subset=[\"AUROC\"])\n",
    "        if not tmp.empty:\n",
    "            idx = tmp[\"AUROC\"].idxmax()\n",
    "            best = tmp.loc[idx]\n",
    "            print(f\"  â€¢ Best subgroup: {best['Subgroup Type']}={best['Subgroup']} at {best['Window']} (AUROC {best['AUROC']:.3f})\")\n",
    "\n",
    "    # Quick flags for Î”AUC across age/sex\n",
    "    for w in TIME_WINDOWS:\n",
    "        if w in all_subgroup_results:\n",
    "            for gv in (\"age\",\"gender\"):\n",
    "                g = all_subgroup_results[w].get(gv, {})\n",
    "                if g:\n",
    "                    vals = [m[\"AUROC\"] for m in g.values() if not np.isnan(m[\"auroc\"]) for m in [ {\"AUROC\": m[\"auroc\"]} ]]\n",
    "                    if len(vals) >= 2 and (max(vals) - min(vals)) > 0.05:\n",
    "                        print(f\"  â€¢ {w}h {gv}: Î”AUROC > 0.05 across levels â€” consider subgroup calibration check.\")\n",
    "\n",
    "    print(\"\\nðŸ“ Outputs:\")\n",
    "    print(f\"  - {OUT_DIR}/clinical_interpretation.txt\")\n",
    "    print(f\"  - {OUT_DIR}/subgroup_performance.csv\")\n",
    "    print(f\"  - {OUT_DIR}/interaction_effects.csv\")\n",
    "    print(f\"  - {OUT_DIR}/shap_time_evolution_heatmap.png\")\n",
    "    print(f\"  - {OUT_DIR}/feature_ranking_evolution.png\")\n",
    "    print(f\"  - {OUT_DIR}/figures/eicu_subgroup_auroc_age.png\")\n",
    "    print(f\"  - {OUT_DIR}/figures/eicu_subgroup_auroc_gender.png\")\n",
    "\n",
    "    return all_subgroup_results, shap_tables, notes\n",
    "\n",
    "# ============================================\n",
    "# EXECUTE\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subgroups, shap_data, clinical_notes_txt = main()\n",
    "    print(\"\\nâœ… Interpretability Analysis COMPLETE!\")\n",
    "    print(\"Next: API deployment (FastAPI + Streamlit).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ICU API (Windows/Notebook safe) â€” dynamic port & Pydantic v2\n",
    "# =========================================================\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import traceback\n",
    "import threading\n",
    "import asyncio\n",
    "import socket\n",
    "import os\n",
    "from contextlib import asynccontextmanager\n",
    "from datetime import datetime, date\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, File, HTTPException, Request, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "\n",
    "# ---------------- Logging ----------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"Sentinel-icu-api\")\n",
    "\n",
    "# ---------------- Config -----------------\n",
    "class Config:\n",
    "    MODEL_DIR = Path(\"outputs/external_validation/models\")\n",
    "    SHAP_DIR = Path(\"outputs/external_validation/shap\")\n",
    "    RESULTS_DIR = Path(\"outputs/external_validation/results\")\n",
    "    INTERP_DIR = Path(\"outputs/external_validation/interpretability\")\n",
    "    ALLOWED_WINDOWS = [6, 12, 18, 24]\n",
    "    MAX_BATCH_SIZE = 100\n",
    "    API_VERSION = \"1.0.0\"\n",
    "    MANIFEST_FILE = MODEL_DIR / \"version.json\"\n",
    "\n",
    "    # FHIR mapping\n",
    "    LOINC_MAPPINGS = {\n",
    "        \"8867-4\": [\"hr_mean\", \"hr_max\", \"hr_min\"],      # Heart rate\n",
    "        \"8480-6\": [\"sbp_mean\", \"sbp_min\"],              # Systolic BP\n",
    "        \"9279-1\": [\"rr_mean\", \"rr_max\"],                # Resp. rate\n",
    "        \"2708-6\": [\"spo2_mean\", \"spo2_min\"],            # SpO2\n",
    "        \"2524-7\": [\"lactate_max\"],\n",
    "        \"2160-0\": [\"creat_max\"],\n",
    "        \"6598-7\": [\"troponin_max\"],\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# -------------- Pydantic v2 models ---------------\n",
    "class RiskCategory(str, Enum):\n",
    "    LOW = \"Low\"\n",
    "    MODERATE = \"Moderate\"\n",
    "    HIGH = \"High\"\n",
    "    VERY_HIGH = \"Very High\"\n",
    "\n",
    "class PatientFeatures(BaseModel):\n",
    "    age_years: float = Field(..., ge=18, le=120)\n",
    "    hr_mean: float = Field(..., ge=20, le=220)\n",
    "    hr_max: float = Field(..., ge=20, le=220)\n",
    "    hr_min: float = Field(..., ge=20, le=220)\n",
    "    sbp_mean: float = Field(..., ge=40, le=250)\n",
    "    sbp_min: float = Field(..., ge=40, le=250)\n",
    "    rr_mean: float = Field(..., ge=5, le=60)\n",
    "    rr_max: float = Field(..., ge=5, le=60)\n",
    "    spo2_mean: float = Field(..., ge=50, le=100)\n",
    "    spo2_min: float = Field(..., ge=50, le=100)\n",
    "    lactate_max: float = Field(..., ge=0, le=30)\n",
    "    creat_max: float = Field(..., ge=0, le=20)\n",
    "    troponin_max: float = Field(..., ge=0, le=1000)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def _check_hr_bounds(self):\n",
    "        if self.hr_max < self.hr_mean:\n",
    "            raise ValueError(\"hr_max must be >= hr_mean\")\n",
    "        if self.hr_min > self.hr_mean:\n",
    "            raise ValueError(\"hr_min must be <= hr_mean\")\n",
    "        return self\n",
    "\n",
    "class FHIRObservation(BaseModel):\n",
    "    resourceType: str = \"Observation\"\n",
    "    code: Dict[str, Any]\n",
    "    valueQuantity: Optional[Dict[str, Any]] = None\n",
    "    valueString: Optional[str] = None\n",
    "    effectiveDateTime: Optional[str] = None\n",
    "    status: str = \"final\"\n",
    "\n",
    "class FHIRPatient(BaseModel):\n",
    "    resourceType: str = \"Patient\"\n",
    "    birthDate: Optional[str] = None\n",
    "\n",
    "class FHIRBundle(BaseModel):\n",
    "    resourceType: str = \"Bundle\"\n",
    "    type: str = \"collection\"\n",
    "    entry: List[Dict[str, Any]]\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: Optional[PatientFeatures] = None\n",
    "    fhir_bundle: Optional[FHIRBundle] = None\n",
    "    window: int = Field(12, description=\"Prediction window in hours\")\n",
    "    include_shap: bool = False\n",
    "\n",
    "    @field_validator(\"window\")\n",
    "    @classmethod\n",
    "    def _window_allowed(cls, v: int) -> int:\n",
    "        if v not in config.ALLOWED_WINDOWS:\n",
    "            raise ValueError(f\"Window must be one of {config.ALLOWED_WINDOWS}\")\n",
    "        return v\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    patient_id: Optional[str] = None\n",
    "    window: int\n",
    "    probability: float\n",
    "    risk_category: RiskCategory\n",
    "    confidence_interval: Dict[str, float]\n",
    "    threshold: float\n",
    "    features_used: int\n",
    "    missing_features: List[str]\n",
    "    model_version: str\n",
    "    timestamp: str\n",
    "    shap_values: Optional[Dict[str, float]] = None\n",
    "\n",
    "class BatchPredictionRequest(BaseModel):\n",
    "    patients: List[PatientFeatures]\n",
    "    window: int = 12\n",
    "    include_shap: bool = False\n",
    "\n",
    "    @field_validator(\"patients\")\n",
    "    @classmethod\n",
    "    def _batch_size_ok(cls, v: List[PatientFeatures]) -> List[PatientFeatures]:\n",
    "        if len(v) > config.MAX_BATCH_SIZE:\n",
    "            raise ValueError(f\"Batch size cannot exceed {config.MAX_BATCH_SIZE}\")\n",
    "        return v\n",
    "\n",
    "class BatchPredictionResponse(BaseModel):\n",
    "    window: int\n",
    "    total_patients: int\n",
    "    timestamp: str\n",
    "    predictions: List[Dict[str, Any]]\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    api_version: str\n",
    "    models_loaded: Dict[int, bool]\n",
    "    uptime_seconds: float\n",
    "    last_prediction: Optional[str]\n",
    "    total_predictions: int\n",
    "    commit: Optional[str] = None\n",
    "\n",
    "# -------------- Model manager --------------------\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.models: Dict[int, Optional[Dict[str, Any]]] = {}\n",
    "        self.feature_names: Dict[int, List[str]] = {}\n",
    "        self.shap_explainers: Dict[int, Optional[shap.Explainer]] = {}\n",
    "        self.prediction_count = 0\n",
    "        self.last_prediction_time: Optional[datetime] = None\n",
    "        self.start_time = datetime.now()\n",
    "        self.manifest = self._load_manifest()\n",
    "\n",
    "    def _load_manifest(self) -> Dict[str, Any]:\n",
    "        if config.MANIFEST_FILE.exists():\n",
    "            try:\n",
    "                return json.loads(config.MANIFEST_FILE.read_text())\n",
    "            except Exception:\n",
    "                pass\n",
    "        return {\"version\": config.API_VERSION, \"created\": datetime.now().isoformat()}\n",
    "\n",
    "    def load_models(self):\n",
    "        logger.info(\"Loading models...\")\n",
    "        for window in config.ALLOWED_WINDOWS:\n",
    "            try:\n",
    "                model_path = config.MODEL_DIR / f\"xgboost_w{window}.pkl\"\n",
    "                cal_path = config.MODEL_DIR / f\"calibrator_w{window}.pkl\"\n",
    "                imp_path = config.MODEL_DIR / f\"imputer_w{window}.pkl\"\n",
    "                feat_path = config.MODEL_DIR / f\"features_w{window}.txt\"\n",
    "                thr_path = config.MODEL_DIR / f\"threshold_w{window}.csv\"\n",
    "\n",
    "                if not (model_path.exists() and cal_path.exists() and imp_path.exists() and feat_path.exists()):\n",
    "                    raise FileNotFoundError(\"One or more required artifacts missing\")\n",
    "\n",
    "                with open(model_path, \"rb\") as f:\n",
    "                    model = pickle.load(f)\n",
    "                with open(cal_path, \"rb\") as f:\n",
    "                    calibrator = pickle.load(f)\n",
    "                with open(imp_path, \"rb\") as f:\n",
    "                    imputer = pickle.load(f)\n",
    "                features = [line.strip() for line in feat_path.read_text().splitlines() if line.strip()]\n",
    "\n",
    "                threshold = 0.5\n",
    "                if thr_path.exists():\n",
    "                    df_thr = pd.read_csv(thr_path)\n",
    "                    if \"threshold\" in df_thr.columns:\n",
    "                        threshold = float(df_thr[\"threshold\"].iloc[0])\n",
    "\n",
    "                self.models[window] = {\n",
    "                    \"model\": model,\n",
    "                    \"calibrator\": calibrator,\n",
    "                    \"imputer\": imputer,\n",
    "                    \"threshold\": threshold,\n",
    "                }\n",
    "                self.feature_names[window] = features\n",
    "\n",
    "                try:\n",
    "                    self.shap_explainers[window] = shap.TreeExplainer(model)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"SHAP explainer init failed for {window}h: {e}\")\n",
    "                    self.shap_explainers[window] = None\n",
    "\n",
    "                logger.info(f\"Loaded artifacts for {window}h (features={len(features)})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load window {window}h: {e}\")\n",
    "                self.models[window] = None\n",
    "                self.feature_names[window] = []\n",
    "                self.shap_explainers[window] = None\n",
    "\n",
    "    def _align(self, df: pd.DataFrame, window: int) -> (pd.DataFrame, List[str]):\n",
    "        expected = self.feature_names[window]\n",
    "        missing = [f for f in expected if f not in df.columns]\n",
    "        for col in missing:\n",
    "            df[col] = np.nan\n",
    "        return df[expected], missing\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, window: int, include_shap: bool = False) -> Dict[str, Any]:\n",
    "        if window not in self.models or self.models[window] is None:\n",
    "            raise HTTPException(status_code=503, detail=f\"Model for {window}h not available\")\n",
    "        mdl = self.models[window]\n",
    "\n",
    "        self.prediction_count += 1\n",
    "        self.last_prediction_time = datetime.now()\n",
    "\n",
    "        X, missing_features = self._align(df.copy(), window)\n",
    "        X_imp = mdl[\"imputer\"].transform(X)\n",
    "        probs = mdl[\"calibrator\"].predict_proba(X_imp)[:, 1]\n",
    "\n",
    "        shap_values = None\n",
    "        if include_shap:\n",
    "            explainer = self.shap_explainers.get(window)\n",
    "            if explainer is not None:\n",
    "                try:\n",
    "                    vals = explainer.shap_values(X_imp[:1])\n",
    "                    if isinstance(vals, list):\n",
    "                        vals = vals[1]\n",
    "                    shap_values = {feat: float(val) for feat, val in zip(self.feature_names[window], np.array(vals)[0])}\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"SHAP calculation failed: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"probabilities\": probs,\n",
    "            \"threshold\": float(mdl[\"threshold\"]),\n",
    "            \"missing_features\": missing_features,\n",
    "            \"shap_values\": shap_values,\n",
    "        }\n",
    "\n",
    "    def commit_hash(self) -> Optional[str]:\n",
    "        return self.manifest.get(\"commit\")\n",
    "\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# -------------- Lifespan --------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    logger.info(\"Starting Sentinel-ICU APIâ€¦\")\n",
    "    try:\n",
    "        model_manager.load_models()\n",
    "        logger.info(\"Models loaded. API ready.\")\n",
    "        logger.info(f\"API Version: {config.API_VERSION}; Windows: {config.ALLOWED_WINDOWS}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Startup error: {e}\")\n",
    "    yield\n",
    "    logger.info(\"Shutting down Sentinel-ICU API...\")\n",
    "\n",
    "# -------------- App -------------------------------\n",
    "tags_metadata = [\n",
    "    {\"name\": \"General\", \"description\": \"Info, health, version\"},\n",
    "    {\"name\": \"Prediction\", \"description\": \"Single/batch predictions\"},\n",
    "    {\"name\": \"Explainability\", \"description\": \"Global & per-prediction explanations\"},\n",
    "    {\"name\": \"FHIR\", \"description\": \"FHIR R4 compatible endpoints\"},\n",
    "    {\"name\": \"Metrics\", \"description\": \"Performance & model card\"},\n",
    "]\n",
    "app = FastAPI(\n",
    "    title=\"Sentinel-ICU Mortality Prediction API\",\n",
    "    description=\"Externally validated ICU mortality risk (6/12/18/24 h), with guardrail thresholds, silent-trial mode, and HL7Â® FHIRÂ® R4 integration\",\n",
    "    version=config.API_VERSION,\n",
    "    openapi_tags=tags_metadata,\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # tighten in prod\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# -------------- Utils -----------------------------\n",
    "def _parse_birthdate_to_age(birth_date: str) -> Optional[float]:\n",
    "    try:\n",
    "        bd = date.fromisoformat(birth_date[:10])\n",
    "        today = date.today()\n",
    "        years = (today - bd).days / 365.25\n",
    "        return float(np.clip(years, 0, 150))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fhir_to_features(bundle: FHIRBundle) -> PatientFeatures:\n",
    "    features: Dict[str, Any] = {}\n",
    "\n",
    "    # Patient age\n",
    "    for entry in bundle.entry:\n",
    "        res = entry.get(\"resource\", {})\n",
    "        if res.get(\"resourceType\") == \"Patient\":\n",
    "            age = _parse_birthdate_to_age(res.get(\"birthDate\", \"\"))\n",
    "            if age is not None:\n",
    "                features[\"age_years\"] = age\n",
    "            break\n",
    "\n",
    "    # Observations\n",
    "    for entry in bundle.entry:\n",
    "        res = entry.get(\"resource\", {})\n",
    "        if res.get(\"resourceType\") != \"Observation\":\n",
    "            continue\n",
    "\n",
    "        code = res.get(\"code\", {})\n",
    "        codings = code.get(\"coding\", []) or []\n",
    "        code_text = (code.get(\"text\") or \"\").lower()\n",
    "\n",
    "        loinc = None\n",
    "        for c in codings:\n",
    "            if c.get(\"system\", \"\").endswith(\"loinc.org\") or \"loinc\" in (c.get(\"system\", \"\").lower()):\n",
    "                loinc = c.get(\"code\")\n",
    "                break\n",
    "            loinc = loinc or c.get(\"code\")\n",
    "\n",
    "        if not loinc:\n",
    "            continue\n",
    "\n",
    "        candidate_feats = config.LOINC_MAPPINGS.get(loinc, [])\n",
    "        if not candidate_feats:\n",
    "            continue\n",
    "\n",
    "        vq = res.get(\"valueQuantity\", {})\n",
    "        value = vq.get(\"value\")\n",
    "        if value is None and isinstance(res.get(\"valueString\"), str):\n",
    "            try:\n",
    "                value = float(res[\"valueString\"])\n",
    "            except Exception:\n",
    "                value = None\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        target = None\n",
    "        if len(candidate_feats) == 1:\n",
    "            target = candidate_feats[0]\n",
    "        else:\n",
    "            if \"mean\" in code_text:\n",
    "                fs = [f for f in candidate_feats if f.endswith(\"_mean\")]\n",
    "                target = fs[0] if fs else None\n",
    "            if target is None and \"min\" in code_text:\n",
    "                fs = [f for f in candidate_feats if f.endswith(\"_min\")]\n",
    "                target = fs[0] if fs else None\n",
    "            if target is None and \"max\" in code_text:\n",
    "                fs = [f for f in candidate_feats if f.endswith(\"_max\")]\n",
    "                target = fs[0] if fs else None\n",
    "            if target is None:\n",
    "                target = candidate_feats[0]\n",
    "\n",
    "        features[target] = float(value)\n",
    "\n",
    "    # Defaults\n",
    "    defaults = {\n",
    "        \"age_years\": 65,\n",
    "        \"hr_mean\": 80, \"hr_max\": 90, \"hr_min\": 70,\n",
    "        \"sbp_mean\": 120, \"sbp_min\": 100,\n",
    "        \"rr_mean\": 18, \"rr_max\": 22,\n",
    "        \"spo2_mean\": 95, \"spo2_min\": 92,\n",
    "        \"lactate_max\": 1.5, \"creat_max\": 1.0, \"troponin_max\": 0.5,\n",
    "    }\n",
    "    for k, v in defaults.items():\n",
    "        features.setdefault(k, v)\n",
    "\n",
    "    return PatientFeatures(**features)\n",
    "\n",
    "def calculate_risk_category(p: float, threshold: float) -> RiskCategory:\n",
    "    if p < 0.10:\n",
    "        return RiskCategory.LOW\n",
    "    if p < 0.30:\n",
    "        return RiskCategory.MODERATE\n",
    "    if p < threshold:\n",
    "        return RiskCategory.HIGH\n",
    "    return RiskCategory.VERY_HIGH\n",
    "\n",
    "def wilson_ci(probability: float, n: int = 1000, z: float = 1.96) -> Dict[str, float]:\n",
    "    p = float(probability)\n",
    "    denom = 1 + z**2 / n\n",
    "    center = (p + z**2 / (2 * n)) / denom\n",
    "    margin = z * np.sqrt((p * (1 - p) / n) + (z**2 / (4 * n**2))) / denom\n",
    "    return {\"lower\": max(0.0, center - margin), \"upper\": min(1.0, center + margin)}\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def _in_notebook_or_running_loop() -> bool:\n",
    "    try:\n",
    "        import IPython  # noqa\n",
    "        if IPython.get_ipython() is not None:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        asyncio.get_running_loop()\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "def _port_is_free(host: str, port: int) -> bool:\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.settimeout(0.2)\n",
    "        return s.connect_ex((host, port)) != 0\n",
    "\n",
    "def _choose_port(preferred: int, host: str, candidates: List[int] | None = None) -> int:\n",
    "    order = [preferred]\n",
    "    if candidates:\n",
    "        order += [p for p in candidates if p not in order]\n",
    "    order += [8000, 8888, 5050, 5001, 5002, 5003]\n",
    "    for p in order:\n",
    "        if _port_is_free(host, p):\n",
    "            return p\n",
    "    # let OS choose an ephemeral port\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((host, 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "# -------------- Routes ----------------------------\n",
    "@app.get(\"/\", tags=[\"General\"])\n",
    "async def root():\n",
    "    return {\n",
    "        \"name\": \"Sentinel-ICU API\",\n",
    "        \"version\": config.API_VERSION,\n",
    "        \"windows\": config.ALLOWED_WINDOWS,\n",
    "        \"docs\": \"/docs\",\n",
    "        \"health\": \"/health\",\n",
    "        \"predict\": \"/predict\",\n",
    "        \"batch\": \"/batch\",\n",
    "        \"explain\": \"/explain\",\n",
    "        \"model_card\": \"/model-card\",\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"General\"])\n",
    "async def health():\n",
    "    uptime = (datetime.now() - model_manager.start_time).total_seconds()\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if all(model_manager.models.get(w) for w in config.ALLOWED_WINDOWS) else \"degraded\",\n",
    "        api_version=config.API_VERSION,\n",
    "        models_loaded={w: bool(model_manager.models.get(w)) for w in config.ALLOWED_WINDOWS},\n",
    "        uptime_seconds=uptime,\n",
    "        last_prediction=model_manager.last_prediction_time.isoformat() if model_manager.last_prediction_time else None,\n",
    "        total_predictions=model_manager.prediction_count,\n",
    "        commit=model_manager.commit_hash(),\n",
    "    )\n",
    "\n",
    "@app.get(\"/version\", tags=[\"General\"])\n",
    "async def version():\n",
    "    return {\"api_version\": config.API_VERSION, \"manifest\": model_manager.manifest}\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse, tags=[\"Prediction\"])\n",
    "async def predict_single(req: PredictionRequest):\n",
    "    try:\n",
    "        if req.features is None and req.fhir_bundle is None:\n",
    "            raise HTTPException(status_code=400, detail=\"Provide either 'features' or 'fhir_bundle'\")\n",
    "        feats = req.features or fhir_to_features(req.fhir_bundle)  # type: ignore\n",
    "        df = pd.DataFrame([feats.model_dump()])\n",
    "        result = model_manager.predict(df, req.window, include_shap=req.include_shap)\n",
    "        prob = float(result[\"probabilities\"][0])\n",
    "        thr = result[\"threshold\"]\n",
    "        return PredictionResponse(\n",
    "            window=req.window,\n",
    "            probability=prob,\n",
    "            risk_category=calculate_risk_category(prob, thr),\n",
    "            confidence_interval=wilson_ci(prob),\n",
    "            threshold=thr,\n",
    "            features_used=len(model_manager.feature_names[req.window]),\n",
    "            missing_features=result[\"missing_features\"],\n",
    "            model_version=config.API_VERSION,\n",
    "            timestamp=_now_iso(),\n",
    "            shap_values=result[\"shap_values\"],\n",
    "        )\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\\n{traceback.format_exc()}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.post(\"/batch\", response_model=BatchPredictionResponse, tags=[\"Prediction\"])\n",
    "async def predict_batch(req: BatchPredictionRequest):\n",
    "    try:\n",
    "        df = pd.DataFrame([p.model_dump() for p in req.patients])\n",
    "        result = model_manager.predict(df, req.window, include_shap=False)\n",
    "        preds = [{\n",
    "            \"patient_index\": i,\n",
    "            \"probability\": float(p),\n",
    "            \"risk_category\": calculate_risk_category(float(p), result[\"threshold\"]).value,\n",
    "        } for i, p in enumerate(result[\"probabilities\"])]\n",
    "        return BatchPredictionResponse(\n",
    "            window=req.window,\n",
    "            total_patients=len(preds),\n",
    "            timestamp=_now_iso(),\n",
    "            predictions=preds,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch prediction error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Batch prediction failed\")\n",
    "\n",
    "@app.post(\"/upload_csv\", tags=[\"Prediction\"])\n",
    "async def upload_csv(file: UploadFile = File(...), window: int = 12):\n",
    "    if not file.filename.lower().endswith(\".csv\"):\n",
    "        raise HTTPException(status_code=400, detail=\"File must be CSV\")\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        result = model_manager.predict(df.copy(), window, include_shap=False)\n",
    "        df[\"mortality_probability\"] = result[\"probabilities\"]\n",
    "        df[\"risk_category\"] = [\n",
    "            calculate_risk_category(float(p), result[\"threshold\"]).value for p in result[\"probabilities\"]\n",
    "        ]\n",
    "        out = io.BytesIO()\n",
    "        df.to_csv(out, index=False)\n",
    "        out.seek(0)\n",
    "        return StreamingResponse(\n",
    "            out,\n",
    "            media_type=\"text/csv\",\n",
    "            headers={\"Content-Disposition\": f\"attachment; filename=predictions_{window}h.csv\"},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"CSV processing failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process CSV\")\n",
    "\n",
    "@app.get(\"/explain/{window}\", tags=[\"Explainability\"])\n",
    "async def global_importance(window: int):\n",
    "    if window not in config.ALLOWED_WINDOWS:\n",
    "        raise HTTPException(status_code=400, detail=f\"Window must be one of {config.ALLOWED_WINDOWS}\")\n",
    "    fi_path = config.SHAP_DIR / f\"feature_importance_w{window}.csv\"\n",
    "    if not fi_path.exists():\n",
    "        raise HTTPException(status_code=404, detail=\"Feature importance not available\")\n",
    "    df = pd.read_csv(fi_path)\n",
    "    return {\"window\": window, \"features\": df.to_dict(\"records\"), \"top_5\": df.head(5)[\"feature\"].tolist()}\n",
    "\n",
    "@app.post(\"/explain\", tags=[\"Explainability\"])\n",
    "async def explain_prediction(req: PredictionRequest):\n",
    "    req.include_shap = True\n",
    "    resp: PredictionResponse = await predict_single(req)  # type: ignore\n",
    "    if not resp.shap_values:\n",
    "        raise HTTPException(status_code=500, detail=\"SHAP calculation failed\")\n",
    "    items = sorted(resp.shap_values.items(), key=lambda kv: abs(kv[1]), reverse=True)\n",
    "    return {\n",
    "        \"window\": resp.window,\n",
    "        \"probability\": resp.probability,\n",
    "        \"risk_category\": resp.risk_category.value,\n",
    "        \"top_contributors\": [{\"feature\": k, \"impact\": v} for k, v in items[:5]],\n",
    "        \"shap_values\": resp.shap_values,\n",
    "        \"timestamp\": resp.timestamp,\n",
    "    }\n",
    "\n",
    "@app.get(\"/performance/{window}\", tags=[\"Metrics\"])\n",
    "async def get_performance(window: int):\n",
    "    if window not in config.ALLOWED_WINDOWS:\n",
    "        raise HTTPException(status_code=400, detail=f\"Window must be one of {config.ALLOWED_WINDOWS}\")\n",
    "\n",
    "    metrics_file = config.RESULTS_DIR / \"all_metrics.csv\"\n",
    "    perf: Dict[str, Dict[str, float]] = {}\n",
    "    if metrics_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_file)\n",
    "            df_w = df[df[\"window\"] == window]\n",
    "            for ds in [\"MIMIC-IV\", \"eICU\"]:\n",
    "                row = df_w[df_w[\"dataset\"] == ds]\n",
    "                if not row.empty:\n",
    "                    perf[ds] = {\n",
    "                        \"auroc\": float(row[\"auroc\"].values[0]),\n",
    "                        \"auprc\": float(row[\"auprc\"].values[0]),\n",
    "                        \"brier\": float(row[\"brier\"].values[0]),\n",
    "                        \"ece\": float(row.get(\"ece\", pd.Series([np.nan])).values[0]),\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed reading metrics CSV: {e}\")\n",
    "\n",
    "    if not perf:\n",
    "        defaults = {\n",
    "            6: {\"MIMIC-IV\": {\"auroc\": 0.912}, \"eICU\": {\"auroc\": 0.857, \"brier\": 0.077}},\n",
    "            12: {\"MIMIC-IV\": {\"auroc\": 0.929}, \"eICU\": {\"auroc\": 0.864, \"brier\": 0.076}},\n",
    "            18: {\"MIMIC-IV\": {\"auroc\": 0.903}, \"eICU\": {\"auroc\": 0.835, \"brier\": 0.087}},\n",
    "            24: {\"MIMIC-IV\": {\"auroc\": 0.930}, \"eICU\": {\"auroc\": 0.825, \"brier\": 0.091}},\n",
    "        }\n",
    "        perf = defaults.get(window, {})\n",
    "    return {\"window\": window, \"performance\": perf}\n",
    "\n",
    "@app.get(\"/model-card\", tags=[\"Metrics\"])\n",
    "async def model_card():\n",
    "    card = {\"version\": config.API_VERSION, \"windows\": config.ALLOWED_WINDOWS, \"commit\": model_manager.commit_hash()}\n",
    "\n",
    "    sub_path = config.INTERP_DIR / \"subgroup_performance.csv\"\n",
    "    if sub_path.exists():\n",
    "        try:\n",
    "            sdf = pd.read_csv(sub_path)\n",
    "            snap = sdf[sdf[\"Window\"] == \"12h\"][[\"Subgroup Type\", \"Subgroup\", \"N\", \"AUROC\"]].copy()\n",
    "            snap = snap.sort_values([\"Subgroup Type\", \"Subgroup\"])\n",
    "            card[\"subgroup_snapshot_12h\"] = snap.to_dict(\"records\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load subgroup file: {e}\")\n",
    "\n",
    "    allm = config.RESULTS_DIR / \"all_metrics.csv\"\n",
    "    if allm.exists():\n",
    "        try:\n",
    "            mdf = pd.read_csv(allm)\n",
    "            best = (\n",
    "                mdf[mdf[\"dataset\"] == \"eICU\"]\n",
    "                .sort_values([\"auroc\", \"window\"], ascending=[False, True])\n",
    "                .head(1)\n",
    "                .to_dict(\"records\")\n",
    "            )\n",
    "            card[\"best_external\"] = best\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to parse all_metrics: {e}\")\n",
    "\n",
    "    thresholds: Dict[int, float] = {}\n",
    "    for w in config.ALLOWED_WINDOWS:\n",
    "        thr_path = config.MODEL_DIR / f\"threshold_w{w}.csv\"\n",
    "        if thr_path.exists():\n",
    "            try:\n",
    "                thresholds[w] = float(pd.read_csv(thr_path)[\"threshold\"].iloc[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "    card[\"thresholds\"] = thresholds\n",
    "    return card\n",
    "\n",
    "# -------------- Error handlers --------------------\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request: Request, exc: HTTPException):\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\"error\": exc.detail, \"status_code\": exc.status_code, \"timestamp\": _now_iso()},\n",
    "    )\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request: Request, exc: Exception):\n",
    "    logger.error(f\"Unhandled exception: {exc}\\n{traceback.format_exc()}\")\n",
    "    return JSONResponse(\n",
    "        status_code=500, content={\"error\": \"Internal server error\", \"status_code\": 500, \"timestamp\": _now_iso()}\n",
    "    )\n",
    "\n",
    "# -------------- Main: dynamic port & host ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Prefer ENV but fall back safely\n",
    "    preferred_port = int(os.getenv(\"API_PORT\", \"8080\"))\n",
    "    # Notebook/Windows friendliness: bind to loopback\n",
    "    in_nb = _in_notebook_or_running_loop()\n",
    "    host = os.getenv(\"API_HOST\", \"127.0.0.1\" if in_nb else \"0.0.0.0\")\n",
    "\n",
    "    # If preferred is blocked, pick a free one and print it\n",
    "    port = _choose_port(preferred_port, host)\n",
    "\n",
    "    if in_nb:\n",
    "        logger.info(\"Detected running event loop / notebook. Launching Uvicorn in a background thread.\")\n",
    "        cfg = uvicorn.Config(app=app, host=host, port=port, reload=False, log_level=\"info\")\n",
    "        server = uvicorn.Server(cfg)\n",
    "        t = threading.Thread(target=server.run, daemon=True)\n",
    "        t.start()\n",
    "        print(f\"âœ… API is running at http://{host}:{port} (auto-selected if 8080 was blocked)\")\n",
    "        print(\"   Docs:        /docs\")\n",
    "        print(\"   Health:      /health\")\n",
    "    else:\n",
    "        uvicorn.run(app=app, host=host, port=port, reload=False, log_level=\"info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "print(requests.get(f\"{BASE}/health\").json())\n",
    "\n",
    "features = {\n",
    "    \"age_years\": 65, \"hr_mean\": 80, \"hr_max\": 90, \"hr_min\": 70,\n",
    "    \"sbp_mean\": 120, \"sbp_min\": 100, \"rr_mean\": 18, \"rr_max\": 22,\n",
    "    \"spo2_mean\": 95, \"spo2_min\": 92, \"lactate_max\": 1.5,\n",
    "    \"creat_max\": 1.0, \"troponin_max\": 0.5\n",
    "}\n",
    "print(requests.post(f\"{BASE}/predict\",\n",
    "                    json={\"features\": features, \"window\": 12, \"include_shap\": True}\n",
    "                   ).json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"streamlit\", \"plotly\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, time, socket\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "_streamlit_proc = None\n",
    "_streamlit_port = None\n",
    "\n",
    "def _pick_port(requested=8501):\n",
    "    s = socket.socket()\n",
    "    try:\n",
    "        s.bind((\"127.0.0.1\", requested))\n",
    "        s.close()\n",
    "        return requested\n",
    "    except OSError:\n",
    "        s.close()\n",
    "        s = socket.socket()\n",
    "        s.bind((\"127.0.0.1\", 0))\n",
    "        port = s.getsockname()[1]\n",
    "        s.close()\n",
    "        return port\n",
    "\n",
    "def start_streamlit_ui(api_url=\"http://127.0.0.1:8000\", port=8501, app_path=\"app.py\"):\n",
    "    \"\"\"Launch Streamlit in the background and show a clickable link.\"\"\"\n",
    "    global _streamlit_proc, _streamlit_port\n",
    "    if _streamlit_proc and _streamlit_proc.poll() is None:\n",
    "        print(f\"âœ… Streamlit already running on http://127.0.0.1:{_streamlit_port}\")\n",
    "        display(HTML(f'<a href=\"http://127.0.0.1:{_streamlit_port}\" target=\"_blank\">Open Streamlit UI</a>'))\n",
    "        return\n",
    "\n",
    "    _streamlit_port = _pick_port(port)\n",
    "    env = os.environ.copy()\n",
    "    env[\"API_URL\"] = api_url  # your FastAPI base URL\n",
    "\n",
    "    # write logs to a file to keep the notebook clean\n",
    "    log_file = open(\"streamlit.log\", \"w\", buffering=1)\n",
    "    _streamlit_proc = subprocess.Popen(\n",
    "        [sys.executable, \"-m\", \"streamlit\", \"run\", app_path,\n",
    "         \"--server.address\", \"127.0.0.1\",\n",
    "         \"--server.port\", str(_streamlit_port),\n",
    "         \"--server.headless\", \"true\"],\n",
    "        env=env,\n",
    "        stdout=log_file,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "\n",
    "    # small wait so it comes up before we print the link\n",
    "    time.sleep(3)\n",
    "    print(f\"âœ… Streamlit started on http://127.0.0.1:{_streamlit_port}\")\n",
    "    print(\"   (logs â†’ streamlit.log)\")\n",
    "    display(HTML(f'<a href=\"http://127.0.0.1:{_streamlit_port}\" target=\"_blank\">Open Streamlit UI</a>'))\n",
    "\n",
    "def stop_streamlit_ui():\n",
    "    \"\"\"Terminate the background Streamlit process.\"\"\"\n",
    "    global _streamlit_proc\n",
    "    if _streamlit_proc and _streamlit_proc.poll() is None:\n",
    "        _streamlit_proc.terminate()\n",
    "        _streamlit_proc = None\n",
    "        print(\"ðŸ›‘ Streamlit stopped.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Streamlit was not running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://127.0.0.1:8000/health\", timeout=3).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "APP_CODE = r\"\"\"\n",
    "import os, requests, pandas as pd, streamlit as st\n",
    "from math import isnan\n",
    "\n",
    "API_URL = os.environ.get(\"API_URL\", \"http://127.0.0.1:8000\")\n",
    "st.set_page_config(page_title=\"Sentinel-ICU UI\", layout=\"wide\")\n",
    "st.sidebar.title(\"Sentinel-ICU UI\")\n",
    "st.sidebar.write(f\"API: **{API_URL}**\")\n",
    "\n",
    "# ---------- health ----------\n",
    "def get_health():\n",
    "    try:\n",
    "        r = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        if r.ok:\n",
    "            return r.json()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "health = get_health()\n",
    "if health:\n",
    "    st.sidebar.success(f\"API: {health['status']} â€¢ v{health['api_version']}\")\n",
    "    st.sidebar.caption(f\"Models: {', '.join([k for k,v in health['models_loaded'].items() if v])}\")\n",
    "else:\n",
    "    st.sidebar.error(\"API not reachable. Start the FastAPI first.\")\n",
    "    st.stop()\n",
    "\n",
    "st.title(\"ðŸ¥ Sentinel-ICU Mortality Prediction\")\n",
    "\n",
    "tabs = st.tabs([\"ðŸ”® Single Prediction\", \"ðŸ“Š Metrics\"])\n",
    "\n",
    "# ---------- SINGLE PREDICTION ----------\n",
    "with tabs[0]:\n",
    "    st.subheader(\"Enter patient features\")\n",
    "    c1, c2, c3 = st.columns(3)\n",
    "\n",
    "    with c1:\n",
    "        age_years  = st.number_input(\"Age (years)\", 18, 120, 65)\n",
    "        hr_mean    = st.number_input(\"HR mean (bpm)\", 20, 220, 80)\n",
    "        hr_max     = st.number_input(\"HR max (bpm)\", hr_mean, 220, max(90, hr_mean+10))\n",
    "        hr_min     = st.number_input(\"HR min (bpm)\", 20, hr_mean, max(20, hr_mean-10))\n",
    "    with c2:\n",
    "        sbp_mean   = st.number_input(\"SBP mean (mmHg)\", 40, 250, 120)\n",
    "        sbp_min    = st.number_input(\"SBP min (mmHg)\", 40, sbp_mean, max(40, sbp_mean-20))\n",
    "        rr_mean    = st.number_input(\"RR mean\", 5, 60, 18)\n",
    "        rr_max     = st.number_input(\"RR max\", rr_mean, 60, max(22, rr_mean+4))\n",
    "    with c3:\n",
    "        spo2_mean  = st.number_input(\"SpOâ‚‚ mean (%)\", 50, 100, 95)\n",
    "        spo2_min   = st.number_input(\"SpOâ‚‚ min (%)\", 50, spo2_mean, max(50, spo2_mean-3))\n",
    "        lactate_max= st.number_input(\"Lactate max (mmol/L)\", 0.0, 30.0, 1.5, step=0.1)\n",
    "        creat_max  = st.number_input(\"Creatinine max (mg/dL)\", 0.0, 20.0, 1.0, step=0.1)\n",
    "        troponin_max = st.number_input(\"Troponin max (ng/mL)\", 0.0, 1000.0, 0.5, step=0.1)\n",
    "\n",
    "    window = st.select_slider(\"Prediction window (hours)\", [6,12,18,24], value=12)\n",
    "    include_shap = st.checkbox(\"Include SHAP explanation\", value=True)\n",
    "\n",
    "    if st.button(\"ðŸ”® Predict\", use_container_width=True, type=\"primary\"):\n",
    "        payload = {\n",
    "            \"features\": {\n",
    "                \"age_years\": age_years, \"hr_mean\": hr_mean, \"hr_max\": hr_max, \"hr_min\": hr_min,\n",
    "                \"sbp_mean\": sbp_mean, \"sbp_min\": sbp_min, \"rr_mean\": rr_mean, \"rr_max\": rr_max,\n",
    "                \"spo2_mean\": spo2_mean, \"spo2_min\": spo2_min, \"lactate_max\": lactate_max,\n",
    "                \"creat_max\": creat_max, \"troponin_max\": troponin_max\n",
    "            },\n",
    "            \"window\": int(window),\n",
    "            \"include_shap\": include_shap\n",
    "        }\n",
    "        try:\n",
    "            r = requests.post(f\"{API_URL}/predict\", json=payload, timeout=30)\n",
    "            if not r.ok:\n",
    "                st.error(r.text); st.stop()\n",
    "            res = r.json()\n",
    "\n",
    "            colA, colB, colC, colD = st.columns(4)\n",
    "            colA.metric(\"Mortality risk\", f\"{res['probability']*100:.1f}%\")\n",
    "            colB.metric(\"Category\", res['risk_category'])\n",
    "            colC.metric(\"Window\", f\"{res['window']}h\")\n",
    "            colD.metric(\"Threshold\", f\"{res['threshold']*100:.1f}%\")\n",
    "\n",
    "            st.caption(f\"95% CI: {res['confidence_interval']['lower']:.3f} â€“ {res['confidence_interval']['upper']:.3f}  â€¢  features used: {res['features_used']}\")\n",
    "\n",
    "            if include_shap and res.get(\"shap_values\"):\n",
    "                st.subheader(\"Top feature contributions (SHAP)\")\n",
    "                sv = res[\"shap_values\"]\n",
    "                # sort by absolute value, show top 10\n",
    "                items = sorted(sv.items(), key=lambda kv: abs(kv[1]), reverse=True)[:10]\n",
    "                df = pd.DataFrame(items, columns=[\"feature\",\"impact\"]).set_index(\"feature\")\n",
    "                st.bar_chart(df)\n",
    "                st.dataframe(df, use_container_width=True)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Request failed: {e}\")\n",
    "\n",
    "# ---------- METRICS ----------\n",
    "with tabs[1]:\n",
    "    st.subheader(\"Performance\")\n",
    "    w = st.selectbox(\"Window\", [6,12,18,24], index=1)\n",
    "    try:\n",
    "        perf = requests.get(f\"{API_URL}/performance/{int(w)}\", timeout=10).json()[\"performance\"]\n",
    "        st.write(perf)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load performance: {e}\")\n",
    "\n",
    "    st.subheader(\"Global feature importance (top-10)\")\n",
    "    try:\n",
    "        imp = requests.get(f\"{API_URL}/explain/{int(w)}\", timeout=10).json()\n",
    "        feats = imp.get(\"features\", [])[:10]\n",
    "        if feats:\n",
    "            df = pd.DataFrame(feats)\n",
    "            st.bar_chart(df.set_index(\"feature\")[\"importance\"])\n",
    "            st.dataframe(df, use_container_width=True)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load importance: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "Path(\"app.py\").write_text(APP_CODE, encoding=\"utf-8\")\n",
    "print(\"âœ… Wrote app.py at:\", Path(\"app.py\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Launch Streamlit UI (auto-find app.py) ===\n",
    "import os, sys, subprocess, time, socket, requests\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "API_URL = \"http://127.0.0.1:8000\"\n",
    "LOG_PATH = Path(\"streamlit.log\")\n",
    "\n",
    "# API check\n",
    "requests.get(f\"{API_URL}/health\", timeout=3).raise_for_status()\n",
    "\n",
    "# ensure streamlit is installed in THIS env\n",
    "try:\n",
    "    import streamlit  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"streamlit\"])\n",
    "\n",
    "# find app.py\n",
    "def find_app_py(start: Path):\n",
    "    for p in [start] + list(start.parents):\n",
    "        c = p / \"app.py\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "    for p in start.rglob(\"app.py\"):\n",
    "        return p\n",
    "    return None\n",
    "\n",
    "app_path = find_app_py(Path.cwd())\n",
    "assert app_path and app_path.exists(), \"app.py not found (run the cell that writes it first)\"\n",
    "print(\"Found app.py at:\", app_path)\n",
    "\n",
    "# pick free port\n",
    "def free_port(cands=(8501,8502,8503)):\n",
    "    import socket\n",
    "    for p in cands:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind((\"127.0.0.1\", p)); return p\n",
    "            except OSError:\n",
    "                pass\n",
    "    raise RuntimeError(\"No free port\")\n",
    "\n",
    "PORT = free_port()\n",
    "\n",
    "# launch\n",
    "env = os.environ.copy()\n",
    "env[\"API_URL\"] = API_URL\n",
    "if LOG_PATH.exists():\n",
    "    try: LOG_PATH.unlink()\n",
    "    except: pass\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"streamlit\", \"run\", str(app_path),\n",
    "       \"--server.address\",\"127.0.0.1\",\"--server.port\",str(PORT),\"--server.headless\",\"true\"]\n",
    "proc = subprocess.Popen(cmd, stdout=LOG_PATH.open(\"wb\"), stderr=subprocess.STDOUT, env=env)\n",
    "\n",
    "time.sleep(4)\n",
    "url = f\"http://127.0.0.1:{PORT}\"\n",
    "display(HTML(f'<div style=\"padding:12px;border:1px solid #ddd;border-radius:8px;background:#f7f9fc\">'\n",
    "             f'<b>âœ… Streamlit started</b><br>Open: <a href=\"{url}\" target=\"_blank\">{url}</a><br>'\n",
    "             f'(If it does not open, see log: {LOG_PATH.resolve()})</div>'))\n",
    "print(\"Log:\", LOG_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, pandas as pd\n",
    "\n",
    "API = \"http://127.0.0.1:8000\"\n",
    "os.makedirs(\"doctor_pack\", exist_ok=True)\n",
    "\n",
    "# --- Two demo patients you can tweak in the UI as well ---\n",
    "patients = {\n",
    "    \"LowRisk\": {\n",
    "        \"age_years\": 62, \"hr_mean\": 78, \"hr_max\": 88, \"hr_min\": 68,\n",
    "        \"sbp_mean\": 125, \"sbp_min\": 105,\n",
    "        \"rr_mean\": 18, \"rr_max\": 22,\n",
    "        \"spo2_mean\": 97, \"spo2_min\": 94,\n",
    "        \"lactate_max\": 1.3, \"creat_max\": 1.0, \"troponin_max\": 0.4\n",
    "    },\n",
    "    \"HighRisk\": {\n",
    "        \"age_years\": 83, \"hr_mean\": 100, \"hr_max\": 120, \"hr_min\": 85,\n",
    "        \"sbp_mean\": 95, \"sbp_min\": 80,\n",
    "        \"rr_mean\": 26, \"rr_max\": 32,\n",
    "        \"spo2_mean\": 88, \"spo2_min\": 82,\n",
    "        \"lactate_max\": 4.2, \"creat_max\": 2.4, \"troponin_max\": 35.0\n",
    "    }\n",
    "}\n",
    "\n",
    "def predict_one(features: dict, window: int, shap=False):\n",
    "    r = requests.post(f\"{API}/predict\",\n",
    "                      json={\"features\": features, \"window\": window, \"include_shap\": shap},\n",
    "                      timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# --- Run predictions across windows and save a tidy table ---\n",
    "rows = []\n",
    "for pid, feats in patients.items():\n",
    "    for w in [6, 12, 18, 24]:\n",
    "        res = predict_one(feats, w)\n",
    "        rows.append({\n",
    "            \"patient\": pid,\n",
    "            \"window_h\": w,\n",
    "            \"probability\": res[\"probability\"],\n",
    "            \"risk_category\": res[\"risk_category\"],\n",
    "            \"threshold\": res[\"threshold\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"doctor_pack/predictions_demo.csv\", index=False)\n",
    "\n",
    "# Save the raw inputs for reference\n",
    "with open(\"doctor_pack/patients_demo.json\", \"w\") as f:\n",
    "    json.dump(patients, f, indent=2)\n",
    "\n",
    "print(\"âœ… Wrote: doctor_pack/predictions_demo.csv and patients_demo.json\")\n",
    "display(df.pivot(index=\"patient\", columns=\"window_h\", values=\"probability\").round(3))\n",
    "\n",
    "# --- Nice-to-have: show SHAP top5 for the HighRisk @12h so clinicians see drivers ---\n",
    "res12 = predict_one(patients[\"HighRisk\"], 12, shap=True)\n",
    "top5 = sorted(res12[\"shap_values\"].items(), key=lambda kv: abs(kv[1]), reverse=True)[:5]\n",
    "print(f\"\\nHighRisk @12h â†’ risk={res12['probability']:.1%}, category={res12['risk_category']}, thr={res12['threshold']:.1%}\")\n",
    "print(\"Top contributors (SHAP):\")\n",
    "for k, v in top5:\n",
    "    print(f\"  â€¢ {k:<12} impact={v:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Batch testing helper (Jupyter) --------------------------------------\n",
    "import os, io, json, requests, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "API = \"http://127.0.0.1:8000\"\n",
    "os.makedirs(\"doctor_pack\", exist_ok=True)\n",
    "\n",
    "# 1) Create a small CSV template doctors can fill (also includes 4 demo rows)\n",
    "cols = [\"age_years\",\"hr_mean\",\"hr_max\",\"hr_min\",\n",
    "        \"sbp_mean\",\"sbp_min\",\"rr_mean\",\"rr_max\",\n",
    "        \"spo2_mean\",\"spo2_min\",\"lactate_max\",\"creat_max\",\"troponin_max\"]\n",
    "\n",
    "demo = pd.DataFrame([\n",
    "    # Low risk-ish\n",
    "    [62,78,88,68,125,105,18,22,97,94,1.3,1.0,0.4],\n",
    "    # High risk-ish\n",
    "    [83,100,120,85,95,80,26,32,88,82,4.2,2.4,35.0],\n",
    "    # Moderate example\n",
    "    [70,90,102,78,110,92,22,28,93,88,2.2,1.5,8.0],\n",
    "    # Another moderate/high\n",
    "    [68,96,110,84,108,90,24,30,92,86,3.1,1.8,15.0],\n",
    "], columns=cols)\n",
    "\n",
    "template_path = \"doctor_pack/template_patients.csv\"\n",
    "demo.to_csv(template_path, index=False)\n",
    "print(f\"âœ… Wrote template with demo rows â†’ {template_path}\")\n",
    "\n",
    "# 2) Function: send any CSV to the API and save predictions CSV\n",
    "def run_batch(csv_path: str, window: int = 12):\n",
    "    with open(csv_path, \"rb\") as f:\n",
    "        files = {\"file\": (\"patients.csv\", f, \"text/csv\")}\n",
    "        r = requests.post(f\"{API}/upload_csv\", params={\"window\": window}, files=files, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    out_path = f\"doctor_pack/predictions_batch_{window}h_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    with open(out_path, \"wb\") as g:\n",
    "        g.write(r.content)\n",
    "    df = pd.read_csv(io.BytesIO(r.content))\n",
    "    print(f\"âœ… Saved predictions â†’ {out_path}  (rows={len(df)})\")\n",
    "    return df, out_path\n",
    "\n",
    "# 3) Run batch on the demo template (edit the CSV first if you like)\n",
    "df12, out12 = run_batch(template_path, window=12)\n",
    "\n",
    "# 4) Quick summary for the report\n",
    "summary = {\n",
    "    \"n\": len(df12),\n",
    "    \"mean_risk\": df12[\"mortality_probability\"].mean(),\n",
    "    \"max_risk\": df12[\"mortality_probability\"].max(),\n",
    "    \"by_category\": df12[\"risk_category\"].value_counts().to_dict(),\n",
    "}\n",
    "print(\"\\nSummary (12h):\", summary)\n",
    "\n",
    "# Show top 10 highest risk patients\n",
    "display(df12.sort_values(\"mortality_probability\", ascending=False)\n",
    "            .head(10)\n",
    "            .assign(mortality_probability=lambda d: (d[\"mortality_probability\"]*100).round(1))\n",
    "            .rename(columns={\"mortality_probability\":\"risk_%\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Build a clinician PDF pack from the latest batch --------------------\n",
    "import pandas as pd, numpy as np, requests, json, glob, os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "API = \"http://127.0.0.1:8000\"\n",
    "os.makedirs(\"doctor_pack\", exist_ok=True)\n",
    "\n",
    "# 1) pick the latest batch CSV you just created\n",
    "cands = sorted(glob.glob(\"doctor_pack/predictions_batch_12h_*.csv\"))\n",
    "assert cands, \"No batch file found in doctor_pack/. Run Step 2 first.\"\n",
    "csv_path = cands[-1]\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2) summary stats\n",
    "mean_risk = df[\"mortality_probability\"].mean()\n",
    "max_risk  = df[\"mortality_probability\"].max()\n",
    "by_cat    = df[\"risk_category\"].value_counts().to_dict()\n",
    "\n",
    "# 3) choose top case for SHAP explain\n",
    "top_row = df.sort_values(\"mortality_probability\", ascending=False).iloc[0]\n",
    "features_cols = [\"age_years\",\"hr_mean\",\"hr_max\",\"hr_min\",\"sbp_mean\",\"sbp_min\",\n",
    "                 \"rr_mean\",\"rr_max\",\"spo2_mean\",\"spo2_min\",\"lactate_max\",\"creat_max\",\"troponin_max\"]\n",
    "features = {k: float(top_row[k]) for k in features_cols}\n",
    "exp = requests.post(f\"{API}/predict\", json={\"features\": features, \"window\": 12, \"include_shap\": True}, timeout=30).json()\n",
    "shap_items = sorted(exp.get(\"shap_values\", {}).items(), key=lambda kv: abs(kv[1]), reverse=True)[:7]\n",
    "\n",
    "# 4) write a compact PDF\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "pdf_path = f\"doctor_pack/clinician_pack_12h_{stamp}.pdf\"\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "\n",
    "    # Page 1 â€“ Title & summary\n",
    "    plt.figure(figsize=(8.5, 11))\n",
    "    plt.axis('off')\n",
    "    txt = (\n",
    "        \"Sentinel-ICU Mortality Prediction â€“ Clinician Pack (12h)\\n\\n\"\n",
    "        f\"Source CSV: {os.path.basename(csv_path)}\\n\"\n",
    "        f\"Patients: {len(df)}\\n\"\n",
    "        f\"Mean risk: {mean_risk:.1%}\\n\"\n",
    "        f\"Max risk: {max_risk:.1%}\\n\"\n",
    "        f\"Risk counts: {by_cat}\\n\"\n",
    "        \"\\nInterpretation: risk â‰¥ displayed threshold is flagged as Very High.\\n\"\n",
    "        \"Use this pack to spot high-risk cases and see top contributing features.\"\n",
    "    )\n",
    "    plt.text(0.05, 0.95, txt, va='top', fontsize=12)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # Page 2 â€“ Risk distribution\n",
    "    plt.figure(figsize=(8.5, 6))\n",
    "    plt.hist(df[\"mortality_probability\"], bins=20)\n",
    "    plt.xlabel(\"Mortality probability\")\n",
    "    plt.ylabel(\"Number of patients\")\n",
    "    plt.title(\"Risk distribution (12h)\")\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # Page 3 â€“ Top 10 patients table\n",
    "    top10 = (df.sort_values(\"mortality_probability\", ascending=False)\n",
    "               .head(10)\n",
    "               .assign(risk_pct=lambda d: (d[\"mortality_probability\"]*100).round(1))\n",
    "               [features_cols + [\"risk_pct\",\"risk_category\"]])\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 6))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Top 10 highest risk patients\", pad=12)\n",
    "    tbl = ax.table(cellText=top10.values,\n",
    "                   colLabels=top10.columns,\n",
    "                   loc='center')\n",
    "    tbl.auto_set_font_size(False); tbl.set_fontsize(8); tbl.scale(1, 1.2)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # Page 4 â€“ SHAP for #1 patient\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 6))\n",
    "    feats = [k for k, v in shap_items][::-1]\n",
    "    vals  = [float(v) for k, v in shap_items][::-1]\n",
    "    ax.barh(range(len(vals)), vals)\n",
    "    ax.set_yticks(range(len(vals))); ax.set_yticklabels(feats)\n",
    "    ax.set_xlabel(\"SHAP impact on risk\")\n",
    "    ax.set_title(f\"Top contributors for highest-risk patient (risk {exp['probability']:.1%})\")\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "print(f\"âœ… PDF ready â†’ {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-cell \"Clinician Pack\" generator for Jupyter\n",
    "# - Calls your FastAPI (/upload_csv) to get predictions if needed\n",
    "# - Creates a clean, multi-page PDF (A4): cover â†’ histogram â†’ paginated table â†’ optional SHAP page\n",
    "# - Keeps table readable (fewer columns, wrapped text, smaller fonts, row pagination)\n",
    "\n",
    "from __future__ import annotations\n",
    "import io, textwrap, math, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def _get_threshold(api_url: str, window: int) -> Optional[float]:\n",
    "    try:\n",
    "        r = requests.get(f\"{api_url}/model-card\", timeout=8)\n",
    "        r.raise_for_status()\n",
    "        th = r.json().get(\"thresholds\", {})\n",
    "        # keys may be str in JSON; try both\n",
    "        return th.get(window) or th.get(str(window))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _risk_counts(df: pd.DataFrame) -> dict:\n",
    "    vc = df[\"risk_category\"].astype(str).value_counts()\n",
    "    return {k:int(v) for k,v in vc.items()}\n",
    "\n",
    "def _fmt_pct(x: float) -> str:\n",
    "    try:\n",
    "        return f\"{100*float(x):.1f}%\"\n",
    "    except Exception:\n",
    "        return \"â€”\"\n",
    "\n",
    "def _color_for_category(cat: str) -> str:\n",
    "    c = (cat or \"\").lower()\n",
    "    if \"very\" in c: return \"#dc3545\"   # red\n",
    "    if \"high\" in c: return \"#fd7e14\"   # orange\n",
    "    if \"moderate\" in c: return \"#ffc107\" # amber\n",
    "    return \"#28a745\"                   # green\n",
    "\n",
    "def _clean_columns_for_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Keep a concise set that fits on A4 landscape nicely\n",
    "    preferred = [\n",
    "        \"age_years\",\"hr_mean\",\"sbp_mean\",\"rr_mean\",\"spo2_mean\",\n",
    "        \"lactate_max\",\"creat_max\",\"troponin_max\",\n",
    "        \"mortality_probability\",\"risk_category\"\n",
    "    ]\n",
    "    have = [c for c in preferred if c in df.columns]\n",
    "    out = df[have].copy()\n",
    "    # Format numbers (keep 1â€“2 decimals; prob as %)\n",
    "    if \"mortality_probability\" in out.columns:\n",
    "        out[\"risk_%\"] = out[\"mortality_probability\"].map(_fmt_pct)\n",
    "        out.drop(columns=[\"mortality_probability\"], inplace=True)\n",
    "    for c in out.columns:\n",
    "        if c == \"risk_category\" or c == \"risk_%\": \n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(out[c]):\n",
    "            out[c] = out[c].map(lambda v: f\"{v:.2f}\")\n",
    "    # Put risk columns at end\n",
    "    cols = [c for c in out.columns if c not in (\"risk_%\",\"risk_category\")] + \\\n",
    "           [c for c in (\"risk_%\",\"risk_category\") if c in out.columns]\n",
    "    return out[cols]\n",
    "\n",
    "def _draw_cover(pdf: PdfPages, title: str, subtitle: str, meta_lines: List[str]):\n",
    "    fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape in inches\n",
    "    ax = plt.axes([0,0,1,1]); ax.axis(\"off\")\n",
    "    plt.text(0.05, 0.82, title, fontsize=24, weight=\"bold\")\n",
    "    plt.text(0.05, 0.74, subtitle, fontsize=14)\n",
    "    y = 0.66\n",
    "    for line in meta_lines:\n",
    "        plt.text(0.05, y, line, fontsize=12)\n",
    "        y -= 0.05\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "def _draw_hist(pdf: PdfPages, probs: np.ndarray, threshold: Optional[float], window: int):\n",
    "    fig = plt.figure(figsize=(11.69, 8.27))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(probs, bins=20)\n",
    "    ax.set_title(f\"Risk Distribution (window {window}h)\")\n",
    "    ax.set_xlabel(\"Mortality risk\"); ax.set_ylabel(\"Patients\")\n",
    "    if threshold is not None:\n",
    "        ax.axvline(threshold, linestyle=\"--\")\n",
    "        ax.text(threshold, ax.get_ylim()[1]*0.9, f\" Thr={threshold:.2f}\", rotation=90, va=\"top\")\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "def _draw_table_pages(pdf: PdfPages, table_df: pd.DataFrame, rows_per_page: int = 22, title: str = \"Patient Summary\"):\n",
    "    n = len(table_df)\n",
    "    pages = max(1, math.ceil(n/rows_per_page))\n",
    "    for p in range(pages):\n",
    "        chunk = table_df.iloc[p*rows_per_page:(p+1)*rows_per_page]\n",
    "        fig = plt.figure(figsize=(11.69, 8.27))\n",
    "        ax = plt.axes([0,0,1,1]); ax.axis(\"off\")\n",
    "        plt.text(0.02, 0.96, f\"{title}  (Page {p+1}/{pages})\", fontsize=14, weight=\"bold\")\n",
    "        # table\n",
    "        ax2 = fig.add_axes([0.02, 0.05, 0.96, 0.86]); ax2.axis(\"off\")\n",
    "        tbl = ax2.table(cellText=chunk.values, colLabels=chunk.columns, loc=\"upper left\", cellLoc=\"left\")\n",
    "        tbl.auto_set_font_size(False)\n",
    "        tbl.set_fontsize(8)\n",
    "        tbl.scale(1, 1.25)  # make rows a bit taller\n",
    "        # wrap long cell text\n",
    "        for key, cell in tbl.get_celld().items():\n",
    "            cell.set_text_props(wrap=True, fontsize=8)\n",
    "        pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "def _draw_shap_page(pdf: PdfPages, api_url: str, features_row: dict, window: int, title: str):\n",
    "    try:\n",
    "        r = requests.post(f\"{api_url}/predict\", json={\"features\": features_row, \"window\": window, \"include_shap\": True}, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        shap_vals = data.get(\"shap_values\") or {}\n",
    "        if not shap_vals:  # nothing to show\n",
    "            return\n",
    "        items = sorted(shap_vals.items(), key=lambda kv: abs(kv[1]), reverse=True)[:10]\n",
    "        feats = [k for k,_ in items][::-1]\n",
    "        vals  = [v for _,v in items][::-1]\n",
    "\n",
    "        fig = plt.figure(figsize=(11.69, 8.27))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.barh(range(len(feats)), vals)\n",
    "        ax.set_yticks(range(len(feats))); ax.set_yticklabels(feats)\n",
    "        ax.set_xlabel(\"Impact on risk (SHAP)\"); ax.set_title(title)\n",
    "        pdf.savefig(fig); plt.close(fig)\n",
    "    except Exception:\n",
    "        # Skip SHAP page silently if anything fails\n",
    "        pass\n",
    "\n",
    "def build_clinician_pack(\n",
    "    input_csv: Path,\n",
    "    output_pdf: Path,\n",
    "    api_url: str = \"http://127.0.0.1:8000\",\n",
    "    window: int = 12,\n",
    "    want_shap: bool = True\n",
    ") -> Tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    input_csv:\n",
    "      - EITHER a features CSV (13 columns). The function will call the API /upload_csv to get predictions.\n",
    "      - OR a predictions CSV that already includes columns ['mortality_probability','risk_category'].\n",
    "    Returns (prediction_dataframe, output_pdf_path).\n",
    "    \"\"\"\n",
    "    input_csv = Path(input_csv)\n",
    "    output_pdf = Path(output_pdf)\n",
    "    output_pdf.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load the CSV the user passed\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "\n",
    "    # Decide whether we must call the API\n",
    "    need_api = not {\"mortality_probability\",\"risk_category\"}.issubset(set(df_in.columns))\n",
    "\n",
    "    if need_api:\n",
    "        # Send CSV to /upload_csv to get predictions back\n",
    "        buf = io.BytesIO()\n",
    "        df_in.to_csv(buf, index=False); buf.seek(0)\n",
    "        files = {\"file\": (\"patients.csv\", buf.getvalue(), \"text/csv\")}\n",
    "        resp = requests.post(f\"{api_url}/upload_csv\", files=files, params={\"window\": window}, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        df_pred = pd.read_csv(io.BytesIO(resp.content))\n",
    "    else:\n",
    "        df_pred = df_in.copy()\n",
    "\n",
    "    # Ensure expected columns exist\n",
    "    assert \"mortality_probability\" in df_pred.columns, \"Predictions CSV missing 'mortality_probability'\"\n",
    "    assert \"risk_category\" in df_pred.columns, \"Predictions CSV missing 'risk_category'\"\n",
    "\n",
    "    # Overall summary\n",
    "    probs = df_pred[\"mortality_probability\"].astype(float).to_numpy()\n",
    "    thr = _get_threshold(api_url, window)\n",
    "    counts = _risk_counts(df_pred)\n",
    "    mean_r = float(np.mean(probs)) if len(probs) else 0.0\n",
    "    max_r  = float(np.max(probs))  if len(probs) else 0.0\n",
    "\n",
    "    # Table data (clean subset of columns, paginated)\n",
    "    table_df = _clean_columns_for_table(df_pred)\n",
    "\n",
    "    # Pick top-risk row for optional SHAP\n",
    "    top_idx = int(np.argmax(probs)) if len(probs) else None\n",
    "    top_row = df_pred.iloc[top_idx].to_dict() if top_idx is not None else None\n",
    "\n",
    "    # ---- Build PDF ----\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        # Cover\n",
    "        meta = [\n",
    "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "            f\"Window: {window}h\",\n",
    "            f\"Patients: {len(df_pred)}\",\n",
    "            f\"Mean risk: {_fmt_pct(mean_r)}    |    Max risk: {_fmt_pct(max_r)}\",\n",
    "            \"By category: \" + \", \".join([f\"{k}: {v}\" for k,v in counts.items()]) if counts else \"By category: â€”\",\n",
    "            f\"Decision threshold: {thr:.2f}\" if thr is not None else \"Decision threshold: (not available)\",\n",
    "        ]\n",
    "        _draw_cover(pdf, \"Clinician Pack â€” ICU MI Mortality Risk\", \"For clinical review & triage support\", meta)\n",
    "\n",
    "        # Histogram of risks\n",
    "        _draw_hist(pdf, probs, thr, window)\n",
    "\n",
    "        # Paginated patient table\n",
    "        _draw_table_pages(pdf, table_df, rows_per_page=22, title=\"Patients (key features + risk)\")\n",
    "\n",
    "        # Optional SHAP page for the single highest-risk patient\n",
    "        if want_shap and top_row is not None:\n",
    "            # Build a minimal feature dict for /predict (only the model features)\n",
    "            model_feats = [\n",
    "                \"age_years\",\"hr_mean\",\"hr_max\",\"hr_min\",\n",
    "                \"sbp_mean\",\"sbp_min\",\"rr_mean\",\"rr_max\",\n",
    "                \"spo2_mean\",\"spo2_min\",\"lactate_max\",\"creat_max\",\"troponin_max\"\n",
    "            ]\n",
    "            feat_payload = {k: float(top_row[k]) for k in model_feats if k in top_row}\n",
    "            title = f\"SHAP â€” Top Contributors (patient @ max risk={_fmt_pct(max_r)})\"\n",
    "            _draw_shap_page(pdf, api_url, feat_payload, window, title)\n",
    "\n",
    "    print(f\"âœ… Saved clinician pack â†’ {output_pdf}  (rows={len(df_pred)})\")\n",
    "    return df_pred, output_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pred_df, pdf_path = build_clinician_pack(\n",
    "    input_csv=Path(r\"doctor_pack/template_patients.csv\"),\n",
    "    output_pdf=Path(r\"doctor_pack/clinician_pack_12h.pdf\"),\n",
    "    api_url=\"http://127.0.0.1:8000\",\n",
    "    window=12,\n",
    "    want_shap=True\n",
    ")\n",
    "pdf_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tabulate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 â€” paths & helpers\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "RES = ROOT / \"outputs\" / \"external_validation\"\n",
    "MODELS = RES / \"models\"\n",
    "RESULTS = RES / \"results\"\n",
    "SHAPDIR = ROOT / \"outputs\" / \"external_validation\" / \"shap\"\n",
    "INTERP = ROOT / \"outputs\" / \"external_validation\" / \"interpretability\"\n",
    "\n",
    "MS = ROOT / \"manuscript\"\n",
    "FIGS = MS / \"figures\"\n",
    "TABS = MS / \"tables\"\n",
    "for d in [MS, FIGS, TABS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Looking for:\")\n",
    "print(\"  metrics:\", RESULTS / \"all_metrics.csv\")\n",
    "print(\"  thresholds:\", [MODELS / f\"threshold_w{w}.csv\" for w in [6,12,18,24]])\n",
    "print(\"  subgroup:\", INTERP / \"subgroup_performance.csv\")\n",
    "print(\"  shap:\", [SHAPDIR / f\"feature_importance_w{w}.csv\" for w in [6,12,18,24]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2b â€” Table A (robust; no warnings)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_metrics_path = RESULTS / \"all_metrics.csv\"\n",
    "assert all_metrics_path.exists(), f\"Missing {all_metrics_path}\"\n",
    "\n",
    "m = pd.read_csv(all_metrics_path)\n",
    "\n",
    "# Keep the key metrics\n",
    "cols = [\"window\", \"dataset\", \"auroc\", \"auprc\", \"brier\"]\n",
    "if \"ece\" in m.columns:\n",
    "    cols += [\"ece\"]\n",
    "m = m[cols].copy()\n",
    "\n",
    "def fmt(x):\n",
    "    return \"â€”\" if (pd.isna(x) or x is None) else f\"{x:.3f}\"\n",
    "\n",
    "tbl = (m\n",
    "       .sort_values([\"window\", \"dataset\"])\n",
    "       .assign(window=lambda df: df[\"window\"].astype(int))\n",
    "       .pivot(index=\"window\", columns=\"dataset\")\n",
    "      )\n",
    "\n",
    "tbl.columns = [f\"{c[0].upper()} ({c[1]})\" for c in tbl.columns]\n",
    "for c in tbl.columns:\n",
    "    tbl[c] = tbl[c].map(fmt)\n",
    "\n",
    "display(tbl)\n",
    "\n",
    "# Always save CSV\n",
    "tbl.to_csv(TABS / \"tableA_performance_by_window.csv\")\n",
    "print(\"âœ“ Saved:\", TABS / \"tableA_performance_by_window.csv\")\n",
    "\n",
    "# Try to save Markdown if 'tabulate' is available\n",
    "try:\n",
    "    md = tbl.to_markdown()\n",
    "    (TABS / \"tableA_performance_by_window.md\").write_text(md, encoding=\"utf-8\")\n",
    "    print(\"âœ“ Saved:\", TABS / \"tableA_performance_by_window.md\")\n",
    "except Exception as e:\n",
    "    print(\"â„¹ï¸ Skipping Markdown export (install 'tabulate' to enable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 â€” thresholds table (reads threshold_w{w}.csv files)\n",
    "rows = []\n",
    "for w in [6, 12, 18, 24]:\n",
    "    p = MODELS / f\"threshold_w{w}.csv\"\n",
    "    if p.exists():\n",
    "        t = pd.read_csv(p)\n",
    "        thr = float(t[\"threshold\"].iloc[0])\n",
    "        rows.append({\"window_h\": w, \"threshold\": thr})\n",
    "    else:\n",
    "        rows.append({\"window_h\": w, \"threshold\": np.nan})\n",
    "\n",
    "thr_tbl = pd.DataFrame(rows)\n",
    "thr_tbl[\"threshold\"] = thr_tbl[\"threshold\"].map(lambda x: \"â€”\" if pd.isna(x) else f\"{x:.3f}\")\n",
    "display(thr_tbl)\n",
    "\n",
    "thr_tbl.to_csv(TABS / \"tableB_thresholds.csv\", index=False)\n",
    "print(\"âœ“ Saved:\", TABS / \"tableB_thresholds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 â€” AUROC bar chart\n",
    "m = pd.read_csv(RESULTS / \"all_metrics.csv\")\n",
    "\n",
    "plot_df = (m[[\"window\",\"dataset\",\"auroc\"]]\n",
    "           .dropna()\n",
    "           .assign(window=lambda d: d[\"window\"].astype(int))\n",
    "           .query(\"dataset in ['MIMIC-IV','eICU']\")\n",
    "          )\n",
    "\n",
    "plt.figure(figsize=(7,4.2))\n",
    "for i, ds in enumerate([\"MIMIC-IV\", \"eICU\"]):\n",
    "    sub = plot_df[plot_df[\"dataset\"] == ds].sort_values(\"window\")\n",
    "    plt.plot(sub[\"window\"], sub[\"auroc\"], marker=\"o\", label=ds)\n",
    "\n",
    "plt.xticks([6,12,18,24])\n",
    "plt.ylim(0.75, 1.00)\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.xlabel(\"Prediction window (hours)\")\n",
    "plt.title(\"Discrimination by Window\")\n",
    "plt.legend()\n",
    "out = FIGS / \"fig1_auroc_by_window.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(out, dpi=300)\n",
    "plt.show()\n",
    "print(\"âœ“ Saved figure:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” global SHAP top-10 for 12h\n",
    "imp_path = SHAPDIR / \"feature_importance_w12.csv\"\n",
    "assert imp_path.exists(), f\"Missing {imp_path} (created in SHAP step)\"\n",
    "\n",
    "imp = pd.read_csv(imp_path)\n",
    "# Expect columns like ['feature','importance'] â€” adapt if yours differ\n",
    "cols = [c.lower() for c in imp.columns]\n",
    "if \"feature\" not in cols:\n",
    "    raise ValueError(f\"Unexpected SHAP file columns: {imp.columns.tolist()}\")\n",
    "# Normalize column names\n",
    "ren = {c: c.lower() for c in imp.columns}\n",
    "imp = imp.rename(columns=ren)\n",
    "if \"importance\" not in imp.columns:\n",
    "    # Some exports name it 'mean_abs_shap' or similar\n",
    "    cand = [c for c in imp.columns if \"shap\" in c.lower() or \"importance\" in c.lower()]\n",
    "    assert cand, f\"Couldn't find importance column in {imp.columns.tolist()}\"\n",
    "    imp[\"importance\"] = imp[cand[0]]\n",
    "\n",
    "top = imp.sort_values(\"importance\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(7,4.6))\n",
    "plt.barh(top[\"feature\"][::-1], top[\"importance\"][::-1])\n",
    "plt.xlabel(\"Global importance (mean |SHAP|)\")\n",
    "plt.title(\"Top-10 Features (12h model)\")\n",
    "plt.tight_layout()\n",
    "out = FIGS / \"fig2_shap_top10_12h.png\"\n",
    "plt.savefig(out, dpi=300)\n",
    "plt.show()\n",
    "print(\"âœ“ Saved figure:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 â€” subgroup snapshot (12h)\n",
    "sg_path = INTERP / \"subgroup_performance.csv\"\n",
    "if sg_path.exists():\n",
    "    sg = pd.read_csv(sg_path)\n",
    "    snap = (sg[sg[\"Window\"].astype(str).str.contains(\"12\")]\n",
    "              .loc[:, [\"Subgroup Type\",\"Subgroup\",\"N\",\"AUROC\"]]\n",
    "              .sort_values([\"Subgroup Type\",\"Subgroup\"]))\n",
    "    snap[\"AUROC\"] = snap[\"AUROC\"].map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"â€”\")\n",
    "    display(snap)\n",
    "    snap.to_csv(TABS / \"tableC_subgroups_12h.csv\", index=False)\n",
    "    print(\"âœ“ Saved:\", TABS / \"tableC_subgroups_12h.csv\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No subgroup_performance.csv found â€” skipping Table C.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 â€” baselines (optional)\n",
    "base_path = RESULTS / \"baseline_metrics.csv\"\n",
    "if base_path.exists():\n",
    "    base = pd.read_csv(base_path)\n",
    "    keep = base[[\"window\",\"model\",\"dataset\",\"auroc\",\"auprc\",\"brier\"]].copy()\n",
    "    keep[\"window\"] = keep[\"window\"].astype(int)\n",
    "    keep = keep.sort_values([\"window\",\"dataset\",\"model\"])\n",
    "    for c in [\"auroc\",\"auprc\",\"brier\"]:\n",
    "        keep[c] = keep[c].map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"â€”\")\n",
    "    display(keep)\n",
    "    keep.to_csv(TABS / \"tableD_baselines.csv\", index=False)\n",
    "    print(\"âœ“ Saved:\", TABS / \"tableD_baselines.csv\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No baseline_metrics.csv found â€” skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build manuscript bundle: README + ZIP (one cell)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "TABS  = MANUS / \"tables\"\n",
    "FIGS  = MANUS / \"figures\"\n",
    "\n",
    "MANUS.mkdir(exist_ok=True, parents=True)\n",
    "TABS.mkdir(exist_ok=True, parents=True)\n",
    "FIGS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ---- Load artifacts we already created ----\n",
    "all_metrics_csv = TABS / \"tableA_performance_by_window.csv\"\n",
    "thr_csv         = TABS / \"tableB_thresholds.csv\"\n",
    "sg_csv          = TABS / \"tableC_subgroups_12h.csv\"  # may not exist\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(p) if p.exists() else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "dfA = _read_csv_safe(all_metrics_csv)\n",
    "dfB = _read_csv_safe(thr_csv)\n",
    "dfC = _read_csv_safe(sg_csv)\n",
    "\n",
    "# ---- Build Markdown sections ----\n",
    "lines = []\n",
    "lines.append(\"# Sentinel-ICU Results Bundle\\n\")\n",
    "lines.append(f\"_Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\\n\")\n",
    "\n",
    "# A) Performance by window\n",
    "lines.append(\"## A. Discrimination & Calibration (by window)\")\n",
    "if dfA is not None and not dfA.empty:\n",
    "    try:\n",
    "        mdA = dfA.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdA = dfA.to_csv(index=False)\n",
    "    lines.append(mdA + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table A not found.*\\n\")\n",
    "\n",
    "# B) Decision thresholds\n",
    "lines.append(\"## B. Learned decision thresholds\")\n",
    "if dfB is not None and not dfB.empty:\n",
    "    try:\n",
    "        mdB = dfB.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdB = dfB.to_csv(index=False)\n",
    "    lines.append(mdB + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table B not found.*\\n\")\n",
    "\n",
    "# C) Subgroup snapshot (12h)\n",
    "lines.append(\"## C. Subgroup snapshot (12 h window)\")\n",
    "if dfC is not None and not dfC.empty:\n",
    "    try:\n",
    "        mdC = dfC.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdC = dfC.to_csv(index=False)\n",
    "    lines.append(mdC + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table C not found (optional).* \\n\")\n",
    "\n",
    "# D) Figures (paths only; images are already saved)\n",
    "fig1 = FIGS / \"fig1_auroc_by_window.png\"\n",
    "fig2 = FIGS / \"fig2_shap_top10_12h.png\"\n",
    "lines.append(\"## D. Figures\\n\")\n",
    "if fig1.exists():\n",
    "    lines.append(f\"- **Figure 1**: AUROC by window â†’ `manuscript/figures/{fig1.name}`\")\n",
    "if fig2.exists():\n",
    "    lines.append(f\"- **Figure 2**: SHAP top-10 (12 h) â†’ `manuscript/figures/{fig2.name}`\")\n",
    "if not (fig1.exists() or fig2.exists()):\n",
    "    lines.append(\"- (No figures found yet)\")\n",
    "\n",
    "lines.append(\"\")\n",
    "\n",
    "# E) How clinicians can test the model (local)\n",
    "lines.append(\"## E. How clinicians can test the model (local)\")\n",
    "lines.append(\"- **API docs**:  `http://127.0.0.1:8000/docs`\")\n",
    "lines.append(\"- **Health**:    `http://127.0.0.1:8000/health`\")\n",
    "lines.append(\"- **Streamlit UI**:  `http://127.0.0.1:8501`  *(adjust port if you launched on 8502/8503)*\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Example (12 h prediction)**\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8000/predict \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"features\\\": {\\\"age_years\\\": 70, \\\"hr_mean\\\": 95, \\\"hr_max\\\": 105, \\\"hr_min\\\": 85,')\n",
    "lines.append('                 \\\"sbp_mean\\\": 110, \\\"sbp_min\\\": 90, \\\"rr_mean\\\": 22, \\\"rr_max\\\": 26,')\n",
    "lines.append('                 \\\"spo2_mean\\\": 92, \\\"spo2_min\\\": 88, \\\"lactate_max\\\": 3.2,')\n",
    "lines.append('                 \\\"creat_max\\\": 1.8, \\\"troponin_max\\\": 8.5}, \\\"window\\\": 12 }\\'')\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Save README\n",
    "readme_path = MANUS / \"README_results.md\"\n",
    "readme_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"âœ“ Wrote:\", readme_path)\n",
    "\n",
    "# ---- Zip the manuscript folder ----\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "zip_base = ROOT / f\"manuscript_bundle_{ts}\"\n",
    "zip_path = shutil.make_archive(str(zip_base), \"zip\", MANUS)\n",
    "print(\"âœ“ Zipped bundle:\", zip_path)\n",
    "\n",
    "# Show a tiny summary\n",
    "print(\"\\nBundle contents preview:\")\n",
    "for p in sorted(MANUS.rglob(\"*\")):\n",
    "    if p.is_file():\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(\" -\", rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration plot from predictions\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT  = Path.cwd()\n",
    "RDIR  = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "FIGS  = MANUS / \"figures\"\n",
    "TABS  = MANUS / \"tables\"\n",
    "\n",
    "for d in [MANUS, FIGS, TABS, RDIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOW = 12\n",
    "pred_path = RDIR / f\"eicu_preds_{WINDOW}h.csv\"\n",
    "\n",
    "# --- Require predictions; do not try to rebuild them from features ---\n",
    "if not pred_path.exists():\n",
    "    raise SystemExit(\n",
    "        f\"âŒ Missing predictions: {pred_path}\\n\"\n",
    "        \"This script no longer constructs predictions from feature tables. \"\n",
    "        \"Please generate eICU predictions first.\"\n",
    "    )\n",
    "print(\"Using predictions:\", pred_path)\n",
    "\n",
    "# --- Load predictions & detect columns ---\n",
    "dfp = pd.read_csv(pred_path)\n",
    "label_cols = [\"label\",\"hospital_expire_flag\",\"y_true\",\"death\",\"outcome\",\"y\"]\n",
    "prob_cols  = [\"mortality_probability\",\"prob_cal\",\"prob\",\"pred_prob\",\"y_proba\",\"probability\",\"risk_prob\",\"proba\",\"pred\"]\n",
    "\n",
    "y_col = next((c for c in label_cols if c in dfp.columns), None)\n",
    "p_col = next((c for c in prob_cols  if c in dfp.columns), None)\n",
    "if y_col is None or p_col is None:\n",
    "    raise SystemExit(f\"âŒ {pred_path.name} must have a label column and a probability column.\\n\"\n",
    "                     f\"Looked for labels in: {label_cols}\\n\"\n",
    "                     f\"and probabilities in: {prob_cols}\")\n",
    "\n",
    "y_true = dfp[y_col].astype(int).values\n",
    "y_prob = dfp[p_col].astype(float).values\n",
    "print(f\"Detected columns â†’ label='{y_col}', prob='{p_col}', n={len(y_true):,}\")\n",
    "\n",
    "# --- Quantile-binned reliability + ECE ---\n",
    "def bin_reliability_quantile(y, p, n_bins=10):\n",
    "    # handle degenerate cases\n",
    "    uq = np.unique(p)\n",
    "    nb = int(min(n_bins, max(2, len(uq))))\n",
    "    q = pd.qcut(pd.Series(p), q=nb, duplicates=\"drop\")\n",
    "    tmp = pd.DataFrame({\"y\": y, \"p\": p, \"bin\": q})\n",
    "    grp = tmp.groupby(\"bin\", observed=True)\n",
    "    out = pd.DataFrame({\n",
    "        \"bin\": range(1, len(grp)+1),\n",
    "        \"bin_count\": grp.size().values,\n",
    "        \"mean_pred\": grp[\"p\"].mean().values,\n",
    "        \"empirical\": grp[\"y\"].mean().values,\n",
    "    })\n",
    "    w = out[\"bin_count\"] / out[\"bin_count\"].sum()\n",
    "    ece = float(np.sum(w * np.abs(out[\"empirical\"] - out[\"mean_pred\"])))\n",
    "    return out, ece\n",
    "\n",
    "bins_df, ece = bin_reliability_quantile(y_true, y_prob, n_bins=10)\n",
    "frac_pos, mean_pred = calibration_curve(\n",
    "    y_true, y_prob, n_bins=min(10, max(2, len(np.unique(y_prob)))), strategy=\"quantile\"\n",
    ")\n",
    "\n",
    "# --- Figure ---\n",
    "plt.figure(figsize=(5.2, 5.2))\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", linewidth=1, label=\"Perfect\", zorder=1)\n",
    "plt.scatter(bins_df[\"mean_pred\"], bins_df[\"empirical\"], s=40, zorder=3, label=\"Binned (quantiles)\")\n",
    "plt.plot(mean_pred, frac_pos, linewidth=2, zorder=2, label=\"Calibration curve\")\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Calibration â€” 12 h (eICU)\")\n",
    "plt.legend(loc=\"upper left\"); plt.grid(alpha=0.25)\n",
    "fig_path = FIGS / \"fig_calibration_reliability_w12h.png\"\n",
    "plt.tight_layout(); plt.savefig(fig_path, dpi=300); plt.close()\n",
    "print(\"âœ“ Saved figure:\", fig_path)\n",
    "\n",
    "# --- Bins table + README note ---\n",
    "bins_csv = TABS / \"calib_bins_w12h.csv\"\n",
    "bins_df.to_csv(bins_csv, index=False)\n",
    "print(\"âœ“ Saved bins table:\", bins_csv)\n",
    "print(f\"ECE (10 quantile bins): {ece:.3f}\")\n",
    "\n",
    "readme = MANUS / \"README_results.md\"\n",
    "try:\n",
    "    txt = readme.read_text(encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    txt = \"# Sentinel-ICU Results Bundle\\n\\n\"\n",
    "extra = f\"\"\"\n",
    "## Calibration (12 h, eICU)\n",
    "- **ECE (10 quantile bins)**: `{ece:.3f}`\n",
    "- **Bins CSV**: `manuscript/tables/{bins_csv.name}`\n",
    "- **Figure**: `manuscript/figures/{fig_path.name}`\n",
    "\"\"\"\n",
    "readme.write_text(txt.rstrip() + \"\\n\" + extra, encoding=\"utf-8\")\n",
    "print(\"âœ“ Updated README with calibration summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Operating-point metrics @12h \n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "ROOT  = Path.cwd()\n",
    "RDIR  = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MDIR  = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "TABS  = MANUS / \"tables\"\n",
    "FIGS  = MANUS / \"figures\"\n",
    "for p in [TABS, FIGS]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOW = 12\n",
    "preds  = pd.read_csv(RDIR / f\"eicu_preds_{WINDOW}h.csv\")\n",
    "\n",
    "# Detect columns\n",
    "y_col  = next((c for c in [\"label\",\"hospital_expire_flag\",\"y_true\",\"death\",\"outcome\",\"y\"] if c in preds.columns), None)\n",
    "p_col  = next((c for c in [\"mortality_probability\",\"prob_cal\",\"prob\",\"pred_prob\",\"y_proba\",\"probability\"] if c in preds.columns), None)\n",
    "assert y_col and p_col, \"Need label + probability columns.\"\n",
    "\n",
    "# Threshold (learned from VAL set)\n",
    "thr_csv = MDIR / f\"threshold_w{WINDOW}.csv\"\n",
    "thr = float(pd.read_csv(thr_csv)[\"threshold\"].iloc[0]) if thr_csv.exists() else 0.36\n",
    "print(f\"Using threshold={thr:.3f}\")\n",
    "\n",
    "y = preds[y_col].astype(int).values\n",
    "p = preds[p_col].astype(float).values\n",
    "yhat = (p >= thr).astype(int)\n",
    "\n",
    "TP = int(((y==1)&(yhat==1)).sum())\n",
    "TN = int(((y==0)&(yhat==0)).sum())\n",
    "FP = int(((y==0)&(yhat==1)).sum())\n",
    "FN = int(((y==1)&(yhat==0)).sum())\n",
    "N  = len(y)\n",
    "\n",
    "sens = TP/(TP+FN) if TP+FN else np.nan  # recall\n",
    "spec = TN/(TN+FP) if TN+FP else np.nan\n",
    "ppv  = TP/(TP+FP) if TP+FP else np.nan\n",
    "npv  = TN/(TN+FN) if TN+FN else np.nan\n",
    "f1   = (2*TP)/(2*TP+FP+FN) if 2*TP+FP+FN else np.nan\n",
    "prev = y.mean()\n",
    "\n",
    "metrics_tbl = pd.DataFrame([{\n",
    "    \"window_h\": WINDOW,\n",
    "    \"threshold\": thr,\n",
    "    \"N\": N,\n",
    "    \"prevalence\": prev,\n",
    "    \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "    \"sensitivity\": sens, \"specificity\": spec,\n",
    "    \"PPV\": ppv, \"NPV\": npv, \"F1\": f1,\n",
    "    \"alerts_rate\": (TP+FP)/N\n",
    "}])\n",
    "out_csv = TABS / f\"tableE_operating_point_{WINDOW}h.csv\"\n",
    "metrics_tbl.to_csv(out_csv, index=False)\n",
    "print(\"âœ“ Saved:\", out_csv)\n",
    "metrics_tbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion-style bar chart for the paper appendix\n",
    "plt.figure(figsize=(5.5,3.2))\n",
    "plt.bar([\"TP\",\"FP\",\"TN\",\"FN\"], [TP,FP,TN,FN])\n",
    "plt.title(f\"12h Operating Point (thr={thr:.2f})\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "out_fig = FIGS / \"fig4_operating_point_12h.png\"\n",
    "plt.savefig(out_fig, dpi=300)\n",
    "plt.close()\n",
    "print(\"âœ“ Saved figure:\", out_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Alerts per 100 patients across thresholds (12h) ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT  = Path.cwd()\n",
    "RDIR  = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "FIGS  = MANUS / \"figures\"; FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOW = 12\n",
    "df = pd.read_csv(RDIR / f\"eicu_preds_{WINDOW}h.csv\")\n",
    "y_col  = next((c for c in [\"label\",\"hospital_expire_flag\",\"y_true\",\"death\",\"outcome\",\"y\"] if c in df.columns), None)\n",
    "p_col  = next((c for c in [\"mortality_probability\",\"prob_cal\",\"prob\",\"pred_prob\",\"y_proba\",\"probability\"] if c in df.columns), None)\n",
    "y = df[y_col].astype(int).values\n",
    "p = df[p_col].astype(float).values\n",
    "N = len(y)\n",
    "\n",
    "grid = np.round(np.arange(0.05, 0.61, 0.05), 2)\n",
    "rows = []\n",
    "for t in grid:\n",
    "    yhat = (p >= t).astype(int)\n",
    "    alerts = (yhat==1).sum()\n",
    "    alerts_per_100 = 100*alerts/N\n",
    "    tp = int(((y==1)&(yhat==1)).sum())\n",
    "    fp = int(((y==0)&(yhat==1)).sum())\n",
    "    sens = tp/ (y==1).sum()\n",
    "    ppv  = tp/ max(1,(tp+fp))\n",
    "    rows.append({\"threshold\": float(t), \"alerts_per_100\": alerts_per_100, \"sensitivity\": sens, \"PPV\": ppv})\n",
    "\n",
    "plot = pd.DataFrame(rows)\n",
    "\n",
    "plt.figure(figsize=(6.4,4))\n",
    "plt.plot(plot[\"threshold\"], plot[\"alerts_per_100\"], marker=\"o\", label=\"Alerts/100 pts\")\n",
    "plt.plot(plot[\"threshold\"], 100*plot[\"sensitivity\"], marker=\"o\", label=\"Sensitivity Ã—100\")\n",
    "plt.plot(plot[\"threshold\"], 100*plot[\"PPV\"], marker=\"o\", label=\"PPV Ã—100\")\n",
    "plt.xlabel(\"Decision threshold\")\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.title(\"Alert volume & performance vs threshold â€” 12h (eICU)\")\n",
    "plt.xticks(grid)\n",
    "plt.ylim(0,100)\n",
    "plt.grid(alpha=0.25)\n",
    "plt.legend()\n",
    "out = FIGS / \"fig5_alerts_vs_threshold_12h.png\"\n",
    "plt.tight_layout(); plt.savefig(out, dpi=300); plt.close()\n",
    "print(\"âœ“ Saved figure:\", out)\n",
    "\n",
    "out_csv = MANUS / \"tables\" / \"tableF_alerts_vs_threshold_12h.csv\"\n",
    "plot.to_csv(out_csv, index=False)\n",
    "print(\"âœ“ Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Build clinician 1-slide deck (12h) ===\n",
    "# Requires: python-pptx (auto-installs if missing)\n",
    "\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import pptx  # noqa\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-pptx\"])\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT  = Path.cwd()\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "FIGS  = MANUS / \"figures\"\n",
    "TABS  = MANUS / \"tables\"\n",
    "SLIDES = MANUS / \"slides\"; SLIDES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Load numbers we already produced ----\n",
    "WINDOW = 12\n",
    "op_path = TABS / f\"tableE_operating_point_{WINDOW}h.csv\"\n",
    "perf_path = TABS / \"tableA_performance_by_window.csv\"\n",
    "\n",
    "op = pd.read_csv(op_path).iloc[0]\n",
    "perf = pd.read_csv(perf_path).set_index(\"window\").sort_index()\n",
    "\n",
    "thr    = float(op[\"threshold\"])\n",
    "N      = int(op[\"N\"])\n",
    "prev   = float(op[\"prevalence\"])\n",
    "sens   = float(op[\"sensitivity\"])\n",
    "spec   = float(op[\"specificity\"])\n",
    "ppv    = float(op[\"PPV\"])\n",
    "npv    = float(op[\"NPV\"])\n",
    "f1     = float(op[\"F1\"])\n",
    "alerts = float(op[\"alerts_rate\"])\n",
    "\n",
    "auroc_eicu_12  = float(perf.loc[12, \"AUROC (eICU)\"])\n",
    "auroc_mimic_12 = float(perf.loc[12, \"AUROC (MIMIC-IV)\"])\n",
    "\n",
    "# ---- Figures (use what you generated in Step 2/3/4) ----\n",
    "fig_auroc = FIGS / \"fig1_auroc_by_window.png\"\n",
    "fig_cal   = FIGS / \"fig3_calibration_12h.png\"\n",
    "fig_alert = FIGS / \"fig5_alerts_vs_threshold_12h.png\"\n",
    "\n",
    "for p in [fig_auroc, fig_cal, fig_alert]:\n",
    "    if not p.exists():\n",
    "        print(f\"âš ï¸ Missing figure: {p.name} â€” slide will skip it.\")\n",
    "\n",
    "# ---- Build slide ----\n",
    "prs = Presentation()                 # layout 6 = blank\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "\n",
    "# Title\n",
    "title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(12.5), Inches(0.8))\n",
    "tf = title_box.text_frame\n",
    "tf.clear()\n",
    "run = tf.paragraphs[0].add_run()\n",
    "run.text = \"Sentinel-ICU Mortality Risk @12h â€” External Validation (eICU)\"\n",
    "run.font.size = Pt(28); run.font.bold = True\n",
    "\n",
    "# Subtitle with headline numbers\n",
    "sub = slide.shapes.add_textbox(Inches(0.5), Inches(1.1), Inches(12.5), Inches(0.6)).text_frame\n",
    "sub.clear()\n",
    "p = sub.paragraphs[0]; p.alignment = PP_ALIGN.LEFT\n",
    "r = p.add_run()\n",
    "r.text = (f\"AUROC (eICU): {auroc_eicu_12:.3f}   |   Threshold: {thr:.2f}   â†’   \"\n",
    "          f\"Sens {sens:.1%}  Spec {spec:.1%}  PPV {ppv:.1%}  NPV {npv:.1%}   \"\n",
    "          f\"(Prev {prev:.1%}, Alerts {alerts*100:.1f}% of pts)\")\n",
    "r.font.size = Pt(14)\n",
    "\n",
    "# Metrics box (left)\n",
    "box = slide.shapes.add_textbox(Inches(0.5), Inches(1.8), Inches(5.8), Inches(3.4))\n",
    "tfb = box.text_frame; tfb.clear()\n",
    "\n",
    "def add_line(text, bold=False, size=14):\n",
    "    para = tfb.add_paragraph() if tfb.paragraphs[0].text else tfb.paragraphs[0]\n",
    "    run = para.add_run(); run.text = text\n",
    "    run.font.size = Pt(size); run.font.bold = bold\n",
    "\n",
    "add_line(\"Clinical operating point (12h):\", bold=True, size=16)\n",
    "add_line(f\"â€¢ Dataset: eICU  (n={N:,})\")\n",
    "add_line(f\"â€¢ Threshold: {thr:.2f}  (learned on VAL)\")\n",
    "add_line(f\"â€¢ Sensitivity: {sens:.1%}   Specificity: {spec:.1%}\")\n",
    "add_line(f\"â€¢ PPV: {ppv:.1%}   NPV: {npv:.1%}\")\n",
    "add_line(f\"â€¢ Alerts per 100 patients: {alerts*100:.1f}\")\n",
    "add_line(f\"â€¢ Prevalence: {prev:.1%}\")\n",
    "add_line(f\"â€¢ F1-score: {f1:.3f}\")\n",
    "\n",
    "# Small footnote\n",
    "foot = slide.shapes.add_textbox(Inches(0.5), Inches(7.0), Inches(12.0), Inches(0.4)).text_frame\n",
    "foot.clear()\n",
    "fp = foot.paragraphs[0]; fr = fp.add_run()\n",
    "fr.text = f\"Sentinel-ICU v1.0 | Trained: MIMIC-IV | External: eICU | Generated {datetime.now():%Y-%m-%d %H:%M}\"\n",
    "fr.font.size = Pt(10); fr.font.color.rgb = RGBColor(110,110,110)\n",
    "\n",
    "# Images (right side)\n",
    "x0, y0, w = Inches(6.6), Inches(1.6), Inches(6.3)\n",
    "h = Inches(2.0)\n",
    "if fig_auroc.exists():\n",
    "    slide.shapes.add_picture(str(fig_auroc), x0, y0, width=w)\n",
    "    y0 = y0 + Inches(2.1)\n",
    "if fig_cal.exists():\n",
    "    slide.shapes.add_picture(str(fig_cal), x0, y0, width=w)\n",
    "    y0 = y0 + Inches(2.1)\n",
    "if fig_alert.exists():\n",
    "    slide.shapes.add_picture(str(fig_alert), x0, y0, width=w)\n",
    "\n",
    "# Save\n",
    "out_pptx = SLIDES / \"clinician_summary_12h.pptx\"\n",
    "prs.save(out_pptx)\n",
    "print(\"âœ… Saved slide:\", out_pptx.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6: Build a shareable bundle (folder + zip) ===\n",
    "import shutil, webbrowser, sys, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "ROOT   = Path.cwd()\n",
    "MANUS  = ROOT / \"manuscript\"\n",
    "FIGS   = MANUS / \"figures\"\n",
    "TABS   = MANUS / \"tables\"\n",
    "SLIDES = MANUS / \"slides\"\n",
    "DPACK  = ROOT / \"doctor_pack\"\n",
    "\n",
    "# Output folder name with timestamp\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "bundle_dir = ROOT / f\"Sentinel-ICU_v1_12h_bundle_{stamp}\"\n",
    "bundle_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- What to include (copy if exists) ----\n",
    "to_copy = [\n",
    "    # Slides + PDF\n",
    "    SLIDES / \"clinician_summary_12h.pptx\",\n",
    "    DPACK  / \"clinician_pack_12h.pdf\",             # if you generated it locally\n",
    "    Path(r\"/mnt/data/clinician_pack_12h.pdf\"),     # fallback from earlier run (if exists)\n",
    "\n",
    "    # Figures used in the deck\n",
    "    FIGS / \"fig1_auroc_by_window.png\",\n",
    "    FIGS / \"fig3_calibration_12h.png\",\n",
    "    FIGS / \"fig4_operating_point_12h.png\",\n",
    "    FIGS / \"fig5_alerts_vs_threshold_12h.png\",\n",
    "\n",
    "    # Tables\n",
    "    TABS / \"tableA_performance_by_window.csv\",\n",
    "    TABS / \"tableA_performance_by_window.md\",\n",
    "    TABS / \"tableB_thresholds.csv\",\n",
    "    TABS / \"tableC_subgroups_12h.csv\",\n",
    "    TABS / \"tableD_calibration_bins_12h.csv\",\n",
    "    TABS / \"tableE_operating_point_12h.csv\",\n",
    "    TABS / \"tableF_alerts_vs_threshold_12h.csv\",\n",
    "\n",
    "    # Clinician-facing CSV template + example predictions\n",
    "    DPACK / \"template_patients.csv\",\n",
    "    DPACK / \"predictions_batch_12h.csv\",           # if created earlier\n",
    "]\n",
    "\n",
    "copied = []\n",
    "for p in to_copy:\n",
    "    try:\n",
    "        if p and p.exists():\n",
    "            shutil.copy2(p, bundle_dir / p.name)\n",
    "            copied.append(p.name)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not copy {p}: {e}\")\n",
    "\n",
    "# ---- Pull headline numbers into README ----\n",
    "op = (TABS / \"tableE_operating_point_12h.csv\")\n",
    "perf = (TABS / \"tableA_performance_by_window.csv\")\n",
    "\n",
    "thr = sens = spec = ppv = npv = prev = alerts = auroc_eicu = auroc_mimic = \"N/A\"\n",
    "N = \"N/A\"\n",
    "try:\n",
    "    opdf = pd.read_csv(op).iloc[0]\n",
    "    thr    = f\"{float(opdf['threshold']):.2f}\"\n",
    "    N      = f\"{int(opdf['N']):,}\"\n",
    "    prev   = f\"{float(opdf['prevalence']):.1%}\"\n",
    "    sens   = f\"{float(opdf['sensitivity']):.1%}\"\n",
    "    spec   = f\"{float(opdf['specificity']):.1%}\"\n",
    "    ppv    = f\"{float(opdf['PPV']):.1%}\"\n",
    "    npv    = f\"{float(opdf['NPV']):.1%}\"\n",
    "    alerts = f\"{float(opdf['alerts_rate'])*100:.1f}%\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    pdf = pd.read_csv(perf).set_index(\"window\")\n",
    "    auroc_eicu  = f\"{float(pdf.loc[12, 'AUROC (eICU)']):.3f}\"\n",
    "    auroc_mimic = f\"{float(pdf.loc[12, 'AUROC (MIMIC-IV)']):.3f}\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---- README for recipients ----\n",
    "readme = f\"\"\"# Sentinel-ICU â€” External Validation (12h window) â€” Share Pack\n",
    "\n",
    "**What this contains**\n",
    "- 1-slide summary: `clinician_summary_12h.pptx`\n",
    "- Clinician PDF handout: `clinician_pack_12h.pdf` (if present)\n",
    "- Figures: AUROC by window, calibration plot, operating point, alerts vs threshold\n",
    "- Tables: performance, thresholds, subgroup, calibration bins, operating-point metrics\n",
    "- CSV template for testing the API/UI: `template_patients.csv`\n",
    "\n",
    "**Headline numbers (eICU, 12h)**\n",
    "- AUROC (MIMIC-IV internal): {auroc_mimic}\n",
    "- AUROC (eICU external): {auroc_eicu}\n",
    "- Operating point (threshold = {thr})\n",
    "  - N={N}, prevalence={prev}\n",
    "  - Sensitivity={sens}, Specificity={spec}\n",
    "  - PPV={ppv}, NPV={npv}\n",
    "  - Alerts rate = {alerts}\n",
    "\n",
    "**How to view the interactive UI (if youâ€™re on my machine)**\n",
    "1) API (already running): http://127.0.0.1:8000/health  \n",
    "   Docs: http://127.0.0.1:8000/docs  \n",
    "2) UI: http://127.0.0.1:8501\n",
    "\n",
    "**How to test with the template**\n",
    "- Open the UI â†’ â€œBatch Uploadâ€ tab â†’ upload `template_patients.csv` â†’ select 12h â†’ run.\n",
    "- Or use the single-patient form on â€œSingle Predictionâ€ tab.\n",
    "\n",
    "**Notes**\n",
    "- Model was trained on MIMIC-IV and externally validated on eICU as summarized above.\n",
    "- Figures and tables correspond to the 12h evaluation unless otherwise labeled.\n",
    "\"\"\"\n",
    "\n",
    "(bundle_dir / \"README.md\").write_text(readme, encoding=\"utf-8\")\n",
    "\n",
    "# ---- Tiny helper to open links (Windows .bat) ----\n",
    "bat = bundle_dir / \"open_links.bat\"\n",
    "bat.write_text(\n",
    "    '@echo off\\n'\n",
    "    'start \"\" \"http://127.0.0.1:8000/health\"\\n'\n",
    "    'start \"\" \"http://127.0.0.1:8000/docs\"\\n'\n",
    "    'start \"\" \"http://127.0.0.1:8501\"\\n'\n",
    "    'pause\\n', encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# ---- Zip the bundle ----\n",
    "zip_path = bundle_dir.with_suffix(\".zip\")\n",
    "if zip_path.exists():\n",
    "    try: zip_path.unlink()\n",
    "    except: pass\n",
    "shutil.make_archive(str(bundle_dir), \"zip\", root_dir=bundle_dir)\n",
    "\n",
    "print(\"âœ… Bundle folder:\", bundle_dir)\n",
    "print(\"âœ… Zip archive:  \", zip_path)\n",
    "print(\"\\nIncluded files:\")\n",
    "for name in sorted(copied):\n",
    "    print(\" -\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import HTMLResponse, PlainTextResponse\n",
    "from fastapi.encoders import jsonable_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fmt_duration(seconds: float) -> str:\n",
    "    seconds = int(seconds or 0)\n",
    "    h, rem = divmod(seconds, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    return f\"{h}h {m}m {s}s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/health/html\", response_class=HTMLResponse, tags=[\"General\"])\n",
    "async def health_html(refresh: int = 10):\n",
    "    \"\"\"\n",
    "    Human-friendly health page.\n",
    "    - refresh: seconds between auto-refresh (0 to disable)\n",
    "    \"\"\"\n",
    "    data = await health()  # reuse your existing JSON health\n",
    "    d = jsonable_encoder(data)\n",
    "\n",
    "    status = d.get(\"status\", \"unknown\")\n",
    "    color = {\"healthy\": \"#16a34a\", \"degraded\": \"#f59e0b\", \"down\": \"#dc2626\"}.get(status, \"#6b7280\")\n",
    "    models = d.get(\"models_loaded\", {})\n",
    "    models_html = \"\".join(\n",
    "        f\"<span class='chip {'ok' if loaded else 'bad'}'>{w}h</span>\"\n",
    "        for w, loaded in models.items()\n",
    "    )\n",
    "\n",
    "    html = f\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  <meta http-equiv=\"refresh\" content=\"{max(0, int(refresh))}\">\n",
    "  <title>Sentinel-ICU â€¢ Health</title>\n",
    "  <style>\n",
    "    :root {{\n",
    "      --ok: #16a34a;        /* green */\n",
    "      --warn: #f59e0b;      /* amber */\n",
    "      --bad: #dc2626;       /* red */\n",
    "      --muted: #6b7280;     /* gray */\n",
    "      --bg: #0f172a;        /* slate-900 */\n",
    "      --card: #111827;      /* gray-900 */\n",
    "      --text: #e5e7eb;      /* gray-200 */\n",
    "      --soft: #1f2937;      /* gray-800 */\n",
    "    }}\n",
    "    * {{ box-sizing:border-box; }}\n",
    "    body {{\n",
    "      margin: 0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial;\n",
    "      background: linear-gradient(180deg, #0b1220 0%, #0f172a 100%);\n",
    "      color: var(--text);\n",
    "    }}\n",
    "    .wrap {{ max-width: 960px; margin: 36px auto; padding: 0 16px; }}\n",
    "    .hero {{\n",
    "      background: {color}22; border: 1px solid {color}55; border-radius: 16px; padding: 16px 20px;\n",
    "      display:flex; justify-content:space-between; align-items:center; gap:16px;\n",
    "    }}\n",
    "    .badge {{\n",
    "      display:inline-flex; align-items:center; gap:10px; font-weight:600; padding:8px 12px;\n",
    "      border-radius: 999px; background: {color}33; color: #fff; border:1px solid {color}66;\n",
    "    }}\n",
    "    .dot {{ width:10px; height:10px; border-radius:50%; background:{color}; display:inline-block; }}\n",
    "    .grid {{\n",
    "      display:grid; grid-template-columns: repeat(4, 1fr); gap:14px; margin-top:16px;\n",
    "    }}\n",
    "    .card {{\n",
    "      background: var(--card); border:1px solid var(--soft); border-radius: 12px; padding:14px 16px;\n",
    "    }}\n",
    "    .label {{ color:#9ca3af; font-size:12px; text-transform:uppercase; letter-spacing:.06em; }}\n",
    "    .value {{ font-size:18px; font-weight:600; margin-top:6px; }}\n",
    "    .chips {{ display:flex; flex-wrap:wrap; gap:8px; margin-top:6px; }}\n",
    "    .chip {{\n",
    "      padding:4px 10px; border-radius: 999px; font-size:13px; border:1px solid var(--soft); background:#0b1220;\n",
    "    }}\n",
    "    .chip.ok {{ border-color: #0ea5e9; background:#0c1b2c; }}\n",
    "    .chip.bad {{ border-color: var(--bad); background:#2a0f13; }}\n",
    "    .links {{ display:flex; gap:10px; margin-top:18px; }}\n",
    "    .button {{\n",
    "      display:inline-block; padding:8px 12px; border-radius:8px; border:1px solid var(--soft);\n",
    "      background:#0b1220; color:#d1d5db; text-decoration:none;\n",
    "    }}\n",
    "    .muted {{ color:#9ca3af; }}\n",
    "    @media (max-width: 720px) {{ .grid {{ grid-template-columns: 1fr 1fr; }} }}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"wrap\">\n",
    "    <div class=\"hero\">\n",
    "      <div>\n",
    "        <div class=\"badge\"><span class=\"dot\"></span> <span>Status: {status.title()}</span></div>\n",
    "        <div class=\"muted\" style=\"margin-top:6px;\">Auto-refresh: every {max(0, int(refresh))}s â€” change with <code>?refresh=5</code></div>\n",
    "      </div>\n",
    "      <div class=\"links\">\n",
    "        <a class=\"button\" href=\"/docs\">Swagger</a>\n",
    "        <a class=\"button\" href=\"/redoc\">ReDoc</a>\n",
    "        <a class=\"button\" href=\"/health\">JSON</a>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"grid\">\n",
    "      <div class=\"card\">\n",
    "        <div class=\"label\">API Version</div>\n",
    "        <div class=\"value\">{d.get('api_version','â€”')}</div>\n",
    "      </div>\n",
    "      <div class=\"card\">\n",
    "        <div class=\"label\">Uptime</div>\n",
    "        <div class=\"value\">{_fmt_duration(d.get('uptime_seconds'))}</div>\n",
    "      </div>\n",
    "      <div class=\"card\">\n",
    "        <div class=\"label\">Total Predictions</div>\n",
    "        <div class=\"value\">{d.get('total_predictions', 0)}</div>\n",
    "      </div>\n",
    "      <div class=\"card\">\n",
    "        <div class=\"label\">Last Prediction</div>\n",
    "        <div class=\"value\">{d.get('last_prediction') or 'â€”'}</div>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"card\" style=\"margin-top:14px;\">\n",
    "      <div class=\"label\">Models Loaded</div>\n",
    "      <div class=\"chips\">{models_html}</div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"card\" style=\"margin-top:14px;\">\n",
    "      <div class=\"label\">Commit</div>\n",
    "      <div class=\"value\">{d.get('commit') or 'â€”'}</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "    return HTMLResponse(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/healthz\", response_class=PlainTextResponse, include_in_schema=False)\n",
    "async def healthz():\n",
    "    d = await health()\n",
    "    return \"ok\" if d.status == \"healthy\" else \"degraded\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\", tags=[\"General\"])\n",
    "async def root():\n",
    "    return {\n",
    "        \"name\": \"Sentinel-ICU API\",\n",
    "        \"version\": config.API_VERSION,\n",
    "        \"windows\": config.ALLOWED_WINDOWS,\n",
    "        \"docs\": \"/docs\",\n",
    "        \"redoc\": \"/redoc\",\n",
    "        \"health\": \"/health\",\n",
    "        \"health_html\": \"/health/html\",   # â† add this\n",
    "        \"predict\": \"/predict\",\n",
    "        \"batch\": \"/batch\",\n",
    "        \"explain\": \"/explain\",\n",
    "        \"model_card\": \"/model-card\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Health Page & simple healthz probe -------------------------------\n",
    "from fastapi.responses import HTMLResponse, PlainTextResponse\n",
    "\n",
    "@app.get(\"/health/html\", response_class=HTMLResponse, tags=[\"General\"])\n",
    "async def health_html():\n",
    "    # Small HTML shell that reads your existing /health JSON and renders a dashboard.\n",
    "    return HTMLResponse(\"\"\"\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/>\n",
    "<title>Sentinel-ICU â€¢ Health</title>\n",
    "<style>\n",
    "  :root{--ok:#16a34a;--warn:#f59e0b;--bad:#dc2626;--ink:#111827;--mut:#6b7280;--bg:#f8fafc;--card:#ffffff}\n",
    "  body{margin:0;font-family:ui-sans-serif,-apple-system,Segoe UI,Roboto,Helvetica,Arial;color:var(--ink);background:var(--bg)}\n",
    "  .wrap{max-width:980px;margin:40px auto;padding:0 18px}\n",
    "  .hdr{display:flex;gap:12px;align-items:center}\n",
    "  .badge{font-size:12px;padding:4px 8px;border-radius:999px;color:white}\n",
    "  .b-ok{background:var(--ok)} .b-warn{background:var(--warn)} .b-bad{background:var(--bad)}\n",
    "  .grid{display:grid;grid-template-columns:repeat(2,minmax(0,1fr));gap:14px;margin-top:16px}\n",
    "  @media (max-width:700px){.grid{grid-template-columns:1fr}}\n",
    "  .card{background:var(--card);border:1px solid #e5e7eb;border-radius:12px;padding:14px}\n",
    "  h1{font-size:24px;margin:0} h2{font-size:14px;color:var(--mut);margin:0 0 6px}\n",
    "  .mono{font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace}\n",
    "  ul{margin:8px 0 0 18px}\n",
    "  .links a{margin-right:10px;text-decoration:none}\n",
    "  .pill{display:inline-block;border:1px solid #e5e7eb;border-radius:999px;padding:2px 8px;margin:2px 6px 0 0}\n",
    "  .mut{color:var(--mut)}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"wrap\">\n",
    "    <div class=\"hdr\">\n",
    "      <h1>Sentinel-ICU â€¢ Health</h1>\n",
    "      <span id=\"statusBadge\" class=\"badge\">â€¦</span>\n",
    "    </div>\n",
    "    <div class=\"mut\" id=\"subtitle\">checkingâ€¦</div>\n",
    "\n",
    "    <div class=\"grid\">\n",
    "      <div class=\"card\">\n",
    "        <h2>Version</h2>\n",
    "        <div>API: <span class=\"mono\" id=\"apiVersion\">â€”</span></div>\n",
    "        <div>Commit: <span class=\"mono\" id=\"commit\">â€”</span></div>\n",
    "        <div>Uptime: <span class=\"mono\" id=\"uptime\">â€”</span></div>\n",
    "      </div>\n",
    "\n",
    "      <div class=\"card\">\n",
    "        <h2>Prediction activity</h2>\n",
    "        <div>Total predictions: <b id=\"nPred\">â€”</b></div>\n",
    "        <div>Last prediction: <span class=\"mono\" id=\"lastPred\">â€”</span></div>\n",
    "      </div>\n",
    "\n",
    "      <div class=\"card\">\n",
    "        <h2>Models loaded</h2>\n",
    "        <div id=\"models\">â€”</div>\n",
    "      </div>\n",
    "\n",
    "      <div class=\"card\">\n",
    "        <h2>Quick links</h2>\n",
    "        <div class=\"links\">\n",
    "          <a href=\"/docs\" target=\"_blank\">Swagger /docs</a>\n",
    "          <a href=\"/redoc\" target=\"_blank\">ReDoc</a>\n",
    "          <a href=\"/model-card\" target=\"_blank\">Model Card JSON</a>\n",
    "          <a href=\"/performance/12\" target=\"_blank\">Performance (12h)</a>\n",
    "        </div>\n",
    "        <div class=\"mut\" style=\"margin-top:6px\">Auto-refresh every 5s</div>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "<script>\n",
    "async function refresh(){\n",
    "  try{\n",
    "    const r = await fetch('/health',{cache:'no-store'});\n",
    "    const h = await r.json();\n",
    "\n",
    "    const st = (h.status || '').toLowerCase();\n",
    "    const badge = document.getElementById('statusBadge');\n",
    "    badge.textContent = h.status;\n",
    "    badge.className = 'badge ' + (st==='healthy' ? 'b-ok' : (st==='degraded' ? 'b-warn' : 'b-bad'));\n",
    "\n",
    "    document.getElementById('apiVersion').textContent = h.api_version ?? 'â€”';\n",
    "    document.getElementById('commit').textContent = h.commit ?? 'â€”';\n",
    "    document.getElementById('nPred').textContent = h.total_predictions ?? 0;\n",
    "    document.getElementById('lastPred').textContent = h.last_prediction ?? 'â€”';\n",
    "\n",
    "    // uptime nice\n",
    "    const s = Math.floor(h.uptime_seconds || 0);\n",
    "    const hrs = Math.floor(s/3600), mins = Math.floor((s%3600)/60), secs = s%60;\n",
    "    document.getElementById('uptime').textContent = `${hrs}h ${mins}m ${secs}s`;\n",
    "\n",
    "    // models\n",
    "    const m = h.models_loaded || {};\n",
    "    const box = document.getElementById('models');\n",
    "    box.innerHTML = '';\n",
    "    Object.keys(m).sort((a,b)=>parseInt(a)-parseInt(b)).forEach(k=>{\n",
    "      const ok = m[k] ? 'âœ“' : 'âœ—';\n",
    "      const span = document.createElement('span');\n",
    "      span.className = 'pill';\n",
    "      span.textContent = `${k}h ${ok}`;\n",
    "      box.appendChild(span);\n",
    "    });\n",
    "\n",
    "    document.getElementById('subtitle').textContent =\n",
    "      `API ${h.status} â€¢ windows: ` + Object.keys(m).join(', ');\n",
    "  }catch(e){\n",
    "    const badge = document.getElementById('statusBadge');\n",
    "    badge.textContent = 'DOWN';\n",
    "    badge.className = 'badge b-bad';\n",
    "    document.getElementById('subtitle').textContent = 'Failed to fetch /health';\n",
    "  }\n",
    "}\n",
    "refresh();\n",
    "setInterval(refresh, 5000);\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\")\n",
    "\n",
    "# Plain text probe for load balancers/K8s\n",
    "@app.get(\"/healthz\", response_class=PlainTextResponse, include_in_schema=False)\n",
    "async def healthz():\n",
    "    # We keep it simple: if JSON /health would return 200, this is OK.\n",
    "    return PlainTextResponse(\"ok\", status_code=200)\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, datetime as dt\n",
    "API = \"http://127.0.0.1:8000\"\n",
    "\n",
    "payload = {\n",
    "    \"window\": 12,\n",
    "    \"patient_id\": \"demo-1\",\n",
    "    \"features\": {\n",
    "        \"age_years\": 72, \"hr_mean\": 88, \"hr_max\": 110, \"hr_min\": 70,\n",
    "        \"sbp_mean\": 118, \"sbp_min\": 95, \"rr_mean\": 22, \"rr_max\": 30,\n",
    "        \"spo2_mean\": 95, \"spo2_min\": 88, \"lactate_max\": 2.6,\n",
    "        \"creat_max\": 1.4, \"troponin_max\": 0.12\n",
    "    },\n",
    "    \"return_shap\": True\n",
    "}\n",
    "r = requests.post(f\"{API}/predict\", json=payload, timeout=10)\n",
    "print(r.status_code, r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import HTMLResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/model-card/html\", response_class=HTMLResponse, tags=[\"General\"])\n",
    "async def model_card_html():\n",
    "    return r\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "<title>Sentinel-ICU â€¢ Model Card</title>\n",
    "<style>\n",
    "  :root { --bg:#0b1220; --panel:#111827; --muted:#94a3b8; --ok:#22c55e; --chip:#1f2937; --link:#93c5fd;}\n",
    "  html,body{margin:0;background:var(--bg);color:#e5e7eb;font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, \"Helvetica Neue\", Arial;}\n",
    "  a{color:var(--link);text-decoration:none}\n",
    "  .wrap{max-width:1100px;margin:24px auto;padding:0 16px;}\n",
    "  .hdr{display:flex;gap:12px;align-items:center;margin-bottom:16px}\n",
    "  .badge{background:#16a34a22;color:#86efac;border:1px solid #16a34a55;border-radius:999px;padding:4px 10px;font-size:12px}\n",
    "  .grid{display:grid;grid-template-columns:1fr;gap:16px}\n",
    "  @media(min-width:980px){.grid{grid-template-columns:1.2fr .8fr}}\n",
    "  .card{background:var(--panel);border:1px solid #1f2937; border-radius:12px;padding:16px}\n",
    "  .kvs{display:grid;grid-template-columns:160px 1fr; gap:6px 10px; font-size:14px}\n",
    "  .muted{color:var(--muted)}\n",
    "  table{width:100%;border-collapse:collapse;margin-top:8px;font-size:14px}\n",
    "  th,td{border-bottom:1px solid #1f2937;padding:8px;text-align:left}\n",
    "  .chips{display:flex;flex-wrap:wrap;gap:6px;margin-top:6px}\n",
    "  .chip{background:var(--chip);border:1px solid #374151;border-radius:999px;padding:4px 10px;font-size:12px}\n",
    "  .foot{margin-top:16px;font-size:12px;color:var(--muted)}\n",
    "  pre{white-space:pre-wrap;word-break:break-word;background:#0b1020;border:1px solid #1e293b;padding:10px;border-radius:8px}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"wrap\">\n",
    "  <div class=\"hdr\">\n",
    "    <h2 style=\"margin:0\">Sentinel-ICU â€¢ Model Card</h2>\n",
    "    <span class=\"badge\" id=\"version\">vâ€”</span>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"grid\">\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 10px\">Overview</h3>\n",
    "      <div class=\"kvs\">\n",
    "        <div class=\"muted\">Windows</div><div id=\"windows\">â€”</div>\n",
    "        <div class=\"muted\">Commit</div><div id=\"commit\">â€”</div>\n",
    "        <div class=\"muted\">Best external (12h)</div><div id=\"best_external\">â€”</div>\n",
    "      </div>\n",
    "      <div class=\"foot\">Source JSON: <a href=\"/model-card\" target=\"_blank\">/model-card</a></div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 10px\">Thresholds</h3>\n",
    "      <table id=\"thres_tbl\">\n",
    "        <thead><tr><th>Window</th><th>Threshold</th></tr></thead>\n",
    "        <tbody></tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"card\" style=\"margin-top:16px\">\n",
    "    <h3 style=\"margin:4px 0 10px\">Subgroup snapshot (12h)</h3>\n",
    "    <table id=\"sub_tbl\">\n",
    "      <thead><tr><th>Type</th><th>Subgroup</th><th>N</th><th>AUROC</th></tr></thead>\n",
    "      <tbody></tbody>\n",
    "    </table>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"card\" style=\"margin-top:16px\">\n",
    "    <details>\n",
    "      <summary style=\"cursor:pointer\">Raw JSON</summary>\n",
    "      <pre id=\"raw\">Loadingâ€¦</pre>\n",
    "    </details>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "async function main(){\n",
    "  const r = await fetch('/model-card');\n",
    "  if(!r.ok){ document.body.innerHTML = '<div class=\"wrap\"><div class=\"card\">Failed to load /model-card</div></div>'; return; }\n",
    "  const j = await r.json();\n",
    "  document.getElementById('version').textContent = 'v' + (j.version ?? 'â€”');\n",
    "  document.getElementById('windows').innerHTML = (j.windows||[]).map(w=>'<span class=\"chip\">'+w+'h</span>').join(' ');\n",
    "  document.getElementById('commit').textContent = j.commit ?? 'â€”';\n",
    "\n",
    "  if(j.best_external){\n",
    "    const be = j.best_external;\n",
    "    document.getElementById('best_external').textContent =\n",
    "      `${be.dataset} (n=${be.n_samples}, prev=${(be.prevalence*100).toFixed(1)}%): `\n",
    "      + `AUROC ${(be.auroc||0).toFixed(3)}, AUPRC ${(be.auprc||0).toFixed(3)}, Brier ${(be.brier||0).toFixed(3)}, `\n",
    "      + `Se ${(be.sensitivity||0).toFixed(3)}, Sp ${(be.specificity||0).toFixed(3)}, PPV ${(be.ppv||0).toFixed(3)}, NPV ${(be.npv||0).toFixed(3)}`;\n",
    "  }\n",
    "\n",
    "  const tb = document.querySelector('#thres_tbl tbody');\n",
    "  tb.innerHTML = '';\n",
    "  if(j.thresholds){\n",
    "    Object.entries(j.thresholds).forEach(([w,t])=>{\n",
    "      const tr = document.createElement('tr');\n",
    "      tr.innerHTML = `<td>${w}h</td><td>${(+t).toFixed(3)}</td>`;\n",
    "      tb.appendChild(tr);\n",
    "    });\n",
    "  }\n",
    "\n",
    "  const sub = document.querySelector('#sub_tbl tbody');\n",
    "  sub.innerHTML = '';\n",
    "  (j.subgroup_snapshot_12h||[]).forEach(s=>{\n",
    "    const tr = document.createElement('tr');\n",
    "    tr.innerHTML = `<td>${s[\"Subgroup Type\"]||''}</td><td>${s[\"Subgroup\"]||''}</td><td>${s[\"N\"]||''}</td><td>${(s[\"AUROC\"]||0).toFixed(3)}</td>`;\n",
    "    sub.appendChild(tr);\n",
    "  });\n",
    "\n",
    "  document.getElementById('raw').textContent = JSON.stringify(j, null, 2);\n",
    "}\n",
    "main();\n",
    "</script>\n",
    "</body>\n",
    "</html>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/performance/{window}/html\", response_class=HTMLResponse, tags=[\"Metrics\"])\n",
    "async def performance_html(window: int):\n",
    "    return r\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\" /><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "<title>Sentinel-ICU â€¢ Performance</title>\n",
    "<style>\n",
    "  :root{--bg:#0b1220;--panel:#111827;--muted:#94a3b8;--chip:#1f2937}\n",
    "  html,body{margin:0;background:var(--bg);color:#e5e7eb;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,\"Helvetica Neue\",Arial}\n",
    "  a{color:#93c5fd;text-decoration:none}\n",
    "  .wrap{max-width:900px;margin:24px auto;padding:0 16px}\n",
    "  .hdr{display:flex;align-items:center;gap:10px;margin-bottom:16px}\n",
    "  .chip{background:var(--chip);border:1px solid #374151;border-radius:999px;padding:4px 10px;font-size:12px}\n",
    "  .grid{display:grid;grid-template-columns:1fr;gap:12px}\n",
    "  @media(min-width:800px){.grid{grid-template-columns:1fr 1fr}}\n",
    "  .card{background:var(--panel);border:1px solid #1f2937;border-radius:12px;padding:14px}\n",
    "  .kv{display:grid;grid-template-columns:1fr 1fr;gap:6px 8px}\n",
    "  .kv div{padding:4px 0;border-bottom:1px solid #1f2937}\n",
    "  .muted{color:var(--muted)}\n",
    "  .foot{margin-top:10px;font-size:12px;color:var(--muted)}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"wrap\">\n",
    "  <div class=\"hdr\">\n",
    "    <h2 style=\"margin:0\">Performance <span id=\"w\" class=\"chip\">â€”</span></h2>\n",
    "  </div>\n",
    "  <div class=\"grid\">\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 8px\">MIMIC-IV</h3>\n",
    "      <div class=\"kv\" id=\"mimic\"></div>\n",
    "    </div>\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 8px\">eICU</h3>\n",
    "      <div class=\"kv\" id=\"eicu\"></div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"foot\">Source JSON: <a id=\"src\" href=\"#\" target=\"_blank\">(link)</a></div>\n",
    "</div>\n",
    "<script>\n",
    "function fillKV(el, obj){\n",
    "  const order = [\"auroc\",\"auprc\",\"brier\",\"ece\"];\n",
    "  const labels = {auroc:\"AUROC\", auprc:\"AUPRC\", brier:\"Brier\", ece:\"ECE\"};\n",
    "  el.innerHTML = order.map(k=>{\n",
    "    const v = obj?.[k]; return `<div class=\"muted\">${labels[k]}</div><div>${(v??0).toFixed(3)}</div>`;\n",
    "  }).join(\"\");\n",
    "}\n",
    "async function main(){\n",
    "  const path = location.pathname.replace(/\\/html$/,'');\n",
    "  document.getElementById('src').href = path;\n",
    "  const r = await fetch(path);\n",
    "  if(!r.ok){ document.body.innerHTML='<div class=\"wrap\"><div class=\"card\">Failed to load '+path+'</div></div>'; return; }\n",
    "  const j = await r.json();\n",
    "  document.getElementById('w').textContent = (j.window||'?')+'h';\n",
    "  fillKV(document.getElementById('mimic'), j.performance?.[\"MIMIC-IV\"]);\n",
    "  fillKV(document.getElementById('eicu'),  j.performance?.eICU || j.performance?.[\"eICU\"]);\n",
    "}\n",
    "main();\n",
    "</script>\n",
    "</body>\n",
    "</html>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/explain/{window}/html\", response_class=HTMLResponse, tags=[\"Explainability\"])\n",
    "async def explain_html(window: int):\n",
    "    return r\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\" /><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "<title>Sentinel-ICU â€¢ Explainability</title>\n",
    "<style>\n",
    "  :root{--bg:#0b1220;--panel:#111827;--muted:#94a3b8;--chip:#1f2937}\n",
    "  html,body{margin:0;background:var(--bg);color:#e5e7eb;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,\"Helvetica Neue\",Arial}\n",
    "  a{color:#93c5fd;text-decoration:none}\n",
    "  .wrap{max-width:980px;margin:24px auto;padding:0 16px}\n",
    "  .hdr{display:flex;align-items:center;gap:10px;margin-bottom:16px}\n",
    "  .chip{background:var(--chip);border:1px solid #374151;border-radius:999px;padding:4px 10px;font-size:12px}\n",
    "  .grid{display:grid;grid-template-columns:1fr;gap:12px}\n",
    "  @media(min-width:980px){.grid{grid-template-columns:1.1fr .9fr}}\n",
    "  .card{background:var(--panel);border:1px solid #1f2937;border-radius:12px;padding:14px}\n",
    "  table{width:100%;border-collapse:collapse;font-size:14px;margin-top:6px}\n",
    "  th,td{border-bottom:1px solid #1f2937;padding:8px;text-align:left}\n",
    "  .foot{margin-top:10px;font-size:12px;color:var(--muted)}\n",
    "  #chart{height:320px}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"wrap\">\n",
    "  <div class=\"hdr\">\n",
    "    <h2 style=\"margin:0\">Explainability <span id=\"w\" class=\"chip\">â€”</span></h2>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"grid\">\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 8px\">Top features (table)</h3>\n",
    "      <table id=\"tbl\">\n",
    "        <thead><tr><th>#</th><th>Feature</th><th>Importance</th><th>Norm</th></tr></thead>\n",
    "        <tbody></tbody>\n",
    "      </table>\n",
    "      <div class=\"foot\">Source JSON: <a id=\"src\" href=\"#\" target=\"_blank\">(link)</a></div>\n",
    "    </div>\n",
    "    <div class=\"card\">\n",
    "      <h3 style=\"margin:4px 0 8px\">Bar chart</h3>\n",
    "      <canvas id=\"chart\"></canvas>\n",
    "      <div class=\"foot\">If the chart library canâ€™t load (offline), the table above is your fallback.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<!-- Chart.js via CDN (tiny; safe to omit if offline) -->\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "<script>\n",
    "async function main(){\n",
    "  const path = location.pathname.replace(/\\/html$/,'');\n",
    "  document.getElementById('src').href = path;\n",
    "  const r = await fetch(path);\n",
    "  if(!r.ok){ document.body.innerHTML = '<div class=\"wrap\"><div class=\"card\">Failed to load '+path+'</div></div>'; return; }\n",
    "  const j = await r.json();\n",
    "  document.getElementById('w').textContent = (j.window||'?')+'h';\n",
    "\n",
    "  const feats = (j.features||[]).slice().sort((a,b)=> (b.importance||0)-(a.importance||0)).slice(0,15);\n",
    "  const tb = document.querySelector('#tbl tbody');\n",
    "  tb.innerHTML = '';\n",
    "  feats.forEach((f,i)=>{\n",
    "    const tr = document.createElement('tr');\n",
    "    tr.innerHTML = `<td>${i+1}</td><td>${f.feature}</td><td>${(f.importance||0).toFixed(3)}</td><td>${(f.importance_normalized||0).toFixed(3)}</td>`;\n",
    "    tb.appendChild(tr);\n",
    "  });\n",
    "\n",
    "  if(window.Chart){\n",
    "    const ctx = document.getElementById('chart');\n",
    "    new Chart(ctx, {\n",
    "      type: 'bar',\n",
    "      data: {\n",
    "        labels: feats.map(f=>f.feature),\n",
    "        datasets: [{label:'Importance (normalized)', data: feats.map(f=>f.importance_normalized||0)}]\n",
    "      },\n",
    "      options: {\n",
    "        indexAxis: 'y',\n",
    "        plugins:{legend:{display:false}},\n",
    "        scales:{x:{beginAtZero:true}}\n",
    "      }\n",
    "    });\n",
    "  }\n",
    "}\n",
    "main();\n",
    "</script>\n",
    "</body>\n",
    "</html>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 0: paths & imports (run once) ---\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, pickle, json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")  # <- adjust only if your project root moved\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "RESULTS = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "FEATURES_DIR = ROOT / \"outputs\" / \"features\"\n",
    "SHAP_DIR = ROOT / \"outputs\" / \"external_validation\" / \"shap\"\n",
    "\n",
    "MANU = ROOT / \"manuscript\"\n",
    "FIGS = MANU / \"figures\"\n",
    "TABS = MANU / \"tables\"\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "TABS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "\n",
    "def _load_model_artifacts(w):\n",
    "    with open(MODELS / f\"xgboost_w{w}.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(MODELS / f\"calibrator_w{w}.pkl\", \"rb\") as f:\n",
    "        calibrator = pickle.load(f)\n",
    "    with open(MODELS / f\"imputer_w{w}.pkl\", \"rb\") as f:\n",
    "        imputer = pickle.load(f)\n",
    "    feats = (MODELS / f\"features_w{w}.txt\").read_text().strip().splitlines()\n",
    "    return model, calibrator, imputer, feats\n",
    "\n",
    "def _ensure_eicu_predictions(w):\n",
    "    \"\"\"\n",
    "    Ensure we have eICU predictions file: results/eicu_preds_{w}h.csv\n",
    "    If missing, create it from combined_w{w}.parquet using saved model+imputer+calibrator.\n",
    "    \"\"\"\n",
    "    out_csv = RESULTS / f\"eicu_preds_{w}h.csv\"\n",
    "    if out_csv.exists():\n",
    "        return out_csv\n",
    "\n",
    "    feat_path = FEATURES_DIR / f\"combined_w{w}.parquet\"\n",
    "    assert feat_path.exists(), f\"Missing {feat_path}\"\n",
    "    df = pd.read_parquet(feat_path)\n",
    "\n",
    "    # We expect these columns from your pipeline\n",
    "    assert \"source\" in df.columns, \"combined parquet must include 'source' to filter eICU\"\n",
    "    assert \"label\" in df.columns, \"combined parquet must include binary 'label' outcome\"\n",
    "\n",
    "    eicu = df[df[\"source\"].str.lower().eq(\"eicu\")].copy()\n",
    "    model, calibrator, imputer, feats = _load_model_artifacts(w)\n",
    "\n",
    "    X = eicu[feats].copy()\n",
    "    X_imp = imputer.transform(X)\n",
    "    raw = model.predict_proba(X_imp)[:, 1]\n",
    "    prob = calibrator.predict_proba(raw.reshape(-1,1))[:, 1] if hasattr(calibrator, \"predict_proba\") else calibrator.transform(raw)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mortality_probability\": prob,\n",
    "        \"label\": eicu[\"label\"].astype(int)\n",
    "    })\n",
    "    # carry optional grouping columns if present\n",
    "    for col in [\"gender\",\"sex\",\"age_years\",\"hospital_id\",\"site\",\"unitid\",\"hospital\",\"admission_year\",\"icu_intime\",\"admittime\"]:\n",
    "        if col in eicu.columns and col not in out.columns:\n",
    "            out[col] = eicu[col]\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ“ Created {out_csv} (n={len(out)})\")\n",
    "    return out_csv\n",
    "\n",
    "def _load_threshold(w, default=0.5):\n",
    "    pth = MODELS / f\"threshold_w{w}.csv\"\n",
    "    if pth.exists():\n",
    "        try:\n",
    "            return float(pd.read_csv(pth)[\"threshold\"].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return float(default)\n",
    "\n",
    "def _logit(p):\n",
    "    p = np.clip(p, 1e-8, 1-1e-8)\n",
    "    return np.log(p/(1-p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sentinel-ICU â€¢ One-cell DCA generator \n",
    "# - Computes eICU probabilities per window using imputer + calibrator\n",
    "# - Normalizes features_w*.txt (one per line) to avoid shape mismatches\n",
    "# - Writes one clean set of outputs (overwrites old DCA tables/figures)\n",
    "# - Plots BMC-style DCA with zoom + optional Î” net benefit vs 12h panel\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, re, json, joblib, shutil, glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- config (adjust only if needed) ------------------------------------\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "FEAT   = ROOT / \"outputs\" / \"features\"\n",
    "\n",
    "OUT_T  = ROOT / \"manuscript\" / \"tables\"\n",
    "OUT_F  = ROOT / \"manuscript\" / \"figures\"\n",
    "OUT_T.mkdir(parents=True, exist_ok=True)\n",
    "OUT_F.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "CLEAN_OLD = True            # remove old tableG_dca_*.csv and fig6_dca_*.png first\n",
    "THRESH_GRID = np.linspace(0.01, 0.99, 99)   # standard DCA grid\n",
    "ZOOM_X = (0.05, 0.60)       # zoomed x-range to separate overlapping lines\n",
    "ZOOM_Y = (-5, 12)           # zoomed y-range (per 100 pts)\n",
    "CLINICAL_GUIDES = [0.1, 0.2, 0.3, 0.4]      # faint vertical guides on zoom\n",
    "MAKE_DELTA_VS_12H = True    # add Î” net benefit vs 12h plot\n",
    "SMOOTH_WINDOW = 3           # rolling smoothing points (>=1; 1 = no smoothing)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Optional tidy-up of older outputs\n",
    "if CLEAN_OLD:\n",
    "    for p in glob.glob(str(OUT_T / \"tableG_dca_*\")): \n",
    "        try: Path(p).unlink()\n",
    "        except: pass\n",
    "    for p in glob.glob(str(OUT_F / \"fig6_dca_*\")):\n",
    "        try: Path(p).unlink()\n",
    "        except: pass\n",
    "\n",
    "# ---- feature utilities (robust) ---------------------------------------------\n",
    "def _normalize_features_txt(w: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Ensures features_w*.txt is one feature per line (fix for shape mismatch).\n",
    "    If params_w*.json contains 'features', it takes precedence.\n",
    "    \"\"\"\n",
    "    params = MODELS / f\"params_w{w}.json\"\n",
    "    if params.exists():\n",
    "        try:\n",
    "            feats = json.loads(params.read_text(encoding=\"utf-8\")).get(\"features\", [])\n",
    "            feats = [f.strip() for f in feats if str(f).strip()]\n",
    "            if feats:\n",
    "                (MODELS / f\"features_w{w}.txt\").write_text(\"\\n\".join(feats), encoding=\"utf-8\")\n",
    "                return feats\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    txt = (MODELS / f\"features_w{w}.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # split on commas or whitespace, strip empties\n",
    "    feats = [t.strip() for t in re.split(r\"[,\\s]+\", txt) if t.strip()]\n",
    "    (MODELS / f\"features_w{w}.txt\").write_text(\"\\n\".join(feats), encoding=\"utf-8\")\n",
    "    return feats\n",
    "\n",
    "# ---- probability extraction --------------------------------------------------\n",
    "def _get_probs_from_artifacts(w: int):\n",
    "    \"\"\"\n",
    "    Loads imputer + calibrator, reads combined_w*.parquet (eICU rows),\n",
    "    aligns features and returns (y_true, prob, features_used).\n",
    "    \"\"\"\n",
    "    imp = joblib.load(MODELS / f\"imputer_w{w}.pkl\")\n",
    "    cal = joblib.load(MODELS / f\"calibrator_w{w}.pkl\")\n",
    "    feats = _normalize_features_txt(w)\n",
    "\n",
    "    df = pd.read_parquet(FEAT / f\"combined_w{w}.parquet\")\n",
    "    # ensure eICU subset\n",
    "    src_col = \"source\" if \"source\" in df.columns else \"dataset\" if \"dataset\" in df.columns else None\n",
    "    if src_col:\n",
    "        eicu = df[df[src_col].astype(str).str.lower().eq(\"eicu\")].copy()\n",
    "    else:\n",
    "        eicu = df.copy()\n",
    "\n",
    "    # find label\n",
    "    label_col = None\n",
    "    for cand in [\"label\",\"hospital_expire_flag\",\"y_true\",\"outcome\",\"death\",\"mortality\",\"hospital_death\"]:\n",
    "        if cand in eicu.columns:\n",
    "            label_col = cand\n",
    "            break\n",
    "    if label_col is None:\n",
    "        raise RuntimeError(f\"[{w}h] No binary label in combined_w{w}.parquet\")\n",
    "\n",
    "    # require exact feature columns (after normalization)\n",
    "    missing = [f for f in feats if f not in eicu.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[{w}h] Missing features in combined_w{w}.parquet: {missing}\")\n",
    "\n",
    "    # build X in the trained order\n",
    "    X = eicu[feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    # imputer dimensionality check\n",
    "    n_imp = int(getattr(imp, \"statistics_\", np.array([])).shape[0])\n",
    "    if X.shape[1] != n_imp:\n",
    "        raise RuntimeError(f\"[{w}h] Feature shape mismatch: X has {X.shape[1]}, imputer expects {n_imp}\")\n",
    "\n",
    "    X_imp = imp.transform(X.values)\n",
    "\n",
    "    # calibrated probabilities\n",
    "    if hasattr(cal, \"predict_proba\"):\n",
    "        p = cal.predict_proba(X_imp)[:, 1]\n",
    "    elif hasattr(cal, \"predict\"):\n",
    "        p = np.clip(np.asarray(cal.predict(X_imp)).ravel(), 0, 1)\n",
    "    else:\n",
    "        raise RuntimeError(f\"[{w}h] Calibrator has no predict/predict_proba\")\n",
    "\n",
    "    y = pd.to_numeric(eicu[label_col], errors=\"coerce\").fillna(0).astype(int).values\n",
    "    return y, p, feats\n",
    "\n",
    "# ---- decision curve analysis -------------------------------------------------\n",
    "def decision_curve(y_true, prob, thresholds=THRESH_GRID):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(prob).astype(float)\n",
    "    N = len(y)\n",
    "    prev = y.mean()\n",
    "\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        pred = (p >= t).astype(int)\n",
    "        TP = ((pred == 1) & (y == 1)).sum()\n",
    "        FP = ((pred == 1) & (y == 0)).sum()\n",
    "        # Net benefit per Vickers & Elkin (per patient)\n",
    "        nb_model = (TP / N) - (FP / N) * (t / (1 - t))\n",
    "        # Treat-all, treat-none references\n",
    "        nb_all   = prev - (1 - prev) * (t / (1 - t))\n",
    "        rows.append((t, nb_model*100.0, nb_all*100.0, 0.0))  # per 100 patients\n",
    "    return pd.DataFrame(rows, columns=[\"threshold\", \"nb_model\", \"nb_all\", \"nb_none\"])\n",
    "\n",
    "def _maybe_smooth(y, k=SMOOTH_WINDOW):\n",
    "    if k and k > 1:\n",
    "        return pd.Series(y).rolling(k, center=True, min_periods=1).mean().values\n",
    "    return y\n",
    "\n",
    "# ---- main -------------------------------------------------------------------\n",
    "curves = {}       # window -> DCA df\n",
    "processed = []\n",
    "failed = []\n",
    "\n",
    "for w in WINDOWS:\n",
    "    try:\n",
    "        y, p, feats = _get_probs_from_artifacts(w)\n",
    "        dca = decision_curve(y, p)\n",
    "        if SMOOTH_WINDOW > 1:\n",
    "            dca[\"nb_model\"] = _maybe_smooth(dca[\"nb_model\"], SMOOTH_WINDOW)\n",
    "        dca[\"window\"] = w\n",
    "        dca.to_csv(OUT_T / f\"tableG_dca_{w}h.csv\", index=False)\n",
    "        curves[w] = dca\n",
    "        processed.append(w)\n",
    "        print(f\"[w{w}] n={len(y):,} | feats={len(feats)} | prob=[{p.min():.3f},{p.max():.3f}] â†’ {OUT_T / f'tableG_dca_{w}h.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[w{w}] âœ— {e}\")\n",
    "        failed.append(w)\n",
    "\n",
    "if not curves:\n",
    "    raise SystemExit(\"No DCA curves produced. Check errors above.\")\n",
    "\n",
    "# Combined table\n",
    "comb = pd.concat(curves.values(), ignore_index=True)\n",
    "comb.to_csv(OUT_T / \"tableG_dca_all_windows.csv\", index=False)\n",
    "\n",
    "# --------- PLOTS --------------------------------------------------------------\n",
    "# (A) Full threshold range\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "for w in processed:\n",
    "    d = curves[w]\n",
    "    plt.plot(d[\"threshold\"], d[\"nb_model\"], label=f\"{w}h\", linewidth=2, alpha=0.9)\n",
    "# references\n",
    "w0 = processed[0]\n",
    "plt.plot(curves[w0][\"threshold\"], curves[w0][\"nb_all\"], linestyle=\"--\", linewidth=1.5, alpha=0.7, label=\"Treat all\")\n",
    "plt.axhline(0, linewidth=1, alpha=0.7, label=\"Treat none\")\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit (per 100 patients)\")\n",
    "plt.title(\"Decision Curve Analysis across windows\")\n",
    "plt.legend()\n",
    "out_full = OUT_F / \"fig6_dca_all_windows.png\"\n",
    "plt.tight_layout(); plt.savefig(out_full, dpi=300); plt.show()\n",
    "\n",
    "# (B) Zoomed range with clinical guides\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "for w in processed:\n",
    "    d = curves[w]\n",
    "    plt.plot(d[\"threshold\"], d[\"nb_model\"], label=f\"{w}h\", linewidth=2, alpha=0.95)\n",
    "# references\n",
    "plt.plot(curves[w0][\"threshold\"], curves[w0][\"nb_all\"], linestyle=\"--\", linewidth=1.5, alpha=0.7, label=\"Treat all\")\n",
    "plt.axhline(0, linewidth=1, alpha=0.7, label=\"Treat none\")\n",
    "# clinical guides\n",
    "for gx in CLINICAL_GUIDES:\n",
    "    plt.axvline(gx, linestyle=\":\", linewidth=1, alpha=0.2)\n",
    "plt.xlim(*ZOOM_X); plt.ylim(*ZOOM_Y)\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit (per 100 patients)\")\n",
    "plt.title(\"Decision Curve Analysis (zoomed)\")\n",
    "plt.legend()\n",
    "out_zoom = OUT_F / \"fig6_dca_zoom.png\"\n",
    "plt.tight_layout(); plt.savefig(out_zoom, dpi=300); plt.show()\n",
    "\n",
    "# (C) Optional Î” net benefit vs 12h (helps separate curves visually)\n",
    "if MAKE_DELTA_VS_12H and 12 in processed:\n",
    "    base = curves[12][[\"threshold\",\"nb_model\"]].rename(columns={\"nb_model\":\"base\"})\n",
    "    plt.figure(figsize=(7.6, 4.6))\n",
    "    for w in processed:\n",
    "        d = curves[w][[\"threshold\",\"nb_model\"]].merge(base, on=\"threshold\", how=\"inner\")\n",
    "        d[\"delta\"] = d[\"nb_model\"] - d[\"base\"]\n",
    "        if SMOOTH_WINDOW > 1:\n",
    "            d[\"delta\"] = _maybe_smooth(d[\"delta\"], SMOOTH_WINDOW)\n",
    "        plt.plot(d[\"threshold\"], d[\"delta\"], label=f\"{w}h âˆ’ 12h\", linewidth=2, alpha=0.95)\n",
    "    plt.axhline(0, linewidth=1, alpha=0.7)\n",
    "    for gx in CLINICAL_GUIDES:\n",
    "        plt.axvline(gx, linestyle=\":\", linewidth=1, alpha=0.2)\n",
    "    plt.xlim(*ZOOM_X)\n",
    "    plt.xlabel(\"Threshold probability\")\n",
    "    plt.ylabel(\"Î” Net benefit vs 12h (per 100 patients)\")\n",
    "    plt.title(\"Î” Net benefit by window (relative to 12h)\")\n",
    "    plt.legend()\n",
    "    out_delta = OUT_F / \"fig6_dca_delta_vs_12h.png\"\n",
    "    plt.tight_layout(); plt.savefig(out_delta, dpi=300); plt.show()\n",
    "else:\n",
    "    out_delta = None\n",
    "\n",
    "print(\"\\nâœ“ Outputs\")\n",
    "print(\"  Tables:\")\n",
    "print(f\"   - {OUT_T / 'tableG_dca_all_windows.csv'}\")\n",
    "for w in processed:\n",
    "    print(f\"   - {OUT_T / f'tableG_dca_{w}h.csv'}\")\n",
    "print(\"  Figures:\")\n",
    "print(f\"   - {out_full}\")\n",
    "print(f\"   - {out_zoom}\")\n",
    "if out_delta:\n",
    "    print(f\"   - {out_delta}\")\n",
    "if failed:\n",
    "    print(f\"\\nâš ï¸ Windows failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust hospital meta-analysis for AUROC \n",
    "# Saves tables + forest plots for 6/12/18/24h\n",
    "# Filters small hospitals \n",
    "# DeLong SE per hospital\n",
    "# Random-effects meta-analysis on logit(AUROC) (DerSimonianâ€“Laird)\n",
    "# Also reports a within-site (stratified) AUROC\n",
    "#\n",
    "# Outputs:\n",
    "#   manuscript/tables/site_auc_filtered_w{6|12|18|24}h.csv\n",
    "#   manuscript/figures/fig_site_forest_filtered_w{6|12|18|24}h.png\n",
    "#   manuscript/tables/site_meta_summary.csv (one row per window)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle, gzip, warnings\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES  = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "FEAT = ROOT / \"outputs\" / \"features\"\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "EICU_DIR = ROOT / \"eicu\"\n",
    "TAB = ROOT / \"manuscript\" / \"tables\"\n",
    "FIG = ROOT / \"manuscript\" / \"figures\"\n",
    "TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "MIN_POS = 20   # filter: at least 20 deaths\n",
    "MIN_NEG = 20   # and at least 20 survivors\n",
    "EPS = 1e-6     \n",
    "\n",
    "# ---------- DeLong variance helpers (vectorized midranks implementation) ----------\n",
    "# Adapted from the standard DeLong approach (AUC variance)\n",
    "def _compute_midrank(x):\n",
    "    # returns midranks for x\n",
    "    order = np.argsort(x)\n",
    "    x_sorted = x[order]\n",
    "    J = len(x)\n",
    "    midranks = np.zeros(J, dtype=float)\n",
    "    i = 0\n",
    "    while i < J:\n",
    "        j = i\n",
    "        while j < J and x_sorted[j] == x_sorted[i]:\n",
    "            j += 1\n",
    "        mid = 0.5*(i + j - 1) + 1  # 1-based midrank\n",
    "        midranks[i:j] = mid\n",
    "        i = j\n",
    "    out = np.empty(J, dtype=float)\n",
    "    out[order] = midranks\n",
    "    return out\n",
    "\n",
    "def _fast_delong(y_true, y_score):\n",
    "    # y_true in {0,1}, y_score continuous\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    pos = y_score[y_true == 1]\n",
    "    neg = y_score[y_true == 0]\n",
    "    m, n = len(pos), len(neg)\n",
    "    if m == 0 or n == 0:\n",
    "        return np.nan, np.nan\n",
    "    # midranks on concatenated scores\n",
    "    all_scores = np.concatenate([pos, neg])\n",
    "    mr_all = _compute_midrank(all_scores)\n",
    "    mr_pos = _compute_midrank(pos)\n",
    "    mr_neg = _compute_midrank(neg)\n",
    "    V10 = (mr_all[:m] - mr_pos) / n\n",
    "    V01 = 1.0 - (mr_all[m:] - mr_neg) / m\n",
    "    auc = (V10.sum() / m)\n",
    "    # DeLong variance\n",
    "    s10 = np.var(V10, ddof=1)\n",
    "    s01 = np.var(V01, ddof=1)\n",
    "    var = s10 / m + s01 / n\n",
    "    return auc, max(var, 1e-12)\n",
    "\n",
    "# ---------- RE meta-analysis on logit(AUC) (DerSimonianâ€“Laird) ----------\n",
    "def _logit(p):\n",
    "    p = np.clip(p, EPS, 1-EPS)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def _inv_logit(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def meta_random_effects_logit_auc(aucs, var_auc):\n",
    "    aucs = np.asarray(aucs, float)\n",
    "    var_auc = np.asarray(var_auc, float)\n",
    "    m = np.isfinite(aucs) & np.isfinite(var_auc) & (var_auc > 0)\n",
    "    aucs = aucs[m]; var_auc = var_auc[m]\n",
    "    if aucs.size == 0:\n",
    "        return None\n",
    "    # delta method: var(logit(AUC)) â‰ˆ var(AUC) / [AUC*(1-AUC)]^2\n",
    "    lg = _logit(aucs)\n",
    "    deriv = 1.0 / (aucs*(1-aucs))\n",
    "    var_lg = var_auc * (deriv**2)\n",
    "    w = 1.0/var_lg\n",
    "    # FE summary on logit scale\n",
    "    theta_fe = np.sum(w*lg)/np.sum(w)\n",
    "    Q = np.sum(w*(lg - theta_fe)**2)\n",
    "    df = len(lg) - 1\n",
    "    C = np.sum(w) - (np.sum(w**2)/np.sum(w))\n",
    "    tau2 = max(0.0, (Q-df)/C) if C > 0 else 0.0\n",
    "    w_re = 1.0/(var_lg + tau2)\n",
    "    theta_re = np.sum(w_re*lg)/np.sum(w_re)\n",
    "    se_re = np.sqrt(1.0/np.sum(w_re))\n",
    "    ci_low_lg = theta_re - 1.96*se_re\n",
    "    ci_high_lg = theta_re + 1.96*se_re\n",
    "    pooled_auc = _inv_logit(theta_re)\n",
    "    ci_low = _inv_logit(ci_low_lg)\n",
    "    ci_high = _inv_logit(ci_high_lg)\n",
    "    I2 = float(max(0.0, (Q-df)/Q)*100) if Q > 0 else 0.0\n",
    "    return {\n",
    "        \"k\": int(len(lg)),\n",
    "        \"theta_re_logit\": float(theta_re),\n",
    "        \"pooled_auc\": float(pooled_auc),\n",
    "        \"ci_low\": float(ci_low),\n",
    "        \"ci_high\": float(ci_high),\n",
    "        \"Q\": float(Q),\n",
    "        \"df\": int(df),\n",
    "        \"I2\": float(I2),\n",
    "        \"tau2_logit\": float(tau2)\n",
    "    }\n",
    "\n",
    "# ---------- Plot ----------\n",
    "def forest_plot_site_auc(df_sites, pooled, title, out_png):\n",
    "    if df_sites.empty:\n",
    "        warnings.warn(f\"No eligible hospitals to plot for {title}\")\n",
    "        return\n",
    "    d = df_sites.sort_values(\"AUROC\")\n",
    "    y = np.arange(len(d))\n",
    "    auc = d[\"AUROC\"].values\n",
    "    se = d[\"SE\"].values\n",
    "    ci_lo = np.clip(auc - 1.96*se, 0.5, 1.0)\n",
    "    ci_hi = np.clip(auc + 1.96*se, 0.5, 1.0)\n",
    "    labels = [f\"H{int(h)} (n={int(n)}, pos={int(p)})\"\n",
    "              for h,n,p in zip(d[\"hospitalid\"], d[\"N\"], d[\"Pos\"])]\n",
    "\n",
    "    plt.figure(figsize=(9.2, max(4, 0.24*len(d)+1)))\n",
    "    plt.hlines(y, ci_lo, ci_hi, lw=2)\n",
    "    plt.plot(auc, y, \"o\", markersize=5)\n",
    "    ytick = y; ylab = labels\n",
    "\n",
    "    if pooled is not None:\n",
    "        y0 = -1.8\n",
    "        plt.hlines(y0, pooled[\"ci_low\"], pooled[\"ci_high\"], lw=4, color=\"tab:red\")\n",
    "        plt.plot([pooled[\"pooled_auc\"]], [y0], \"s\", color=\"tab:red\", markersize=8)\n",
    "        ytick = np.concatenate([[y0], y])\n",
    "        ylab = [\"Pooled (RE, logit)\"] + labels\n",
    "\n",
    "    plt.yticks(ytick, ylab)\n",
    "    plt.xlabel(\"AUROC\"); plt.xlim(0.5, 1.0)\n",
    "    plt.title(title)\n",
    "    plt.grid(axis=\"x\", alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Rebuild preds CSV with hospitalid if missing ----------\n",
    "def ensure_preds_with_hospitalid(window_h: int) -> Path:\n",
    "    preds = RES / f\"eicu_preds_{window_h}h.csv\"\n",
    "    if preds.exists():\n",
    "        df = pd.read_csv(preds)\n",
    "        if {\"hospitalid\"}.issubset(df.columns):\n",
    "            return preds\n",
    "        # else fall-through to rebuild\n",
    "    print(f\"âš ï¸ {preds.name} missing 'hospitalid' â†’ rebuilding from combined + artifacts\")\n",
    "    comb = FEAT / f\"combined_w{window_h}.parquet\"\n",
    "    if not comb.exists():\n",
    "        raise FileNotFoundError(f\"Missing {comb}\")\n",
    "    dfc = pd.read_parquet(comb)\n",
    "    eicu = dfc[dfc[\"source\"].str.lower().eq(\"eicu\")].copy()\n",
    "\n",
    "    # Load features + imputer + calibrator\n",
    "    feats_txt = MODELS / f\"features_w{window_h}.txt\"\n",
    "    imputer_pkl = MODELS / f\"imputer_w{window_h}.pkl\"\n",
    "    calibr_pkl = MODELS / f\"calibrator_w{window_h}.pkl\"\n",
    "    if not feats_txt.exists():\n",
    "        raise FileNotFoundError(feats_txt)\n",
    "    features = [s.strip() for s in feats_txt.read_text().split(\",\") if s.strip()]\n",
    "    with open(imputer_pkl, \"rb\") as f:\n",
    "        imputer = pickle.load(f)\n",
    "    with open(calibr_pkl, \"rb\") as f:\n",
    "        calibrator = pickle.load(f)\n",
    "\n",
    "    X = eicu[features].copy()\n",
    "    X_imp = imputer.transform(X.values)\n",
    "    # Try predict_proba then predict\n",
    "    if hasattr(calibrator, \"predict_proba\"):\n",
    "        proba = calibrator.predict_proba(X_imp)[:,1]\n",
    "    else:\n",
    "        proba = calibrator.predict(X_imp).astype(float)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"stay_id\": eicu[\"stay_id\"].values,\n",
    "        \"label\": eicu[\"hospital_expire_flag\"].astype(int).values,\n",
    "        \"mortality_probability\": proba\n",
    "    })\n",
    "\n",
    "    # Map hospitalid from eICU patient.csv.gz using stay_id==patientunitstayid\n",
    "    pat_csv = EICU_DIR / \"patient.csv.gz\"\n",
    "    if not pat_csv.exists():\n",
    "        raise FileNotFoundError(pat_csv)\n",
    "    # read only needed cols\n",
    "    df_pat = pd.read_csv(pat_csv, usecols=[\"patientunitstayid\", \"hospitalid\"])\n",
    "    out = out.merge(df_pat.rename(columns={\"patientunitstayid\": \"stay_id\"}),\n",
    "                    on=\"stay_id\", how=\"left\")\n",
    "    out.to_csv(preds, index=False)\n",
    "    print(f\"âœ“ (re)built {preds} with hospitalid (n={len(out)})\")\n",
    "    return preds\n",
    "\n",
    "# ---------- Within-site (stratified) AUROC ----------\n",
    "def stratified_within_site_auc(df):\n",
    "    tot_pairs = 0\n",
    "    concord = 0\n",
    "    ties = 0\n",
    "    for hid, g in df.dropna(subset=[\"hospitalid\"]).groupby(\"hospitalid\"):\n",
    "        y = g[\"label\"].astype(int).values\n",
    "        p = g[\"mortality_probability\"].astype(float).values\n",
    "        pos = p[y==1]; neg = p[y==0]\n",
    "        if len(pos)==0 or len(neg)==0: continue\n",
    "        P, N = len(pos), len(neg)\n",
    "        tot_pairs += P*N\n",
    "        vp = pos.reshape(-1,1); vn = neg.reshape(1,-1)\n",
    "        concord += (vp > vn).sum()\n",
    "        ties    += (vp == vn).sum()\n",
    "    if tot_pairs == 0: return np.nan\n",
    "    return (concord + 0.5*ties) / tot_pairs\n",
    "\n",
    "# ---------- Main loop ----------\n",
    "summary_rows = []\n",
    "for w in WINDOWS:\n",
    "    preds = ensure_preds_with_hospitalid(w)\n",
    "    df = pd.read_csv(preds)\n",
    "    # overall sanity check\n",
    "    overall = roc_auc_score(df[\"label\"].astype(int), df[\"mortality_probability\"].astype(float))\n",
    "    # per-hospital\n",
    "    site_rows = []\n",
    "    for hid, g in df.dropna(subset=[\"hospitalid\"]).groupby(\"hospitalid\"):\n",
    "        y = g[\"label\"].astype(int).values\n",
    "        p = g[\"mortality_probability\"].astype(float).values\n",
    "        n1, n0 = int(y.sum()), int((1-y).sum())\n",
    "        if (n1 < MIN_POS) or (n0 < MIN_NEG):\n",
    "            continue\n",
    "        try:\n",
    "            auc_h, var_h = _fast_delong(y, p)   # DeLong AUROC & var\n",
    "            auc_h = float(np.clip(auc_h, 0.5+EPS, 1-EPS))\n",
    "            se_h = float(np.sqrt(var_h))\n",
    "        except Exception:\n",
    "            continue\n",
    "        site_rows.append({\n",
    "            \"hospitalid\": int(hid),\n",
    "            \"N\": int(len(g)),\n",
    "            \"Pos\": n1,\n",
    "            \"Neg\": n0,\n",
    "            \"AUROC\": auc_h,\n",
    "            \"SE\": se_h,\n",
    "            \"CI_low\": max(0.5, auc_h - 1.96*se_h),\n",
    "            \"CI_high\": min(1.0, auc_h + 1.96*se_h),\n",
    "        })\n",
    "    site_df = pd.DataFrame(site_rows).sort_values(\"N\", ascending=False)\n",
    "    out_csv = TAB / f\"site_auc_filtered_w{w}h.csv\"\n",
    "    site_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    pooled = None\n",
    "    if not site_df.empty:\n",
    "        pooled = meta_random_effects_logit_auc(site_df[\"AUROC\"].values,\n",
    "                                               (site_df[\"SE\"].values**2))\n",
    "    forest_png = FIG / f\"fig_site_forest_filtered_w{w}h.png\"\n",
    "    forest_title = f\"Hospital AUROC (eICU, {w}h) â€” filtered (â‰¥{MIN_POS} pos & â‰¥{MIN_NEG} neg)\"\n",
    "    forest_plot_site_auc(site_df, pooled, forest_title, forest_png)\n",
    "\n",
    "    within_site_auc = stratified_within_site_auc(df)\n",
    "\n",
    "    print(f\"[{w}h] overall={overall:.3f} | eligible hospitals={len(site_df)}\")\n",
    "    if pooled:\n",
    "        print(f\"      pooled RE (logit)={pooled['pooled_auc']:.3f} \"\n",
    "              f\"[{pooled['ci_low']:.3f},{pooled['ci_high']:.3f}] | IÂ²={pooled['I2']:.1f}%\")\n",
    "    print(f\"      within-site stratified AUROC={within_site_auc:.3f}\")\n",
    "    print(f\"      â†’ {out_csv}\\n      â†’ {forest_png}\")\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"window_h\": w,\n",
    "        \"overall_eICU_AUROC\": overall,\n",
    "        \"eligible_hospitals\": len(site_df),\n",
    "        \"pooled_RE_AUROC\": pooled[\"pooled_auc\"] if pooled else np.nan,\n",
    "        \"pooled_CI_low\": pooled[\"ci_low\"] if pooled else np.nan,\n",
    "        \"pooled_CI_high\": pooled[\"ci_high\"] if pooled else np.nan,\n",
    "        \"I2_percent\": pooled[\"I2\"] if pooled else np.nan,\n",
    "        \"within_site_stratified_AUROC\": within_site_auc\n",
    "    })\n",
    "\n",
    "# Write summary across windows\n",
    "pd.DataFrame(summary_rows).to_csv(TAB / \"site_meta_summary.csv\", index=False)\n",
    "print(f\"âœ“ Saved summary â†’ {TAB / 'site_meta_summary.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness + Calibration\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "FEAT   = ROOT / \"outputs\" / \"features\"\n",
    "RES    = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "EICU   = ROOT / \"eicu\" / \"patient.csv.gz\"\n",
    "TAB    = ROOT / \"manuscript\" / \"tables\";  TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG    = ROOT / \"manuscript\" / \"figures\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "EPS = 1e-8\n",
    "\n",
    "# --- small helper: logistic calibration (intercept & slope) via IRLS ---\n",
    "def _logit(p):\n",
    "    p = np.clip(p, EPS, 1-EPS)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def calibr_slope_intercept(y, p, max_iter=50, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Fit logistic regression: y ~ intercept + slope * logit(p)\n",
    "    Returns (intercept, slope). IRLS; no sklearn/statsmodels required.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    x = _logit(p)\n",
    "    X = np.column_stack([np.ones_like(x), x])  # [intercept, slope]\n",
    "    beta = np.zeros(2, dtype=float)\n",
    "    for _ in range(max_iter):\n",
    "        z = X @ beta\n",
    "        mu = 1.0 / (1.0 + np.exp(-z))\n",
    "        W = mu * (1.0 - mu) + EPS\n",
    "        # IRLS step: beta_new = beta + (X^T W X)^-1 X^T (y-mu)\n",
    "        XT_W = X.T * W\n",
    "        H = XT_W @ X\n",
    "        g = X.T @ (y - mu)\n",
    "        try:\n",
    "            step = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break\n",
    "        beta_new = beta + step\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "    return float(beta[0]), float(beta[1])\n",
    "\n",
    "def ece_bins(y, p, n_bins=10):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    bins = pd.qcut(p, q=n_bins, duplicates=\"drop\")\n",
    "    df = pd.DataFrame({\"y\": y, \"p\": p, \"bin\": bins})\n",
    "    grp = df.groupby(\"bin\", observed=True)\n",
    "    out = grp.agg(n=(\"y\",\"size\"),\n",
    "                  mean_p=(\"p\",\"mean\"),\n",
    "                  obs=(\"y\",\"mean\")).reset_index(drop=True)\n",
    "    out[\"abs_gap\"] = np.abs(out[\"obs\"] - out[\"mean_p\"])\n",
    "    ece = np.average(out[\"abs_gap\"], weights=out[\"n\"])\n",
    "    return ece, out\n",
    "\n",
    "def tpr_fpr_at_threshold(y, p, thr):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    pred = (p >= thr).astype(int)\n",
    "    TP = ((pred==1) & (y==1)).sum()\n",
    "    FP = ((pred==1) & (y==0)).sum()\n",
    "    TN = ((pred==0) & (y==0)).sum()\n",
    "    FN = ((pred==0) & (y==1)).sum()\n",
    "    tpr = TP / max(TP+FN, 1)\n",
    "    fpr = FP / max(FP+TN, 1)\n",
    "    return float(tpr), float(fpr), int(TP), int(FP), int(TN), int(FN)\n",
    "\n",
    "def _read_threshold(w):\n",
    "    p = MODELS / f\"threshold_w{w}.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return float(pd.read_csv(p).iloc[0][\"threshold\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None  # will still compute metrics without a hard operating point\n",
    "\n",
    "def _prep_window(w):\n",
    "    # predictions (must have: stay_id, label, mortality_probability)\n",
    "    preds_csv = RES / f\"eicu_preds_{w}h.csv\"\n",
    "    if not preds_csv.exists():\n",
    "        raise FileNotFoundError(preds_csv)\n",
    "    pred = pd.read_csv(preds_csv)\n",
    "\n",
    "    # features parquet (for age_years) â€“ eICU subset only\n",
    "    feat_pq = FEAT / f\"combined_w{w}.parquet\"\n",
    "    if not feat_pq.exists():\n",
    "        raise FileNotFoundError(feat_pq)\n",
    "    feat = pd.read_parquet(feat_pq)\n",
    "    feat = feat[feat[\"source\"].astype(str).str.lower().eq(\"eicu\")].copy()\n",
    "\n",
    "    # pull age\n",
    "    if \"stay_id\" not in feat.columns:\n",
    "        raise ValueError(f\"{feat_pq} missing stay_id\")\n",
    "    cols = [\"stay_id\",\"age_years\"]\n",
    "    cols = [c for c in cols if c in feat.columns]\n",
    "    feat = feat[cols].drop_duplicates(\"stay_id\")\n",
    "\n",
    "    # gender from eICU patient.csv.gz\n",
    "    eicu_pat = pd.read_csv(EICU, usecols=[\"patientunitstayid\",\"gender\"])\n",
    "    eicu_pat = eicu_pat.rename(columns={\"patientunitstayid\":\"stay_id\"})\n",
    "\n",
    "    # merge everything\n",
    "    df = pred.merge(feat, on=\"stay_id\", how=\"left\").merge(eicu_pat, on=\"stay_id\", how=\"left\")\n",
    "    # Normalize gender labels\n",
    "    df[\"gender\"] = df[\"gender\"].astype(str).str.strip().str.upper().map(\n",
    "        {\"M\":\"Male\",\"MALE\":\"Male\",\"F\":\"Female\",\"FEMALE\":\"Female\"}\n",
    "    ).fillna(\"Unknown\")\n",
    "    # Age bins\n",
    "    df[\"age_group\"] = np.where(df[\"age_years\"]>=65, \"â‰¥65\", \"<65\")\n",
    "    return df\n",
    "\n",
    "def _reliability_plot(df, by, title, out_png):\n",
    "    plt.figure(figsize=(6.6, 4.2))\n",
    "    groups = df[by].dropna().unique().tolist()\n",
    "    groups = [g for g in groups if g != \"Unknown\"] or [\"All\"]\n",
    "    colors = None  # default cycle\n",
    "\n",
    "    for g in groups:\n",
    "        sub = df[df[by]==g] if g!=\"All\" else df\n",
    "        if len(sub) < 50:\n",
    "            continue\n",
    "        ece, bins = ece_bins(sub[\"label\"], sub[\"mortality_probability\"], n_bins=10)\n",
    "        # step curve\n",
    "        xs = bins[\"mean_p\"].values\n",
    "        ys = bins[\"obs\"].values\n",
    "        order = np.argsort(xs)\n",
    "        plt.plot(xs[order], ys[order], marker=\"o\", label=f\"{g} (ECE={ece:.03f})\")\n",
    "\n",
    "    # reference line\n",
    "    plt.plot([0,1],[0,1], ls=\"--\", lw=1, color=\"gray\", label=\"Perfect calibration\")\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Observed event rate\")\n",
    "    plt.title(title)\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- main ----------\n",
    "summary_rows = []\n",
    "for w in WINDOWS:\n",
    "    try:\n",
    "        df = _prep_window(w)\n",
    "        thr = _read_threshold(w)\n",
    "\n",
    "        y = df[\"label\"].astype(int).values\n",
    "        p = df[\"mortality_probability\"].astype(float).values\n",
    "\n",
    "        # Overall metrics\n",
    "        auroc = roc_auc_score(y, p)\n",
    "        auprc = average_precision_score(y, p)\n",
    "        brier = brier_score_loss(y, p)\n",
    "        intercept, slope = calibr_slope_intercept(y, p)\n",
    "        ece, eceb = ece_bins(y, p, n_bins=10)\n",
    "\n",
    "        # Save calib bins table\n",
    "        eceb.assign(window_h=w).to_csv(TAB / f\"calibration_bins_w{w}h.csv\", index=False)\n",
    "\n",
    "        # Fairness splits\n",
    "        rows = []\n",
    "        for group_name, series in {\n",
    "            \"All\": pd.Series([\"All\"]*len(df), index=df.index),\n",
    "            \"age_group\": df[\"age_group\"],\n",
    "            \"gender\": df[\"gender\"].replace(\"Unknown\", np.nan)  # drop unknown from fairness summaries\n",
    "        }.items():\n",
    "            for gval, sub in df.loc[series.dropna().index].groupby(series.dropna()):\n",
    "                y_g = sub[\"label\"].astype(int).values\n",
    "                p_g = sub[\"mortality_probability\"].astype(float).values\n",
    "                if len(y_g) < 50 or len(np.unique(y_g)) < 2:\n",
    "                    # too small / degenerate, skip\n",
    "                    continue\n",
    "                auroc_g = roc_auc_score(y_g, p_g)\n",
    "                auprc_g = average_precision_score(y_g, p_g)\n",
    "                brier_g = brier_score_loss(y_g, p_g)\n",
    "                icpt_g, slope_g = calibr_slope_intercept(y_g, p_g)\n",
    "                ece_g, _ = ece_bins(y_g, p_g, n_bins=10)\n",
    "                tpr=fpr=TP=FP=TN=FN=np.nan\n",
    "                if thr is not None:\n",
    "                    tpr, fpr, TP, FP, TN, FN = tpr_fpr_at_threshold(y_g, p_g, thr)\n",
    "                rows.append({\n",
    "                    \"window_h\": w,\n",
    "                    \"group_type\": group_name,\n",
    "                    \"group\": gval,\n",
    "                    \"N\": int(len(sub)),\n",
    "                    \"Prevalence\": float(y_g.mean()),\n",
    "                    \"AUROC\": float(auroc_g),\n",
    "                    \"AUPRC\": float(auprc_g),\n",
    "                    \"Brier\": float(brier_g),\n",
    "                    \"Calib_intercept\": icpt_g,\n",
    "                    \"Calib_slope\": slope_g,\n",
    "                    \"ECE_10bin\": float(ece_g),\n",
    "                    \"TPR_at_thr\": tpr,\n",
    "                    \"FPR_at_thr\": fpr,\n",
    "                    \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN\n",
    "                })\n",
    "\n",
    "        # Write per-window fairness table\n",
    "        fair_df = pd.DataFrame(rows)\n",
    "        fair_csv = TAB / f\"fairness_calibration_w{w}h.csv\"\n",
    "        fair_df.to_csv(fair_csv, index=False)\n",
    "\n",
    "        # Reliability plots\n",
    "        _reliability_plot(df, by=\"age_group\",\n",
    "                          title=f\"Calibration by age (eICU, {w}h)\",\n",
    "                          out_png=FIG / f\"fig_calibration_age_w{w}h.png\")\n",
    "        _reliability_plot(df, by=\"gender\",\n",
    "                          title=f\"Calibration by gender (eICU, {w}h)\",\n",
    "                          out_png=FIG / f\"fig_calibration_gender_w{w}h.png\")\n",
    "\n",
    "        # Quick bar of AUROC by subgroup (age & gender)\n",
    "        fig_bar = FIG / f\"fig_auroc_subgroups_w{w}h.png\"\n",
    "        sub_auc = fair_df[fair_df[\"group_type\"].isin([\"age_group\",\"gender\"])]\n",
    "        if not sub_auc.empty:\n",
    "            plt.figure(figsize=(6.8, 3.8))\n",
    "            for i, (gt, gdf) in enumerate(sub_auc.groupby(\"group_type\")):\n",
    "                gdf = gdf.sort_values(\"AUROC\", ascending=False)\n",
    "                plt.bar([f\"{gt}:{g}\" for g in gdf[\"group\"]], gdf[\"AUROC\"])\n",
    "            plt.xticks(rotation=30, ha=\"right\")\n",
    "            plt.ylim(0.6, 0.95); plt.ylabel(\"AUROC\")\n",
    "            plt.title(f\"Subgroup AUROC (eICU, {w}h)\")\n",
    "            plt.tight_layout(); plt.savefig(fig_bar, dpi=300); plt.close()\n",
    "\n",
    "        # Add overall row to summary\n",
    "        summary_rows.append({\n",
    "            \"window_h\": w,\n",
    "            \"N_eICU\": int(len(df)),\n",
    "            \"Prevalence\": float(y.mean()),\n",
    "            \"AUROC\": float(auroc),\n",
    "            \"AUPRC\": float(auprc),\n",
    "            \"Brier\": float(brier),\n",
    "            \"Calib_intercept\": intercept,\n",
    "            \"Calib_slope\": slope,\n",
    "            \"ECE_10bin\": float(ece),\n",
    "            \"thr_used\": thr\n",
    "        })\n",
    "\n",
    "        print(f\"[{w}h] N={len(df)} | AUROC={auroc:.3f} | AUPRC={auprc:.3f} | \"\n",
    "              f\"Brier={brier:.3f} | slope={slope:.3f} | intercept={intercept:.3f} | ECE={ece:.3f}\")\n",
    "        print(f\"     â†’ {fair_csv}\")\n",
    "        print(f\"     â†’ {FIG / f'fig_calibration_age_w{w}h.png'}\")\n",
    "        print(f\"     â†’ {FIG / f'fig_calibration_gender_w{w}h.png'}\")\n",
    "        print(f\"     â†’ {FIG / f'fig_auroc_subgroups_w{w}h.png'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{w}h] âœ— {e}\")\n",
    "\n",
    "# Write overall summary across windows\n",
    "if summary_rows:\n",
    "    pd.DataFrame(summary_rows).to_csv(TAB / \"fairness_calibration_summary.csv\", index=False)\n",
    "    print(\"âœ“ Saved:\", TAB / \"fairness_calibration_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Thresholded subgroup metrics with robust gender handling ===\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES    = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "FEAT   = ROOT / \"outputs\" / \"features\"\n",
    "TAB    = ROOT / \"manuscript\" / \"tables\";  TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG    = ROOT / \"manuscript\" / \"figures\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "\n",
    "def _thr(w):\n",
    "    p = MODELS / f\"threshold_w{w}.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return float(pd.read_csv(p).iloc[0][\"threshold\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallbacks you used earlier\n",
    "    return {6:0.143, 12:0.360, 18:0.250, 24:0.112}.get(w, 0.36)\n",
    "\n",
    "def _safe_rate(num, den):\n",
    "    return float(num/den) if den and den > 0 else np.nan\n",
    "\n",
    "def _gender_labels(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns array of 'Male'/'Female'/'Unknown' with same length as df.\n",
    "    Tries several column names; if none found, returns all 'Unknown'.\n",
    "    \"\"\"\n",
    "    cand_cols = [c for c in [\"gender\",\"sex\",\"Gender\",\"Sex\"] if c in df.columns]\n",
    "    if not cand_cols:\n",
    "        return np.full(len(df), \"Unknown\", dtype=object)\n",
    "    s = df[cand_cols[0]].astype(str).str.strip().str.lower()\n",
    "    # common encodings\n",
    "    male = s.isin([\"m\",\"male\"])\n",
    "    female = s.isin([\"f\",\"female\"])\n",
    "    out = np.full(len(s), \"Unknown\", dtype=object)\n",
    "    out[male] = \"Male\"\n",
    "    out[female] = \"Female\"\n",
    "    # anything else stays 'Unknown'\n",
    "    return out\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for w in WINDOWS:\n",
    "    preds_csv = RES / f\"eicu_preds_{w}h.csv\"\n",
    "    if not preds_csv.exists():\n",
    "        print(f\"[{w}h] âœ— Missing predictions CSV â†’ {preds_csv}\")\n",
    "        continue\n",
    "\n",
    "    preds = pd.read_csv(preds_csv)  # expects at least label, mortality_probability\n",
    "    # bring in features for subgroups\n",
    "    feat_parq = FEAT / f\"combined_w{w}.parquet\"\n",
    "    if not feat_parq.exists():\n",
    "        print(f\"[{w}h] âœ— Missing features parquet â†’ {feat_parq}\")\n",
    "        continue\n",
    "    feats = pd.read_parquet(feat_parq)\n",
    "\n",
    "    # Align rows:\n",
    "    if \"stay_id\" in preds.columns and \"stay_id\" in feats.columns:\n",
    "        df = preds.merge(feats[[\"stay_id\",\"age_years\"] + ([c for c in [\"gender\",\"sex\"] if c in feats.columns])],\n",
    "                         on=\"stay_id\", how=\"left\")\n",
    "    else:\n",
    "        # fallback: same cohort order\n",
    "        df = preds.copy()\n",
    "        for col in [\"age_years\",\"gender\",\"sex\"]:\n",
    "            if col in feats.columns and col not in df.columns:\n",
    "                df[col] = feats[col].values\n",
    "\n",
    "    # Subgroup definitions (robust)\n",
    "    age_group = np.where(pd.to_numeric(df[\"age_years\"], errors=\"coerce\").fillna(-1) >= 65, \"â‰¥65\", \"<65\")\n",
    "    gender_lbl = _gender_labels(df)\n",
    "\n",
    "    thr = _thr(w)\n",
    "    y   = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int).values\n",
    "    p   = pd.to_numeric(df[\"mortality_probability\"], errors=\"coerce\").fillna(0).values\n",
    "    pred = (p >= thr).astype(int)\n",
    "\n",
    "    rows = []\n",
    "    for sub_name, grp in [(\"age_group\", age_group), (\"gender\", gender_lbl)]:\n",
    "        for g in pd.Series(grp).unique():\n",
    "            if g == \"Unknown\":   # skip if we truly don't have this info\n",
    "                continue\n",
    "            m = (grp == g)\n",
    "            TP = int(((pred==1) & (y==1) & m).sum())\n",
    "            FP = int(((pred==1) & (y==0) & m).sum())\n",
    "            TN = int(((pred==0) & (y==0) & m).sum())\n",
    "            FN = int(((pred==0) & (y==1) & m).sum())\n",
    "            N  = int(m.sum())\n",
    "            rows.append({\n",
    "                \"window_h\": w,\n",
    "                \"subgroup\": f\"{sub_name}:{g}\",\n",
    "                \"N\": N,\n",
    "                \"TPR\": _safe_rate(TP, TP+FN),   # sensitivity\n",
    "                \"TNR\": _safe_rate(TN, TN+FP),   # specificity\n",
    "                \"PPV\": _safe_rate(TP, TP+FP),\n",
    "                \"NPV\": _safe_rate(TN, TN+FN),\n",
    "                \"Alerts_rate\": _safe_rate(TP+FP, N)\n",
    "            })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values([\"subgroup\"])\n",
    "    out_path = TAB / f\"tableH_thresholded_fairness_w{w}h.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    # quick visualization: TPR/TNR by subgroup\n",
    "    if not out.empty:\n",
    "        plt.figure(figsize=(8.2, 4.2))\n",
    "        for metric in [\"TPR\",\"TNR\"]:\n",
    "            plt.plot(out[\"subgroup\"], out[metric], marker=\"o\", label=metric)\n",
    "        plt.xticks(rotation=28, ha=\"right\"); plt.ylim(0,1)\n",
    "        plt.title(f\"Thresholded fairness (eICU, {w}h) â€” thr={thr:.3f}\")\n",
    "        plt.ylabel(\"Rate\"); plt.grid(axis=\"y\", alpha=0.2); plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG / f\"fig_fairness_thresholded_w{w}h.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    all_rows.append(out)\n",
    "    print(f\"[{w}h] âœ“ wrote {out_path} (rows={len(out)})\")\n",
    "\n",
    "if all_rows:\n",
    "    pd.concat(all_rows, ignore_index=True).to_csv(TAB / \"tableH_thresholded_fairness_all_windows.csv\", index=False)\n",
    "    print(\"âœ“ Combined table saved â†’\", TAB / \"tableH_thresholded_fairness_all_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Temporal validation with quantile time blocks ====\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, joblib, json, pickle, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "FEAT   = ROOT / \"outputs\" / \"features\"\n",
    "TAB    = ROOT / \"manuscript\" / \"tables\";  TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG    = ROOT / \"manuscript\" / \"figures\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6,12,18,24]\n",
    "DATASETS_KEEP = {\"eicu\", \"mimic\"}   # keep only these datasets if 'source'/'dataset' present\n",
    "MIN_BIN_N   = 800                    # fail-safe: min patients per temporal bin\n",
    "MIN_BIN_POS = 20                     # min positives per bin\n",
    "MIN_BIN_NEG = 20                     # min negatives per bin\n",
    "MAX_BINS    = 12                     # upper bound of bins to try\n",
    "MIN_BINS    = 5                      # lower bound (will fallback to one bin if needed)\n",
    "SMOOTH_K    = 3                      # moving-average window for the line\n",
    "ECE_BINS    = 10\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def _load_pickle(p):\n",
    "    try:\n",
    "        return joblib.load(p)\n",
    "    except Exception:\n",
    "        with open(p, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def _load_artifacts(w):\n",
    "    imp = _load_pickle(MODELS / f\"imputer_w{w}.pkl\")\n",
    "    cal = _load_pickle(MODELS / f\"calibrator_w{w}.pkl\")\n",
    "    # features: prefer params JSON, fallback TXT (1 per line or comma-separated)\n",
    "    feats = None\n",
    "    prm = MODELS / f\"params_w{w}.json\"\n",
    "    if prm.exists():\n",
    "        try:\n",
    "            d = json.loads(prm.read_text(encoding=\"utf-8\"))\n",
    "            feats = [f.strip() for f in d.get(\"features\", []) if str(f).strip()]\n",
    "        except Exception:\n",
    "            feats = None\n",
    "    if not feats:\n",
    "        raw = (MODELS / f\"features_w{w}.txt\").read_text(encoding=\"utf-8\")\n",
    "        feats = [t.strip() for t in raw.replace(\",\", \"\\n\").splitlines() if t.strip()]\n",
    "        # normalize the .txt to one-per-line to avoid future parsing issues\n",
    "        (MODELS / f\"features_w{w}.txt\").write_text(\"\\n\".join(feats), encoding=\"utf-8\")\n",
    "    return imp, cal, feats\n",
    "\n",
    "def _decision_metrics(y, p):\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "    auroc = float(roc_auc_score(y, p)) if len(np.unique(y))==2 else np.nan\n",
    "    auprc = float(average_precision_score(y, p))\n",
    "    brier = float(brier_score_loss(y, p))\n",
    "    return auroc, auprc, brier\n",
    "\n",
    "def _ece(y, p, bins=10):\n",
    "    y = np.asarray(y).astype(int); p = np.asarray(p).astype(float)\n",
    "    edges = np.linspace(0,1,bins+1)\n",
    "    idx   = np.clip(np.digitize(p, edges, right=True)-1, 0, bins-1)\n",
    "    ece = 0.0; n = len(y)\n",
    "    for b in range(bins):\n",
    "        m = (idx==b)\n",
    "        if not m.any(): \n",
    "            continue\n",
    "        obs = y[m].mean()\n",
    "        conf = p[m].mean()\n",
    "        ece += (m.mean()) * abs(obs - conf)\n",
    "    return float(ece)\n",
    "\n",
    "def _calib_slope_intercept_irls(y, p, max_iter=50, tol=1e-6):\n",
    "    \"\"\"Fit y ~ Bernoulli(sigmoid(a + b * logit(p))) via IRLS; returns (b,a).\"\"\"\n",
    "    y = np.asarray(y, int)\n",
    "    p = np.asarray(p, float).clip(1e-6, 1-1e-6)\n",
    "    x = np.log(p/(1-p))\n",
    "    X = np.column_stack([np.ones_like(x), x])  # [a, b]\n",
    "    beta = np.array([0.0, 1.0])                # init intercept=0, slope=1\n",
    "    for _ in range(max_iter):\n",
    "        z = X @ beta\n",
    "        mu = 1/(1+np.exp(-z))\n",
    "        W  = mu*(1-mu) + 1e-12\n",
    "        # IRLS step: beta <- beta + (X'WX)^-1 X'(y-mu)\n",
    "        XT_W = X.T * W\n",
    "        H = XT_W @ X\n",
    "        g = XT_W @ (y - mu)\n",
    "        try:\n",
    "            step = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # ridge fallback\n",
    "            step = np.linalg.solve(H + 1e-6*np.eye(2), g)\n",
    "        beta_new = beta + step\n",
    "        if np.max(np.abs(step)) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "    a, b = float(beta[0]), float(beta[1])\n",
    "    return b, a\n",
    "\n",
    "def _first_valid_time_column(df):\n",
    "    \"\"\"Return a Series for temporal ordering: year if available; else a datetime; else row-order.\"\"\"\n",
    "    # try (year-like) first\n",
    "    for col in [\"admit_year\",\"admission_year\",\"hosp_admit_year\",\"year\",\"anchor_year\"]:\n",
    "        if col in df.columns:\n",
    "            s = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            # treat as year if plausible\n",
    "            if s.notna().sum() > 0 and (s.min() >= 1900 or s.max() >= 1900):\n",
    "                return s, True\n",
    "    # then try timestamps\n",
    "    for col in [\"admittime\",\"admission_time\",\"admit_time\",\"intime\",\"icu_intime\",\"icustay_intime\",\"charttime\",\"hospital_admittime\"]:\n",
    "        if col in df.columns:\n",
    "            t = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            if t.notna().any():\n",
    "                return t, False\n",
    "    # fallback: monotonically increasing index (pre-sorted by something else)\n",
    "    return pd.Series(np.arange(len(df)), index=df.index), False\n",
    "\n",
    "def _temporal_bins_for_dataset(df_ds, n_max=MAX_BINS, n_min=MIN_BINS):\n",
    "    \"\"\"Return dataframe with 'temp_bin', 'x_center' ensuring min N/Pos/Neg; reduces bins if needed.\"\"\"\n",
    "    tcol, is_year = _first_valid_time_column(df_ds)\n",
    "    # order\n",
    "    order = np.argsort(tcol.values)\n",
    "    df_ord = df_ds.iloc[order].reset_index(drop=True).copy()\n",
    "\n",
    "    # how many bins can we support?\n",
    "    n_try = min(n_max, max(n_min, int(len(df_ord) / (MIN_BIN_N or 1))))\n",
    "    n_try = max(n_try, 1)\n",
    "    ok_df = None\n",
    "    while n_try >= 1:\n",
    "        qs = np.linspace(0, 1, n_try+1)\n",
    "        edges = (len(df_ord)*qs).round().astype(int)\n",
    "        edges[-1] = len(df_ord)\n",
    "        temp_bin = np.empty(len(df_ord), dtype=int)\n",
    "        for i in range(n_try):\n",
    "            temp_bin[edges[i]:edges[i+1]] = i\n",
    "        df_ord[\"temp_bin\"] = temp_bin\n",
    "\n",
    "        # constraints\n",
    "        good = True\n",
    "        for b in range(n_try):\n",
    "            m = df_ord[\"temp_bin\"].eq(b)\n",
    "            n  = int(m.sum())\n",
    "            pos = int(df_ord.loc[m,\"label\"].sum())\n",
    "            neg = n - pos\n",
    "            if (n < MIN_BIN_N) or (pos < MIN_BIN_POS) or (neg < MIN_BIN_NEG):\n",
    "                good = False; break\n",
    "        if good:\n",
    "            ok_df = df_ord; break\n",
    "        n_try -= 1\n",
    "\n",
    "    if ok_df is None:\n",
    "        # just return one bin\n",
    "        df_ord[\"temp_bin\"] = 0\n",
    "        n_try = 1\n",
    "        ok_df = df_ord\n",
    "\n",
    "    # x center: year mean if we have year with >1 unique; else use 1..K\n",
    "    if is_year and tcol.nunique() > 1:\n",
    "        # compute mean year per bin (handles weird large years too)\n",
    "        xc = (ok_df.groupby(\"temp_bin\")[\"admit_year\"].mean()\n",
    "              if \"admit_year\" in ok_df.columns else\n",
    "              ok_df.groupby(\"temp_bin\")[tcol.name].mean())\n",
    "        # if datetime, convert to numeric year with .year\n",
    "        if not np.issubdtype(xc.dtype, np.number):\n",
    "            xc = pd.to_datetime(xc).dt.year\n",
    "        x_map = {b: float(v) for b, v in xc.items()}\n",
    "        xlabel = \"Admission year (bin center)\"\n",
    "    else:\n",
    "        x_map = {b: float(b+1) for b in range(n_try)}\n",
    "        xlabel = \"Temporal block (equal-size)\"\n",
    "    ok_df[\"x_center\"] = ok_df[\"temp_bin\"].map(x_map).astype(float)\n",
    "    return ok_df, xlabel\n",
    "\n",
    "def _moving_average(y, k=SMOOTH_K):\n",
    "    if len(y) == 0: return y\n",
    "    if k <= 1: return y\n",
    "    z = pd.Series(y, dtype=float).rolling(k, center=True, min_periods=1).mean().values\n",
    "    return z\n",
    "\n",
    "def _calc_predictions(df, feats, imputer, calibrator):\n",
    "    # strict feature subset\n",
    "    missing = [f for f in feats if f not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing feature columns: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
    "    X = df[feats].copy()\n",
    "    X_imp = imputer.transform(X)\n",
    "    if hasattr(calibrator, \"predict_proba\"):\n",
    "        p = calibrator.predict_proba(X_imp)[:,1]\n",
    "    else:\n",
    "        p = np.asarray(calibrator.predict(X_imp)).ravel()\n",
    "    return p\n",
    "\n",
    "# ---------- main runner ----------\n",
    "def run_temporal_bmc_style(w):\n",
    "    # load artifacts + data\n",
    "    imp, cal, feats = _load_artifacts(w)\n",
    "    df = pd.read_parquet(FEAT / f\"combined_w{w}.parquet\").copy()\n",
    "\n",
    "    # dataset tag\n",
    "    src_col = \"source\" if \"source\" in df.columns else (\"dataset\" if \"dataset\" in df.columns else None)\n",
    "    if src_col is None:\n",
    "        df[\"dataset\"] = \"eicu\"\n",
    "        src_col = \"dataset\"\n",
    "    df[src_col] = df[src_col].astype(str).str.lower()\n",
    "    df = df[df[src_col].isin(DATASETS_KEEP)].copy()\n",
    "\n",
    "    # label\n",
    "    ycol = None\n",
    "    for c in [\"label\",\"hospital_expire_flag\",\"y_true\",\"outcome\",\"death\",\"mortality\",\"hospital_death\"]:\n",
    "        if c in df.columns: ycol = c; break\n",
    "    if ycol is None:\n",
    "        raise RuntimeError(\"No binary label column found.\")\n",
    "    df[\"label\"] = pd.to_numeric(df[ycol], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # predictions\n",
    "    df[\"prob\"] = _calc_predictions(df, feats, imp, cal)\n",
    "\n",
    "    # split per dataset and build temporal bins\n",
    "    all_rows = []\n",
    "    for ds, d in df.groupby(df[src_col]):\n",
    "        d = d.reset_index(drop=True)\n",
    "        # keep column 'admit_year' if exists to help xlabel logic\n",
    "        if \"admit_year\" not in d.columns:\n",
    "            d[\"admit_year\"] = np.nan\n",
    "        d2, xlabel = _temporal_bins_for_dataset(d)\n",
    "        d2[\"dataset\"] = ds\n",
    "        all_rows.append(d2)\n",
    "\n",
    "    dx = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "    # metrics per bin\n",
    "    rows = []\n",
    "    for (ds, b), g in dx.groupby([\"dataset\",\"temp_bin\"]):\n",
    "        y = g[\"label\"].values\n",
    "        p = g[\"prob\"].values\n",
    "        auroc, auprc, brier = _decision_metrics(y, p)\n",
    "        slope, intercept = _calib_slope_intercept_irls(y, p)\n",
    "        ece = _ece(y, p, bins=ECE_BINS)\n",
    "        rows.append({\n",
    "            \"window_h\": w, \"dataset\": ds, \"temp_bin\": int(b),\n",
    "            \"x_center\": float(g[\"x_center\"].mean()),\n",
    "            \"N\": int(len(g)), \"Pos\": int(y.sum()), \"Neg\": int(len(y)-y.sum()),\n",
    "            \"AUROC\": auroc, \"AUPRC\": auprc, \"Brier\": brier,\n",
    "            \"slope\": slope, \"intercept\": intercept, \"ECE\": ece\n",
    "        })\n",
    "    M = pd.DataFrame(rows).sort_values([\"dataset\",\"temp_bin\"]).reset_index(drop=True)\n",
    "    out_csv = TAB / f\"temporal_metrics_w{w}h.csv\"\n",
    "    M.to_csv(out_csv, index=False)\n",
    "\n",
    "    # ---- plots (BMC style) ----\n",
    "    def _style():\n",
    "        plt.grid(axis=\"both\", alpha=0.15, linestyle=\"-\", linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # AUROC\n",
    "    plt.figure(figsize=(11,5))\n",
    "    for ds, g in M.groupby(\"dataset\"):\n",
    "        x = g[\"x_center\"].values; y = g[\"AUROC\"].values\n",
    "        plt.scatter(x, y, s=55, alpha=0.35, label=ds.upper())\n",
    "        plt.plot(x, _moving_average(y, k=SMOOTH_K), linewidth=2)\n",
    "    plt.ylim(0.55, 1.00); plt.xlim(M[\"x_center\"].min()-0.5, M[\"x_center\"].max()+0.5)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"AUROC\"); plt.title(f\"Temporal AUROC by time (BMC style, {w}h)\")\n",
    "    plt.legend(frameon=False); _style()\n",
    "    plt.savefig(FIG / f\"fig_temporal_auroc_w{w}h.png\", dpi=300); plt.close()\n",
    "\n",
    "    # Brier + ECE\n",
    "    plt.figure(figsize=(11,5))\n",
    "    for ds, g in M.groupby(\"dataset\"):\n",
    "        x = g[\"x_center\"].values\n",
    "        b = g[\"Brier\"].values; e = g[\"ECE\"].values\n",
    "        plt.plot(x, _moving_average(b, k=SMOOTH_K), linewidth=2, label=f\"{ds.upper()} Brier\")\n",
    "        plt.plot(x, _moving_average(e, k=SMOOTH_K), linewidth=2, linestyle=\"--\", label=f\"{ds.upper()} ECE\")\n",
    "    plt.ylim(0.0, 0.25); plt.xlim(M[\"x_center\"].min()-0.5, M[\"x_center\"].max()+0.5)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Score\"); plt.title(f\"Temporal Brier / ECE (BMC style, {w}h)\")\n",
    "    plt.legend(frameon=False, ncol=2); _style()\n",
    "    plt.savefig(FIG / f\"fig_temporal_brier_ece_w{w}h.png\", dpi=300); plt.close()\n",
    "\n",
    "    # Calibration slope (winsorized for readability)\n",
    "    plt.figure(figsize=(11,5))\n",
    "    for ds, g in M.groupby(\"dataset\"):\n",
    "        x = g[\"x_center\"].values\n",
    "        s = g[\"slope\"].clip(0, 2)  # winsorize extreme values to keep axis reasonable\n",
    "        plt.plot(x, _moving_average(s, k=SMOOTH_K), linewidth=2, label=ds.upper())\n",
    "        plt.scatter(x, s, s=35, alpha=0.25)\n",
    "    plt.axhline(1.0, color=\"gray\", linestyle=\"--\", linewidth=1.2, label=\"Ideal slope=1\")\n",
    "    plt.ylim(0.0, 2.0); plt.xlim(M[\"x_center\"].min()-0.5, M[\"x_center\"].max()+0.5)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Calibration slope\"); plt.title(f\"Calibration slope over time (BMC style, {w}h)\")\n",
    "    plt.legend(frameon=False); _style()\n",
    "    plt.savefig(FIG / f\"fig_temporal_slope_w{w}h.png\", dpi=300); plt.close()\n",
    "\n",
    "    print(f\"[{w}h] âœ“ temporal metrics â†’ {out_csv}\")\n",
    "    print(f\"[{w}h] âœ“ figs â†’\",\n",
    "          FIG / f\"fig_temporal_auroc_w{w}h.png\", \",\",\n",
    "          FIG / f\"fig_temporal_brier_ece_w{w}h.png\", \",\",\n",
    "          FIG / f\"fig_temporal_slope_w{w}h.png\")\n",
    "\n",
    "# ---- run for all windows; save combined table ----\n",
    "combined = []\n",
    "for w in WINDOWS:\n",
    "    try:\n",
    "        run_temporal_bmc_style(w)\n",
    "        combined.append(pd.read_csv(TAB / f\"temporal_metrics_w{w}h.csv\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[{w}h] âœ— temporal run failed: {e}\")\n",
    "\n",
    "if combined:\n",
    "    pd.concat(combined, ignore_index=True).to_csv(TAB / \"temporal_metrics_all_windows.csv\", index=False)\n",
    "    print(\"âœ“ Saved combined:\", TAB / \"temporal_metrics_all_windows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Sentinel-ICU: One-shot analysis \n",
    "# =========================\n",
    "# Produces, per window (6/12/18/24h):\n",
    "#  â€¢ overall metrics w/ 95% CIs via stratified bootstrap\n",
    "#  â€¢ operating points (0.5 and model threshold if available)\n",
    "#  â€¢ site heterogeneity (per-hospital AUROC + RE meta on logit(AUC), IÂ²)\n",
    "#  â€¢ temporal performance \n",
    "#\n",
    "# Skips re-doing fairness (age/gender) and DCA unless you flip toggles below.\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, json, gzip, warnings, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "# --------- Paths (your tree) ----------\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "MODELS = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "FEAT   = ROOT / \"outputs\" / \"features\"\n",
    "RES    = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "EICU   = ROOT / \"eicu\"\n",
    "MIMIC  = ROOT / \"mimic\"\n",
    "TAB    = ROOT / \"manuscript\" / \"tables\";  TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG    = ROOT / \"manuscript\" / \"figures\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "\n",
    "# --------- Feature toggles ----------\n",
    "DO_BOOTSTRAP_CI  = True     # overall 95% CIs\n",
    "DO_OP_POINTS     = True     # operating-point metrics/mini-fig\n",
    "DO_SITE_HET      = True     # per-hospital + random-effects meta-analysis\n",
    "DO_TEMPORAL      = True     # BMC-style equal-count blocks (smooth lines)\n",
    "DO_FAIRNESS      = False    # you already have subgroup figs/tables\n",
    "DO_DCA           = False    # you already generated DCA\n",
    "\n",
    "# Do not overwrite existing outputs\n",
    "SKIP_IF_EXISTS   = True\n",
    "\n",
    "# Filters / settings\n",
    "MIN_POS = 20\n",
    "MIN_NEG = 20\n",
    "N_BOOT  = 400     # stratified bootstrap reps (tune for speed)\n",
    "N_TEMP_BLOCKS = 8 # equal-count temporal blocks per dataset\n",
    "EPS = 1e-6\n",
    "\n",
    "# --------- Utils to avoid duplicates ----------\n",
    "def _save_df(df: pd.DataFrame, path: Path):\n",
    "    if SKIP_IF_EXISTS and path.exists():\n",
    "        print(\"skip (exists):\", path)\n",
    "        return\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"âœ“ wrote:\", path)\n",
    "\n",
    "def _save_fig(fig_path: Path, save_fn):\n",
    "    if SKIP_IF_EXISTS and fig_path.exists():\n",
    "        print(\"skip (exists):\", fig_path)\n",
    "        return\n",
    "    save_fn()\n",
    "    print(\"âœ“ wrote:\", fig_path)\n",
    "\n",
    "# --------- Artifacts & features ----------\n",
    "def _read_features_list(w: int):\n",
    "    # Prefer JSON params if present\n",
    "    pj = MODELS / f\"params_w{w}.json\"\n",
    "    if pj.exists():\n",
    "        try:\n",
    "            feats = json.loads(pj.read_text(encoding=\"utf-8\"))[\"features\"]\n",
    "            feats = [f.strip() for f in feats if f.strip()]\n",
    "            if feats:\n",
    "                return feats\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: text file (1 feature per line OR comma-separated)\n",
    "    raw = (MODELS / f\"features_w{w}.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    lines = [ln.strip() for ln in raw.replace(\",\", \"\\n\").splitlines() if ln.strip()]\n",
    "    # self-heal to 1-per-line\n",
    "    (MODELS / f\"features_w{w}.txt\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return lines\n",
    "\n",
    "import pickle\n",
    "def _load_imputer_calibrator(w: int):\n",
    "    with open(MODELS / f\"imputer_w{w}.pkl\", \"rb\") as f:\n",
    "        imp = pickle.load(f)\n",
    "    with open(MODELS / f\"calibrator_w{w}.pkl\", \"rb\") as f:\n",
    "        cal = pickle.load(f)\n",
    "    thr = None\n",
    "    thrp = MODELS / f\"threshold_w{w}.csv\"\n",
    "    if thrp.exists():\n",
    "        try:\n",
    "            thr = float(pd.read_csv(thrp).iloc[0][\"threshold\"])\n",
    "        except Exception:\n",
    "            thr = None\n",
    "    return imp, cal, thr\n",
    "\n",
    "# --------- Ensure predictions & hospitalid ----------\n",
    "def ensure_eicu_preds_with_hospitalid(w: int) -> Path:\n",
    "    out_csv = RES / f\"eicu_preds_{w}h.csv\"\n",
    "    if out_csv.exists():\n",
    "        try:\n",
    "            chk = pd.read_csv(out_csv, nrows=5)\n",
    "            if {\"label\",\"mortality_probability\",\"hospitalid\"}.issubset(chk.columns):\n",
    "                return out_csv\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    feat_parq = FEAT / f\"combined_w{w}.parquet\"\n",
    "    if not feat_parq.exists():\n",
    "        raise FileNotFoundError(feat_parq)\n",
    "\n",
    "    df = pd.read_parquet(feat_parq)\n",
    "    df[\"source\"] = df[\"source\"].astype(str).str.lower()\n",
    "    eicu = df[df[\"source\"] == \"eicu\"].copy()\n",
    "\n",
    "    # outcome â†’ label\n",
    "    y_col = None\n",
    "    for c in [\"label\",\"hospital_expire_flag\",\"y_true\",\"outcome\",\"death\",\"mortality\",\"hospital_death\"]:\n",
    "        if c in eicu.columns:\n",
    "            y_col = c; break\n",
    "    if y_col is None:\n",
    "        raise RuntimeError(f\"[{w}h] No binary label column in {feat_parq}\")\n",
    "\n",
    "    # predict\n",
    "    feats = _read_features_list(w)\n",
    "    imp, cal, _ = _load_imputer_calibrator(w)\n",
    "    X = eicu[feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_imp = imp.transform(X.values)\n",
    "    if hasattr(cal, \"predict_proba\"):\n",
    "        prob = cal.predict_proba(X_imp)[:,1]\n",
    "    else:\n",
    "        prob = np.clip(np.asarray(cal.predict(X_imp)).ravel(), 0, 1)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"stay_id\": eicu[\"stay_id\"].values,\n",
    "        \"label\": pd.to_numeric(eicu[y_col], errors=\"coerce\").fillna(0).astype(int),\n",
    "        \"mortality_probability\": prob\n",
    "    })\n",
    "\n",
    "    # add hospitalid\n",
    "    pat_csv = EICU / \"patient.csv.gz\"\n",
    "    if not pat_csv.exists():\n",
    "        raise FileNotFoundError(pat_csv)\n",
    "    pat = pd.read_csv(pat_csv, usecols=[\"patientunitstayid\",\"hospitalid\"])\n",
    "    out = out.merge(pat.rename(columns={\"patientunitstayid\":\"stay_id\"}), on=\"stay_id\", how=\"left\")\n",
    "\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ“ built {out_csv} (n={len(out)})\")\n",
    "    return out_csv\n",
    "\n",
    "# --------- Metrics & calibration slope (no sklearn fit) ----------\n",
    "def _ece(y, p, bins=10):\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float)\n",
    "    q = np.linspace(0,1,bins+1)\n",
    "    idx = np.clip(np.digitize(p, q, right=True)-1, 0, bins-1)\n",
    "    ece = 0.0\n",
    "    for b in range(bins):\n",
    "        m = (idx==b)\n",
    "        if not m.any(): \n",
    "            continue\n",
    "        e_b = y[m].mean()\n",
    "        p_b = p[m].mean()\n",
    "        ece += abs(e_b - p_b) * m.mean()\n",
    "    return float(ece)\n",
    "\n",
    "def _calib_slope_intercept_irls(y, p, max_iter=50, tol=1e-8):\n",
    "    # Fit logistic regression: y ~ 1 + logit(p) using IRLS\n",
    "    y = np.asarray(y, int)\n",
    "    p = np.clip(np.asarray(p, float), EPS, 1-EPS)\n",
    "    x = np.log(p/(1-p)).reshape(-1,1)\n",
    "    X = np.hstack([np.ones((len(y),1)), x])  # intercept + slope\n",
    "    beta = np.zeros(2)  # init\n",
    "    for _ in range(max_iter):\n",
    "        z = X @ beta\n",
    "        mu = 1/(1+np.exp(-z))\n",
    "        W = mu*(1-mu)\n",
    "        # guard against zeros\n",
    "        W = np.clip(W, 1e-8, None)\n",
    "        # weighted least squares step\n",
    "        WX = X * W[:,None]\n",
    "        H = WX.T @ X\n",
    "        g = X.T @ (y - mu)\n",
    "        try:\n",
    "            step = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            step = np.linalg.lstsq(H, g, rcond=None)[0]\n",
    "        beta_new = beta + step\n",
    "        if np.max(np.abs(step)) < tol:\n",
    "            beta = beta_new; break\n",
    "        beta = beta_new\n",
    "    intercept, slope = float(beta[0]), float(beta[1])\n",
    "    return slope, intercept\n",
    "\n",
    "def _bootstrap_metrics(y, p, n_boot=N_BOOT, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float)\n",
    "    n = len(y)\n",
    "    prev = y.mean()\n",
    "    # stratified resample\n",
    "    pos_idx = np.where(y==1)[0]; neg_idx = np.where(y==0)[0]\n",
    "    m_pos, m_neg = len(pos_idx), len(neg_idx)\n",
    "\n",
    "    def _metrics(yb, pb):\n",
    "        return dict(\n",
    "            AUROC = roc_auc_score(yb, pb),\n",
    "            AUPRC = average_precision_score(yb, pb),\n",
    "            Brier = brier_score_loss(yb, pb),\n",
    "            ECE   = _ece(yb, pb, bins=10),\n",
    "            slope = _calib_slope_intercept_irls(yb, pb)[0],\n",
    "            intercept = _calib_slope_intercept_irls(yb, pb)[1],\n",
    "            prevalence = float(np.mean(yb))\n",
    "        )\n",
    "\n",
    "    # point estimates\n",
    "    pe = _metrics(y, p)\n",
    "    # bootstrap CIs\n",
    "    store = {k: [] for k in [\"AUROC\",\"AUPRC\",\"Brier\",\"ECE\",\"slope\",\"intercept\",\"prevalence\"]}\n",
    "    for _ in range(n_boot):\n",
    "        rs_pos = rng.choice(pos_idx, m_pos, replace=True)\n",
    "        rs_neg = rng.choice(neg_idx, m_neg, replace=True)\n",
    "        rs = np.concatenate([rs_pos, rs_neg])\n",
    "        yb, pb = y[rs], p[rs]\n",
    "        m = _metrics(yb, pb)\n",
    "        for k in store:\n",
    "            store[k].append(m[k])\n",
    "    ci = {}\n",
    "    for k, arr in store.items():\n",
    "        lo, hi = np.percentile(arr, [2.5, 97.5])\n",
    "        ci[k] = (float(lo), float(hi))\n",
    "    return pe, ci\n",
    "\n",
    "# --------- Operating points ----------\n",
    "def _operating_points(y, p, thr_list):\n",
    "    out=[]\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float)\n",
    "    for thr in thr_list:\n",
    "        pred = (p >= thr).astype(int)\n",
    "        TP = int(((pred==1)&(y==1)).sum())\n",
    "        FP = int(((pred==1)&(y==0)).sum())\n",
    "        TN = int(((pred==0)&(y==0)).sum())\n",
    "        FN = int(((pred==0)&(y==1)).sum())\n",
    "        N = len(y)\n",
    "        sens = TP / (TP+FN+EPS)\n",
    "        spec = TN / (TN+FP+EPS)\n",
    "        ppv  = TP / (TP+FP+EPS)\n",
    "        npv  = TN / (TN+FN+EPS)\n",
    "        f1   = 2*TP / (2*TP+FP+FN+EPS)\n",
    "        alert_rate = (TP+FP)/N\n",
    "        out.append(dict(\n",
    "            threshold=thr, N=N, prevalence=float(y.mean()),\n",
    "            TP=TP, FP=FP, TN=TN, FN=FN,\n",
    "            sensitivity=sens, specificity=spec, PPV=ppv, NPV=npv, F1=f1, alerts_rate=alert_rate\n",
    "        ))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# --------- DeLong per-site variance + RE meta on logit(AUC) ----------\n",
    "def _midrank(x):\n",
    "    order = np.argsort(x)\n",
    "    x_sorted = x[order]\n",
    "    J = len(x)\n",
    "    midr = np.zeros(J, float)\n",
    "    i=0\n",
    "    while i<J:\n",
    "        j=i\n",
    "        while j<J and x_sorted[j]==x_sorted[i]:\n",
    "            j+=1\n",
    "        mid = 0.5*(i + j - 1) + 1\n",
    "        midr[i:j] = mid\n",
    "        i=j\n",
    "    out = np.empty(J, float); out[order]=midr\n",
    "    return out\n",
    "\n",
    "def delong_auc_var(y, s):\n",
    "    y = np.asarray(y, int); s = np.asarray(s, float)\n",
    "    pos = s[y==1]; neg = s[y==0]\n",
    "    m, n = len(pos), len(neg)\n",
    "    if m==0 or n==0: return np.nan, np.nan\n",
    "    all_scores = np.concatenate([pos, neg])\n",
    "    mr_all = _midrank(all_scores)\n",
    "    mr_pos = _midrank(pos)\n",
    "    mr_neg = _midrank(neg)\n",
    "    V10 = (mr_all[:m] - mr_pos) / n\n",
    "    V01 = 1 - (mr_all[m:] - mr_neg) / m\n",
    "    auc = V10.mean()\n",
    "    s10 = np.var(V10, ddof=1)\n",
    "    s01 = np.var(V01, ddof=1)\n",
    "    var = s10/m + s01/n\n",
    "    return float(auc), float(max(var, 1e-12))\n",
    "\n",
    "def _logit(p): p = np.clip(p, EPS, 1-EPS); return np.log(p/(1-p))\n",
    "def _invlogit(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "def meta_RE_logit_auc(aucs, var_auc):\n",
    "    aucs = np.asarray(aucs, float); var_auc = np.asarray(var_auc, float)\n",
    "    m = np.isfinite(aucs) & np.isfinite(var_auc) & (var_auc > 0)\n",
    "    aucs = aucs[m]; var_auc = var_auc[m]\n",
    "    if aucs.size==0: return None\n",
    "    lg = _logit(aucs)\n",
    "    deriv = 1/(aucs*(1-aucs))\n",
    "    var_lg = var_auc * (deriv**2)\n",
    "    w = 1/var_lg\n",
    "    theta_fe = np.sum(w*lg)/np.sum(w)\n",
    "    Q = np.sum(w*(lg-theta_fe)**2)\n",
    "    df = len(lg)-1\n",
    "    C = np.sum(w) - (np.sum(w**2)/np.sum(w))\n",
    "    tau2 = max(0.0, (Q-df)/C) if C>0 else 0.0\n",
    "    w_re = 1/(var_lg + tau2)\n",
    "    theta_re = np.sum(w_re*lg)/np.sum(w_re)\n",
    "    se_re = math.sqrt(1/np.sum(w_re))\n",
    "    ci_l = theta_re - 1.96*se_re\n",
    "    ci_h = theta_re + 1.96*se_re\n",
    "    return dict(\n",
    "        k=int(len(lg)),\n",
    "        pooled_auc=float(_invlogit(theta_re)),\n",
    "        ci_low=float(_invlogit(ci_l)),\n",
    "        ci_high=float(_invlogit(ci_h)),\n",
    "        I2=float(max(0.0, (Q-df)/Q)*100) if Q>0 else 0.0\n",
    "    )\n",
    "\n",
    "def forest_plot(df_sites, pooled, title, out_png: Path):\n",
    "    def _save():\n",
    "        d = df_sites.sort_values(\"AUROC\")\n",
    "        y = np.arange(len(d))\n",
    "        auc = d[\"AUROC\"].values\n",
    "        se  = d[\"SE\"].values\n",
    "        ci_lo = np.clip(auc - 1.96*se, 0.5, 1.0)\n",
    "        ci_hi = np.clip(auc + 1.96*se, 0.5, 1.0)\n",
    "        labels = [f\"H{int(h)} (n={int(n)}, pos={int(p)})\"\n",
    "                  for h,n,p in zip(d[\"hospitalid\"], d[\"N\"], d[\"Pos\"])]\n",
    "\n",
    "        plt.figure(figsize=(9, max(4, 0.24*len(d)+1)))\n",
    "        plt.hlines(y, ci_lo, ci_hi, lw=2)\n",
    "        plt.plot(auc, y, \"o\", ms=5)\n",
    "        ytick = y; ylab = labels\n",
    "        if pooled:\n",
    "            y0 = -1.8\n",
    "            plt.hlines(y0, pooled[\"ci_low\"], pooled[\"ci_high\"], lw=4, color=\"tab:red\")\n",
    "            plt.plot([pooled[\"pooled_auc\"]],[y0], \"s\", ms=8, color=\"tab:red\")\n",
    "            ytick = np.concatenate([[y0], y])\n",
    "            ylab  = [\"Pooled (RE, logit)\"] + labels\n",
    "        plt.yticks(ytick, ylab)\n",
    "        plt.xlabel(\"AUROC\"); plt.xlim(0.5, 1.0)\n",
    "        plt.title(title); plt.grid(axis=\"x\", alpha=0.25)\n",
    "        plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "    _save_fig(out_png, _save)\n",
    "\n",
    "# --------- Temporal blocks ----------\n",
    "def _extract_year_from_series(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    y = s.dt.year\n",
    "    # clamp realistic range to avoid dirty years\n",
    "    return y.where((y>=2000) & (y<=2035))\n",
    "\n",
    "def build_year_map_safe():\n",
    "    rows=[]\n",
    "    # MIMIC from icustays.intime\n",
    "    icu_p = MIMIC / \"icu\" / \"icustays.csv.gz\"\n",
    "    if icu_p.exists():\n",
    "        icu = pd.read_csv(icu_p, usecols=[\"stay_id\",\"intime\"])\n",
    "        y = _extract_year_from_series(icu[\"intime\"])\n",
    "        rows.append(pd.DataFrame({\"dataset\":\"mimic\",\"stay_id\":icu[\"stay_id\"],\"admit_year\":y}))\n",
    "    # eICU from patient.hospitaladmittime24\n",
    "    e_p = EICU / \"patient.csv.gz\"\n",
    "    if e_p.exists():\n",
    "        pat = pd.read_csv(e_p, usecols=[\"patientunitstayid\",\"hospitaladmittime24\"])\n",
    "        y = _extract_year_from_series(pat[\"hospitaladmittime24\"])\n",
    "        rows.append(pd.DataFrame({\"dataset\":\"eicu\",\"stay_id\":pat[\"patientunitstayid\"],\"admit_year\":y}))\n",
    "    ym = pd.concat(rows, ignore_index=True)\n",
    "    ym.dropna(subset=[\"admit_year\"], inplace=True)\n",
    "    ym[\"admit_year\"] = ym[\"admit_year\"].astype(int)\n",
    "    _save_df((ym.groupby(\"dataset\")[\"admit_year\"]\n",
    "                .agg(min_year=\"min\", max_year=\"max\", n_years=lambda s: s.nunique())\n",
    "                .reset_index()),\n",
    "             TAB / \"admit_year_map_fixed.csv\")\n",
    "    return ym\n",
    "\n",
    "YEAR_MAP = build_year_map_safe()\n",
    "\n",
    "def temporal_blocks(df_joined, n_blocks=N_TEMP_BLOCKS):\n",
    "    # df_joined: rows of one dataset with 'admit_year'\n",
    "    df = df_joined.dropna(subset=[\"admit_year\"]).copy()\n",
    "    if len(df) < n_blocks*200:  # need enough per block\n",
    "        return None\n",
    "    df = df.sort_values(\"admit_year\")\n",
    "    # equal-count blocks by quantiles on rank\n",
    "    q = np.linspace(0,1,n_blocks+1)\n",
    "    ranks = np.linspace(0,1,len(df),endpoint=False)\n",
    "    bins = np.digitize(ranks, q, right=False)\n",
    "    # relabel 1..n_blocks\n",
    "    bins = np.clip(bins, 1, n_blocks)\n",
    "    df[\"time_block\"] = bins\n",
    "    return df\n",
    "\n",
    "# =====================================================\n",
    "# Main per-window run\n",
    "# =====================================================\n",
    "def run_window(w: int):\n",
    "    print(f\"\\n=== Window {w}h ===\")\n",
    "    # ensure preds+hospitalid\n",
    "    preds_csv = ensure_eicu_preds_with_hospitalid(w)\n",
    "    dfp = pd.read_csv(preds_csv)\n",
    "\n",
    "    # Overall CIs (eICU only)\n",
    "    rows_ci=[]\n",
    "    if DO_BOOTSTRAP_CI:\n",
    "        y = dfp[\"label\"].astype(int).values\n",
    "        p = dfp[\"mortality_probability\"].astype(float).values\n",
    "        pe, ci = _bootstrap_metrics(y, p, n_boot=N_BOOT, seed=2025+w)\n",
    "        rows_ci.append(dict(\n",
    "            window_h=w, dataset=\"eICU\", N=len(y), prevalence=pe[\"prevalence\"],\n",
    "            AUROC=pe[\"AUROC\"], AUROC_lo=ci[\"AUROC\"][0], AUROC_hi=ci[\"AUROC\"][1],\n",
    "            AUPRC=pe[\"AUPRC\"], AUPRC_lo=ci[\"AUPRC\"][0], AUPRC_hi=ci[\"AUPRC\"][1],\n",
    "            Brier=pe[\"Brier\"], Brier_lo=ci[\"Brier\"][0], Brier_hi=ci[\"Brier\"][1],\n",
    "            ECE=pe[\"ECE\"], ECE_lo=ci[\"ECE\"][0], ECE_hi=ci[\"ECE\"][1],\n",
    "            slope=pe[\"slope\"], slope_lo=ci[\"slope\"][0], slope_hi=ci[\"slope\"][1],\n",
    "            intercept=pe[\"intercept\"], intercept_lo=ci[\"intercept\"][0], intercept_hi=ci[\"intercept\"][1]\n",
    "        ))\n",
    "        _save_df(pd.DataFrame(rows_ci), TAB / f\"overall_ci_w{w}h.csv\")\n",
    "\n",
    "    # Operating points\n",
    "    if DO_OP_POINTS:\n",
    "        imp, cal, thr = _load_imputer_calibrator(w)\n",
    "        thr_list = [0.5] + ([thr] if isinstance(thr, (float,int)) else [])\n",
    "        op = _operating_points(dfp[\"label\"], dfp[\"mortality_probability\"], thr_list)\n",
    "        _save_df(op, TAB / f\"operating_points_w{w}h.csv\")\n",
    "        # mini figure (ROC-style bar for sens/spec at thresholds)\n",
    "        figp = FIG / f\"fig_operating_points_w{w}h.png\"\n",
    "        def _save():\n",
    "            plt.figure(figsize=(6,3.6))\n",
    "            x = np.arange(len(op))\n",
    "            plt.bar(x-0.2, op[\"sensitivity\"], width=0.4, label=\"Sensitivity\")\n",
    "            plt.bar(x+0.2, op[\"specificity\"], width=0.4, label=\"Specificity\")\n",
    "            plt.xticks(x, [f\"{t:.2f}\" for t in op[\"threshold\"]])\n",
    "            plt.ylim(0,1); plt.ylabel(\"Value\"); plt.xlabel(\"Threshold\")\n",
    "            plt.title(f\"Operating points (eICU, {w}h)\")\n",
    "            plt.legend(); plt.tight_layout(); plt.savefig(figp, dpi=200); plt.close()\n",
    "        _save_fig(figp, _save)\n",
    "\n",
    "    # Site heterogeneity (eICU hospitals)\n",
    "    if DO_SITE_HET:\n",
    "        site_rows=[]\n",
    "        for hid, g in dfp.dropna(subset=[\"hospitalid\"]).groupby(\"hospitalid\"):\n",
    "            y = g[\"label\"].astype(int).values\n",
    "            p = g[\"mortality_probability\"].astype(float).values\n",
    "            pos = int(y.sum()); neg = int((1-y).sum())\n",
    "            if pos < MIN_POS or neg < MIN_NEG: \n",
    "                continue\n",
    "            auc, var = delong_auc_var(y, p)\n",
    "            auc = float(np.clip(auc, 0.5+EPS, 1-EPS))\n",
    "            se = math.sqrt(var)\n",
    "            site_rows.append(dict(hospitalid=int(hid), N=len(g), Pos=pos, Neg=neg,\n",
    "                                  AUROC=auc, SE=se,\n",
    "                                  CI_low=max(0.5, auc-1.96*se), CI_high=min(1.0, auc+1.96*se)))\n",
    "        site_df = pd.DataFrame(site_rows).sort_values(\"N\", ascending=False)\n",
    "        _save_df(site_df, TAB / f\"site_auc_filtered_w{w}h.csv\")\n",
    "        pooled = meta_RE_logit_auc(site_df[\"AUROC\"].values, (site_df[\"SE\"].values**2)) if not site_df.empty else None\n",
    "        if pooled:\n",
    "            pd.DataFrame([dict(window_h=w, **pooled)]).to_csv(TAB / f\"site_meta_summary_w{w}h.csv\", index=False)\n",
    "        forest_plot(site_df, pooled,\n",
    "                    f\"Hospital AUROC (eICU, {w}h) â€” filtered (â‰¥{MIN_POS} pos & â‰¥{MIN_NEG} neg)\",\n",
    "                    FIG / f\"fig_site_forest_filtered_w{w}h.png\")\n",
    "        if pooled:\n",
    "            print(f\"   site meta: pooled={pooled['pooled_auc']:.3f} [{pooled['ci_low']:.3f},{pooled['ci_high']:.3f}]  IÂ²={pooled['I2']:.1f}%  k={len(site_df)}\")\n",
    "        else:\n",
    "            print(\"   site meta: (no eligible hospitals)\")\n",
    "\n",
    "    # Temporal performance (BMC-style equal-count blocks)\n",
    "    if DO_TEMPORAL:\n",
    "        comb = pd.read_parquet(FEAT / f\"combined_w{w}.parquet\")\n",
    "        comb[\"source\"] = comb[\"source\"].astype(str).str.lower()\n",
    "        # predict probs for both datasets so temporal works on each\n",
    "        feats = _read_features_list(w); imp, cal, _ = _load_imputer_calibrator(w)\n",
    "        X = comb[feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X_imp = imp.transform(X.values)\n",
    "        p_hat = cal.predict_proba(X_imp)[:,1] if hasattr(cal,\"predict_proba\") else np.clip(cal.predict(X_imp).ravel(), 0, 1)\n",
    "        comb = comb.assign(prob=p_hat, label=comb[\"hospital_expire_flag\"].astype(int))\n",
    "\n",
    "        # join year map\n",
    "        ym = YEAR_MAP.rename(columns={\"stay_id\":\"stay_id_join\"})\n",
    "        comb = comb.merge(ym, left_on=[\"stay_id\",\"source\"], right_on=[\"stay_id_join\",\"dataset\"], how=\"left\")\n",
    "        comb.drop(columns=[\"stay_id_join\"], inplace=True)\n",
    "        # dataset names to display\n",
    "        comb[\"dataset_disp\"] = comb[\"source\"].map({\"eicu\":\"eICU\",\"mimic\":\"MIMIC-IV\"}).fillna(comb[\"source\"])\n",
    "\n",
    "        # make blocks per dataset\n",
    "        lines = []\n",
    "        for ds in [\"mimic\",\"eicu\"]:\n",
    "            dsd = comb[comb[\"source\"]==ds].dropna(subset=[\"admit_year\"]).copy()\n",
    "            tb = temporal_blocks(dsd, n_blocks=N_TEMP_BLOCKS)\n",
    "            if tb is None:\n",
    "                continue\n",
    "            gb = (tb.groupby(\"time_block\")\n",
    "                    .apply(lambda g: pd.Series(dict(\n",
    "                        N=len(g),\n",
    "                        prevalence=float(g[\"label\"].mean()),\n",
    "                        AUROC=roc_auc_score(g[\"label\"], g[\"prob\"]),\n",
    "                        Brier=brier_score_loss(g[\"label\"], g[\"prob\"]),\n",
    "                        ECE=_ece(g[\"label\"], g[\"prob\"], bins=10),\n",
    "                        slope=_calib_slope_intercept_irls(g[\"label\"], g[\"prob\"])[0]\n",
    "                    ))).reset_index())\n",
    "            gb[\"dataset\"] = \"MIMIC-IV\" if ds==\"mimic\" else \"eICU\"\n",
    "            lines.append(gb)\n",
    "            # save table\n",
    "            _save_df(gb, TAB / f\"temporal_blocks_{ds}_w{w}h.csv\")\n",
    "\n",
    "        if lines:\n",
    "            L = pd.concat(lines, ignore_index=True)\n",
    "            # Figures (smooth lines without dots/years)\n",
    "            # AUROC\n",
    "            outp = FIG / f\"fig_temporal_auroc_bmc_w{w}h.png\"\n",
    "            def _save():\n",
    "                plt.figure(figsize=(8.4,4.2))\n",
    "                for ds, g in L.groupby(\"dataset\"):\n",
    "                    g = g.sort_values(\"time_block\")\n",
    "                    plt.plot(g[\"time_block\"], g[\"AUROC\"], \"-\", marker=None, label=ds)\n",
    "                plt.xlabel(\"Temporal block (equal-count)\"); plt.ylabel(\"AUROC\")\n",
    "                plt.ylim(0.5, 1.0); plt.title(f\"Temporal performance â€” AUROC ({w}h)\")\n",
    "                plt.legend(); plt.tight_layout(); plt.savefig(outp, dpi=200); plt.close()\n",
    "            _save_fig(outp, _save)\n",
    "\n",
    "            # Brier & ECE (two separate simple line figs)\n",
    "            outp2 = FIG / f\"fig_temporal_brier_bmc_w{w}h.png\"\n",
    "            def _save2():\n",
    "                plt.figure(figsize=(8.4,4.2))\n",
    "                for ds, g in L.groupby(\"dataset\"):\n",
    "                    g = g.sort_values(\"time_block\")\n",
    "                    plt.plot(g[\"time_block\"], g[\"Brier\"], \"-\", marker=None, label=ds)\n",
    "                plt.xlabel(\"Temporal block (equal-count)\"); plt.ylabel(\"Brier score\")\n",
    "                plt.title(f\"Temporal performance â€” Brier ({w}h)\")\n",
    "                plt.legend(); plt.tight_layout(); plt.savefig(outp2, dpi=200); plt.close()\n",
    "            _save_fig(outp2, _save2)\n",
    "\n",
    "            outp3 = FIG / f\"fig_temporal_ece_bmc_w{w}h.png\"\n",
    "            def _save3():\n",
    "                plt.figure(figsize=(8.4,4.2))\n",
    "                for ds, g in L.groupby(\"dataset\"):\n",
    "                    g = g.sort_values(\"time_block\")\n",
    "                    plt.plot(g[\"time_block\"], g[\"ECE\"], \"-\", marker=None, label=ds)\n",
    "                plt.xlabel(\"Temporal block (equal-count)\"); plt.ylabel(\"ECE\")\n",
    "                plt.title(f\"Temporal performance â€” ECE ({w}h)\")\n",
    "                plt.legend(); plt.tight_layout(); plt.savefig(outp3, dpi=200); plt.close()\n",
    "            _save_fig(outp3, _save3)\n",
    "\n",
    "            outp4 = FIG / f\"fig_temporal_slope_bmc_w{w}h.png\"\n",
    "            def _save4():\n",
    "                plt.figure(figsize=(8.4,4.2))\n",
    "                for ds, g in L.groupby(\"dataset\"):\n",
    "                    g = g.sort_values(\"time_block\")\n",
    "                    plt.plot(g[\"time_block\"], g[\"slope\"], \"-\", marker=None, label=ds)\n",
    "                plt.xlabel(\"Temporal block (equal-count)\"); plt.ylabel(\"Calibration slope\")\n",
    "                plt.title(f\"Temporal calibration slope ({w}h)\")\n",
    "                plt.legend(); plt.tight_layout(); plt.savefig(outp4, dpi=200); plt.close()\n",
    "            _save_fig(outp4, _save4)\n",
    "        else:\n",
    "            print(\"   temporal: insufficient data for blocks; skipped\")\n",
    "\n",
    "# =====================\n",
    "# Run all windows\n",
    "# =====================\n",
    "for w in WINDOWS:\n",
    "    run_window(w)\n",
    "\n",
    "print(\"\\nâœ… One-shot analysis finished. Check:\")\n",
    "print(\" - Tables:\", TAB)\n",
    "print(\" - Figures:\", FIG)\n",
    "print(\"Toggles used:\",\n",
    "      f\"CI={DO_BOOTSTRAP_CI}, OP={DO_OP_POINTS}, SITE={DO_SITE_HET}, TEMP={DO_TEMPORAL}, FAIR={DO_FAIRNESS}, DCA={DO_DCA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust compact temporal panels (AUROC + Brier + ECE) ---\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# expects: TAB (tables Path), FIG (figures Path), WINDOWS list, DPI int defined\n",
    "# e.g., TAB = Path(\".../manuscript/tables\"); FIG = Path(\".../manuscript/figures\"); WINDOWS=[6,12,18,24]; DPI=200\n",
    "\n",
    "def _norm_cols(df):\n",
    "    \"\"\"Lowercase map of columns for flexible access.\"\"\"\n",
    "    return {c.lower(): c for c in df.columns}\n",
    "\n",
    "def _read_blocks_for_window(w):\n",
    "    \"\"\"Load equal-count temporal blocks if present: temporal_blocks_{dataset}_w{w}h.csv\"\"\"\n",
    "    files = list(TAB.glob(f\"temporal_blocks_*_w{w}h.csv\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    out = []\n",
    "    for p in files:\n",
    "        ds_hint = p.stem.split(\"_\")[2] if len(p.stem.split(\"_\")) >= 3 else \"eicu\"\n",
    "        ds = \"eICU\" if ds_hint.lower().startswith(\"ei\") else \"MIMIC\"\n",
    "        df = pd.read_csv(p)\n",
    "        c = _norm_cols(df)\n",
    "        xcol = c.get(\"block\") or c.get(\"temporal_block\") or c.get(\"bin\") or list(df.columns)[0]\n",
    "        auroc = c.get(\"auroc\") or c.get(\"auc\") or c.get(\"roc_auc\")\n",
    "        brier = c.get(\"brier\")\n",
    "        ece   = c.get(\"ece\") or c.get(\"calibration_ece\")\n",
    "        tmp = pd.DataFrame({\n",
    "            \"dataset\": ds,\n",
    "            \"x\": df[xcol].astype(float),\n",
    "            \"AUROC\": df[auroc].astype(float) if auroc else np.nan,\n",
    "            \"Brier\": df[brier].astype(float) if brier else np.nan,\n",
    "            \"ECE\":   df[ece].astype(float)   if ece   else np.nan,\n",
    "        })\n",
    "        out.append(tmp)\n",
    "    res = pd.concat(out, ignore_index=True)\n",
    "    # keep rows where at least one metric is present\n",
    "    return res.dropna(subset=[\"AUROC\",\"Brier\",\"ECE\"], how=\"all\")\n",
    "\n",
    "def _read_metrics_for_window(w):\n",
    "    \"\"\"Load per-year temporal metrics if present: temporal_metrics_w{w}h.csv\"\"\"\n",
    "    p = TAB / f\"temporal_metrics_w{w}h.csv\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(p)\n",
    "    c = _norm_cols(df)\n",
    "    year = c.get(\"admit_year\") or c.get(\"year\") or c.get(\"admission_year\")\n",
    "    if year is None:\n",
    "        return None\n",
    "    dataset = c.get(\"dataset\")\n",
    "    auroc = c.get(\"auroc\") or c.get(\"auc\")\n",
    "    brier = c.get(\"brier\")\n",
    "    ece   = c.get(\"ece\")\n",
    "    if all(k is None for k in [auroc, brier, ece]):\n",
    "        return None\n",
    "    out = pd.DataFrame({\n",
    "        \"dataset\": (df[dataset].astype(str) if dataset else \"eICU\"),\n",
    "        \"x\": df[year].astype(float),\n",
    "        \"AUROC\": df[auroc].astype(float) if auroc else np.nan,\n",
    "        \"Brier\": df[brier].astype(float) if brier else np.nan,\n",
    "        \"ECE\":   df[ece].astype(float)   if ece   else np.nan,\n",
    "    })\n",
    "    # normalize dataset labels\n",
    "    out[\"dataset\"] = out[\"dataset\"].str.replace(\"^ei.*\", \"eICU\", case=False, regex=True)\\\n",
    "                                   .str.replace(\"^mi.*\", \"MIMIC\", case=False, regex=True)\n",
    "    return out.dropna(subset=[\"AUROC\",\"Brier\",\"ECE\"], how=\"all\")\n",
    "\n",
    "def _prep_temporal_source(w):\n",
    "    \"\"\"Return (df, x_label, source_tag) picking blocks first, else metrics.\"\"\"\n",
    "    df = _read_blocks_for_window(w)\n",
    "    if df is not None and len(df):\n",
    "        return df.sort_values([\"dataset\",\"x\"]), \"Temporal block (equal-count)\", \"blocks\"\n",
    "    df = _read_metrics_for_window(w)\n",
    "    if df is not None and len(df):\n",
    "        return df.sort_values([\"dataset\",\"x\"]), \"Admission year\", \"metrics\"\n",
    "    return None, None, None\n",
    "\n",
    "def _style_axes(ax):\n",
    "    ax.grid(True, color=\"#e6e6e6\", linewidth=1.0, alpha=0.9)\n",
    "    for spine in [\"top\",\"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "def do_temporal_panels():\n",
    "    print(\"=== Compact temporal panels (AUROC/Brier/ECE) ===\")\n",
    "    FIG.mkdir(parents=True, exist_ok=True)\n",
    "    for w in WINDOWS:\n",
    "        df, xlabel, src = _prep_temporal_source(w)\n",
    "        if df is None or df.empty:\n",
    "            print(f\"[{w}h] skipped: no temporal data found (blocks or metrics).\")\n",
    "            continue\n",
    "\n",
    "        # Build figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharex=True)\n",
    "        metrics = [(\"AUROC\", \"AUROC\"), (\"Brier\", \"Brier score\"), (\"ECE\", \"ECE\")]\n",
    "        colors = {\"eICU\":\"C0\", \"MIMIC\":\"C1\"}\n",
    "\n",
    "        for ax, (mcol, mname) in zip(axes, metrics):\n",
    "            for ds, g in df.groupby(\"dataset\"):\n",
    "                # sorted by x\n",
    "                g = g.sort_values(\"x\")\n",
    "                ax.plot(g[\"x\"], g[mcol], marker=\"o\", linewidth=2.2,\n",
    "                        label=ds, alpha=0.95, color=colors.get(ds, None))\n",
    "            ax.set_ylabel(mname)\n",
    "            _style_axes(ax)\n",
    "\n",
    "        axes[0].set_title(f\"Temporal performance â€” {w}h\")\n",
    "        axes[1].legend(loc=\"best\", frameon=False)\n",
    "        axes[1].set_xlabel(xlabel)\n",
    "        axes[2].set_xlabel(xlabel)\n",
    "\n",
    "        out = FIG / f\"fig_temporal_panel_w{w}h.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out, dpi=DPI, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"âœ“ wrote panel: {out}\")\n",
    "\n",
    "# ---- run it ----\n",
    "do_temporal_panels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct eICU LOS and re-filter predictions\n",
    "\n",
    "import pandas as pd, numpy as np, os, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")  # <- change if needed\n",
    "COHORT = ROOT / \"outputs\" / \"cohorts\" / \"eicu_mi_cohort.csv\"\n",
    "\n",
    "def ts(): return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def get_id_col(df):\n",
    "    for c in [\"patientunitstayid\",\"stay_id\",\"icustay_id\",\"subject_id\"]:\n",
    "        if c in df.columns: return c\n",
    "    raise ValueError(\"No ID column found (expected one of patientunitstayid/stay_id/icustay_id/subject_id).\")\n",
    "\n",
    "def compute_los_hours(df):\n",
    "    \"\"\"Return a pd.Series 'los_hours_fixed' computed from the best available inputs.\"\"\"\n",
    "    # 1) Prefer offsets if both present\n",
    "    if {\"unitdischargeoffset\",\"unitadmitoffset\"}.issubset(df.columns):\n",
    "        udo = pd.to_numeric(df[\"unitdischargeoffset\"], errors=\"coerce\")\n",
    "        uao = pd.to_numeric(df[\"unitadmitoffset\"], errors=\"coerce\")\n",
    "        los = (udo - uao) / 60.0\n",
    "        return los\n",
    "\n",
    "    # 2) Else use datetimes if present\n",
    "    if {\"intime\",\"outtime\"}.issubset(df.columns):\n",
    "        tin = pd.to_datetime(df[\"intime\"], errors=\"coerce\")\n",
    "        tout = pd.to_datetime(df[\"outtime\"], errors=\"coerce\")\n",
    "        los = (tout - tin).dt.total_seconds() / 3600.0\n",
    "        return los\n",
    "\n",
    "    # 3) Else fall back to an existing hours column (already in hours)\n",
    "    for c in [\"icu_los_hours\",\"los_icu_hours\",\"los_hours\",\"icu_los\"]:\n",
    "        if c in df.columns:\n",
    "            return pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Cannot compute LOS: no offsets, no datetimes, and no *los*_hours column found.\"\n",
    "    )\n",
    "\n",
    "# --- Load cohort and compute LOS\n",
    "if not COHORT.exists():\n",
    "    raise FileNotFoundError(f\"Missing cohort file: {COHORT}\")\n",
    "\n",
    "co = pd.read_csv(COHORT)\n",
    "id_col = get_id_col(co)\n",
    "\n",
    "los = compute_los_hours(co)\n",
    "\n",
    "# Clean up pathologies\n",
    "los = los.where(los >= 0)                           # drop negative\n",
    "los = los.where(los <= 1e4)                         # sanity cap (~>1 year in ICU)\n",
    "co[\"icu_los_hours_fixed\"] = los\n",
    "\n",
    "# Write a sidecar with ID + LOS (non-breaking), and back up cohort\n",
    "bak = COHORT.with_suffix(f\".bak_{ts()}.csv\")\n",
    "os.replace(COHORT, bak)\n",
    "co.to_csv(COHORT, index=False)\n",
    "sidecar = COHORT.with_name(\"eicu_los_fixed.csv\")\n",
    "co[[id_col, \"icu_los_hours_fixed\"]].to_csv(sidecar, index=False)\n",
    "print(f\"ðŸ”§ Cohort fixed LOS written. Cohort backup â†’ {bak.name} | Sidecar â†’ {sidecar.name}\")\n",
    "\n",
    "# --- Re-filter each eICU prediction file by corrected LOS\n",
    "pred_files = []\n",
    "for p in ROOT.rglob(\"*.csv\"):\n",
    "    if re.search(r\"eicu_preds_(6|12|18|24)h\\.csv$\", p.name, flags=re.I):\n",
    "        pred_files.append(p)\n",
    "\n",
    "if not pred_files:\n",
    "    print(\"âš ï¸ No eicu_preds_{6,12,18,24}h.csv files found under project root.\")\n",
    "else:\n",
    "    print(\"Found prediction files:\")\n",
    "    for p in pred_files: print(\" â€¢\", p)\n",
    "\n",
    "for p in pred_files:\n",
    "    # Infer the window\n",
    "    m = re.search(r\"eicu_preds_(6|12|18|24)h\\.csv$\", p.name, flags=re.I)\n",
    "    w = int(m.group(1))\n",
    "\n",
    "    dfp = pd.read_csv(p)\n",
    "    pid = None\n",
    "    for c in [id_col, \"patientunitstayid\",\"stay_id\",\"icustay_id\",\"subject_id\"]:\n",
    "        if c in dfp.columns:\n",
    "            pid = c; break\n",
    "    if pid is None:\n",
    "        print(f\"  âœ– {p.name}: no ID column; skipped.\")\n",
    "        continue\n",
    "\n",
    "    merged = dfp.merge(co[[id_col,\"icu_los_hours_fixed\"]],\n",
    "                       left_on=pid, right_on=id_col, how=\"left\")\n",
    "    before = len(merged)\n",
    "    keep = merged[\"icu_los_hours_fixed\"] >= w\n",
    "    fixed = merged.loc[keep, dfp.columns]  # drop merge cols, keep original schema\n",
    "\n",
    "    pbak = p.with_suffix(f\".bak_{ts()}.csv\")\n",
    "    os.replace(p, pbak)\n",
    "    fixed.to_csv(p, index=False)\n",
    "    print(f\"[{w}h] {p.name}: kept {len(fixed):,}/{before:,} rows (backup â†’ {pbak.name})\")\n",
    "\n",
    "print(\"âœ… LOS correction & eligibility filter complete. Re-run metrics/figures to refresh outputs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional codemod: replace unitdischargeoffset/60 with (unitdischargeoffset - unitadmitoffset)/60 ---\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "PAT  = re.compile(r\"unitdischargeoffset\\s*/\\s*60\\b\")\n",
    "\n",
    "def ts(): return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def fix_text(txt):\n",
    "    return PAT.sub(\"(unitdischargeoffset - unitadmitoffset)/60\", txt)\n",
    "\n",
    "changed = 0\n",
    "\n",
    "# .py files\n",
    "for p in ROOT.rglob(\"*.py\"):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    new = fix_text(txt)\n",
    "    if new != txt:\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts()}\")\n",
    "        p.replace(bak)\n",
    "        p.write_text(new, encoding=\"utf-8\")\n",
    "        print(f\"âœ” fixed {p} (backup â†’ {bak.name})\"); changed += 1\n",
    "\n",
    "# .ipynb files (update only code cells)\n",
    "for p in ROOT.rglob(\"*.ipynb\"):\n",
    "    try:\n",
    "        nb = json.loads(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception:\n",
    "        continue\n",
    "    touched = False\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") == \"code\" and isinstance(cell.get(\"source\"), list):\n",
    "            src = \"\".join(cell[\"source\"])\n",
    "            new = fix_text(src)\n",
    "            if new != src:\n",
    "                cell[\"source\"] = [new]\n",
    "                touched = True\n",
    "    if touched:\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{ts()}\")\n",
    "        p.replace(bak)\n",
    "        p.write_text(json.dumps(nb, ensure_ascii=False, indent=1), encoding=\"utf-8\")\n",
    "        print(f\"âœ” fixed {p} (backup â†’ {bak.name})\"); changed += 1\n",
    "\n",
    "print(f\"Done. Files changed: {changed}\")\n",
    "print(\"Note: No results change until you re-run the affected notebooks/scripts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Calibration reliability with 95% CIs, slope, intercept, CITL (no sklearn fit) ===\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES  = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "TAB  = ROOT / \"manuscript\" / \"tables\"; TAB.mkdir(parents=True, exist_ok=True)\n",
    "FIG  = ROOT / \"manuscript\" / \"figures\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "WINDOWS = [6,12,18,24]\n",
    "EPS = 1e-12\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clip_probs(p):\n",
    "    p = np.asarray(p, float).ravel()\n",
    "    return np.clip(p, EPS, 1.0 - EPS)\n",
    "\n",
    "def _ece(y, p, bins=10):\n",
    "    # equal-frequency bins\n",
    "    order = np.argsort(p)\n",
    "    yb = y[order]; pb = p[order]\n",
    "    splits = np.array_split(np.arange(len(p)), bins)\n",
    "    ece = 0.0\n",
    "    for idx in splits:\n",
    "        if len(idx)==0: continue\n",
    "        obs = yb[idx].mean()\n",
    "        exp = pb[idx].mean()\n",
    "        ece += (len(idx)/len(p)) * abs(obs-exp)\n",
    "    return float(ece)\n",
    "\n",
    "def _logit(p):\n",
    "    p = _clip_probs(p)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def _irls_logit(y, X, offset=None, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Simple IRLS for logistic regression:\n",
    "      y ~ Bernoulli(sigmoid(X b + offset))\n",
    "    Returns coef vector b (shape d), intercept is included if X has a column of 1s.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float).ravel()\n",
    "    X = np.asarray(X, float)\n",
    "    n, d = X.shape\n",
    "    off = np.zeros(n) if offset is None else np.asarray(offset, float).ravel()\n",
    "    b = np.zeros(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eta = X @ b + off\n",
    "        p = 1.0/(1.0 + np.exp(-eta))\n",
    "        W = p*(1-p) + 1e-12\n",
    "        z = eta + (y - p)/W\n",
    "        # weighted least squares: (X^T W X) b = X^T W (z - off)\n",
    "        WX = X * W[:,None]\n",
    "        XtWX = WX.T @ X\n",
    "        XtWz = WX.T @ (z - off)\n",
    "        try:\n",
    "            b_new = np.linalg.solve(XtWX, XtWz)\n",
    "        except np.linalg.LinAlgError:\n",
    "            b_new = np.linalg.pinv(XtWX) @ XtWz\n",
    "        if np.max(np.abs(b_new - b)) < tol:\n",
    "            b = b_new\n",
    "            break\n",
    "        b = b_new\n",
    "    return b\n",
    "\n",
    "def calib_slope_intercept(y, p):\n",
    "    \"\"\"Fit logit(y) = a + b * logit(p_hat) via IRLS; return slope b, intercept a.\"\"\"\n",
    "    x = _logit(p).reshape(-1,1)\n",
    "    X = np.c_[np.ones_like(x), x]  # [intercept, slope]\n",
    "    b = _irls_logit(y, X)\n",
    "    a, slope = float(b[0]), float(b[1])\n",
    "    return slope, a\n",
    "\n",
    "def citl_offset(y, p):\n",
    "    \"\"\"Calibration-in-the-large: fit logit(y) = alpha + 1*logit(p_hat) (slope fixed at 1) via offset.\"\"\"\n",
    "    x = _logit(p)\n",
    "    X = np.ones((len(y), 1))  # intercept only\n",
    "    b = _irls_logit(y, X, offset=x)\n",
    "    return float(b[0])  # alpha\n",
    "\n",
    "def wilson_ci(k, n, conf=0.95):\n",
    "    # 95% Wilson score interval\n",
    "    if n == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    z = 1.959963984540054 # 1.96\n",
    "    phat = k/n\n",
    "    denom = 1 + z**2/n\n",
    "    center = (phat + z**2/(2*n))/denom\n",
    "    half = (z*np.sqrt((phat*(1-phat) + z**2/(4*n))/n))/denom\n",
    "    return max(0.0, center - half), min(1.0, center + half)\n",
    "\n",
    "def reliability_bins(y, p, bins=10):\n",
    "    # equal-frequency bins with Wilson 95% CI for observed rate\n",
    "    order = np.argsort(p)\n",
    "    yb = y[order]; pb = p[order]\n",
    "    idx_splits = np.array_split(np.arange(len(p)), bins)\n",
    "    rows = []\n",
    "    for i, idx in enumerate(idx_splits, start=1):\n",
    "        if len(idx)==0: continue\n",
    "        obs_n = int(yb[idx].sum()); n = int(len(idx))\n",
    "        obs = obs_n/n\n",
    "        exp = float(pb[idx].mean())\n",
    "        lo, hi = wilson_ci(obs_n, n)\n",
    "        rows.append({\"bin\": i, \"n\": n, \"pred_mean\": exp, \"obs_rate\": obs, \"obs_ci_low\": lo, \"obs_ci_high\": hi})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_reliability(df_bins, slope, intercept, citl, title, out_png):\n",
    "    plt.figure(figsize=(6.5,5.2))\n",
    "    # error bars for observed with CI\n",
    "    plt.errorbar(df_bins[\"pred_mean\"], df_bins[\"obs_rate\"],\n",
    "                 yerr=[df_bins[\"obs_rate\"]-df_bins[\"obs_ci_low\"], df_bins[\"obs_ci_high\"]-df_bins[\"obs_rate\"]],\n",
    "                 fmt=\"o\", capsize=3, alpha=0.9, label=\"Observed (bin-wise 95% CI)\")\n",
    "    # identity\n",
    "    xs = np.linspace(0,1,200)\n",
    "    plt.plot(xs, xs, \"--\", alpha=0.6, label=\"Perfect calibration\")\n",
    "    # optional fitted line on prob scale: logistic(a + b*logit(x))\n",
    "    def inv_logit(z): return 1.0/(1.0 + np.exp(-z))\n",
    "    fit_curve = inv_logit(intercept + slope*np.log(np.clip(xs,EPS,1-EPS)/(1-np.clip(xs,EPS,1-EPS))))\n",
    "    plt.plot(xs, fit_curve, \"-\", linewidth=2, label=\"Fitted calibration\")\n",
    "\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Observed mortality\")\n",
    "    txt = f\"slope={slope:.3f} | intercept={intercept:.3f} | CITL={citl:.3f}\"\n",
    "    plt.title(f\"{title}\\n{txt}\")\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.legend(frameon=False, loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300); plt.close()\n",
    "\n",
    "# ---------- main ----------\n",
    "all_summary = []\n",
    "for w in WINDOWS:\n",
    "    preds = RES / f\"eicu_preds_{w}h.csv\"\n",
    "    if not preds.exists():\n",
    "        print(f\"[{w}h] âœ— missing {preds.name}; skipping.\")\n",
    "        continue\n",
    "    df = pd.read_csv(preds)\n",
    "    # flexible column grab\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    ycol = None\n",
    "    for cand in [\"label\",\"hospital_expire_flag\",\"y_true\",\"outcome\",\"mortality\",\"hospital_death\"]:\n",
    "        if cand in cols: ycol = cols[cand]; break\n",
    "    pcol = None\n",
    "    for cand in [\"mortality_probability\",\"prob\",\"pred\",\"p_hat\",\"prediction\"]:\n",
    "        if cand in cols: pcol = cols[cand]; break\n",
    "    if ycol is None or pcol is None:\n",
    "        print(f\"[{w}h] âœ— cannot find y/p columns; skipping.\")\n",
    "        continue\n",
    "\n",
    "    y = df[ycol].astype(int).values\n",
    "    p = _clip_probs(df[pcol].values.astype(float))\n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(f\"[{w}h] âœ— only one class present; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # metrics\n",
    "    auroc = roc_auc_score(y, p)\n",
    "    auprc = average_precision_score(y, p)\n",
    "    brier = brier_score_loss(y, p)\n",
    "    ece   = _ece(y, p, bins=10)\n",
    "    slope, intercept = calib_slope_intercept(y, p)\n",
    "    citl  = citl_offset(y, p)\n",
    "\n",
    "    # bins + figure\n",
    "    bins_df = reliability_bins(y, p, bins=10)\n",
    "    bins_csv = TAB / f\"calib_bins_w{w}h.csv\"\n",
    "    bins_df.to_csv(bins_csv, index=False)\n",
    "\n",
    "    title = f\"Calibration reliability (eICU, {w}h)\"\n",
    "    out_png = FIG / f\"fig_calibration_reliability_w{w}h.png\"\n",
    "    plot_reliability(bins_df, slope, intercept, citl, title, out_png)\n",
    "\n",
    "    # per-window summary\n",
    "    row = dict(window_h=w, N=int(len(y)), prevalence=float(y.mean()),\n",
    "               AUROC=float(auroc), AUPRC=float(auprc), Brier=float(brier),\n",
    "               ECE=float(ece), slope=float(slope), intercept=float(intercept), CITL=float(citl))\n",
    "    pd.DataFrame([row]).to_csv(TAB / f\"calib_summary_w{w}h.csv\", index=False)\n",
    "    all_summary.append(row)\n",
    "\n",
    "    print(f\"[{w}h] N={len(y)} | AUROC={auroc:.3f} | AUPRC={auprc:.3f} | \"\n",
    "          f\"Brier={brier:.3f} | ECE={ece:.3f} | slope={slope:.3f} | intercept={intercept:.3f} | CITL={citl:.3f}\")\n",
    "    print(f\"     â†’ {bins_csv}\\n     â†’ {out_png}\\n     â†’ {TAB / f'calib_summary_w{w}h.csv'}\")\n",
    "\n",
    "if all_summary:\n",
    "    pd.DataFrame(all_summary).to_csv(TAB / \"calib_summary_all_windows.csv\", index=False)\n",
    "    print(\"âœ“ Saved combined:\", TAB / \"calib_summary_all_windows.csv\")\n",
    "else:\n",
    "    print(\"No calibration outputs produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "must = [\n",
    "  \"outputs/external_validation/models/imputer_w6.pkl\",\n",
    "  \"outputs/external_validation/models/calibrator_w6.pkl\",\n",
    "  \"outputs/external_validation/models/threshold_w6.csv\",\n",
    "  \"outputs/external_validation/results/eicu_preds_6h.csv\",\n",
    "  \"outputs/external_validation/results/eicu_preds_12h.csv\",\n",
    "  \"outputs/external_validation/results/eicu_preds_18h.csv\",\n",
    "  \"outputs/external_validation/results/eicu_preds_24h.csv\",\n",
    "  \"manuscript/figures/fig_dca_w6h.png\",\n",
    "  \"manuscript/figures/fig_temporal_panel_w6h.png\",\n",
    "  \"manuscript/tables/headline_table.csv\",\n",
    "]\n",
    "for rel in must:\n",
    "    p = root/rel\n",
    "    print((\"OK  \" if p.exists() else \"MISS\"), p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "need = [\n",
    "  # features\n",
    "  *[Path(f\"outputs/features/combined_w{w}.parquet\") for w in [6,12,18,24]],\n",
    "  # external-validation artifacts\n",
    "  *[Path(f\"outputs/external_validation/models/imputer_w{w}.pkl\") for w in [6,12,18,24]],\n",
    "  *[Path(f\"outputs/external_validation/models/calibrator_w{w}.pkl\") for w in [6,12,18,24]],\n",
    "  # predictions used by DCA/calibration\n",
    "  *[Path(f\"outputs/external_validation/results/eicu_preds_{w}h.csv\") for w in [6,12,18,24]],\n",
    "]\n",
    "missing = [str(p) for p in need if not p.exists()]\n",
    "print(\"All good!\" if not missing else \"Missing:\\n- \" + \"\\n- \".join(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 95% CIs for PPV/NPV/Sensitivity/Specificity/Accuracy/Prevalence at the operating threshold\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, warnings, math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- paths / config ----\n",
    "ROOT   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES    = ROOT / \"outputs\" / \"external_validation\" / \"results\"\n",
    "MDIR   = ROOT / \"outputs\" / \"external_validation\" / \"models\"\n",
    "TABS   = ROOT / \"manuscript\" / \"tables\"\n",
    "TABS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "ALPHA   = 0.05  # 95% CI\n",
    "\n",
    "# ---- utils ----\n",
    "def _detect_cols(df):\n",
    "    label_opts = [\"label\",\"hospital_expire_flag\",\"y_true\",\"outcome\",\"death\",\"y\"]\n",
    "    prob_opts  = [\"mortality_probability\",\"prob_cal\",\"prob\",\"pred_prob\",\"y_proba\",\"probability\",\"risk_prob\",\"proba\",\"pred\"]\n",
    "    y_col = next((c for c in label_opts if c in df.columns), None)\n",
    "    p_col = next((c for c in prob_opts if c in df.columns), None)\n",
    "    if y_col is None or p_col is None:\n",
    "        raise RuntimeError(\"Could not detect label/probability columns\")\n",
    "    return y_col, p_col\n",
    "\n",
    "def _youden_threshold(y, p):\n",
    "    # Pick threshold that maximizes tpr - fpr (Youden J)\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    # Unique sorted thresholds (include 0 and 1 guards)\n",
    "    th = np.unique(p)\n",
    "    if th[0] > 0: th = np.r_[0.0, th]\n",
    "    if th[-1] < 1: th = np.r_[th, 1.0]\n",
    "    P = (y==1).sum()\n",
    "    N = (y==0).sum()\n",
    "    best = (0.5, -np.inf)\n",
    "    for t in th:\n",
    "        pred = (p >= t)\n",
    "        TP = int(((pred==1) & (y==1)).sum())\n",
    "        FP = int(((pred==1) & (y==0)).sum())\n",
    "        FN = P - TP\n",
    "        TN = N - FP\n",
    "        tpr = TP / P if P else 0.0\n",
    "        fpr = FP / N if N else 0.0\n",
    "        J = tpr - fpr\n",
    "        if J > best[1]: best = (float(t), J)\n",
    "    return float(best[0])\n",
    "\n",
    "# exact (Clopperâ€“Pearson) proportion CI with Wilson fallback\n",
    "try:\n",
    "    from scipy.stats import beta, norm\n",
    "    _HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    _HAVE_SCIPY = False\n",
    "\n",
    "def prop_ci(k, n, alpha=ALPHA):\n",
    "    if n == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    p = k / n\n",
    "    if _HAVE_SCIPY:\n",
    "        lo = beta.ppf(alpha/2, k, n-k+1) if k>0 else 0.0\n",
    "        hi = beta.ppf(1-alpha/2, k+1, n-k) if k<n else 1.0\n",
    "        return float(lo), float(hi)\n",
    "    # Wilson fallback\n",
    "    z = 1.959963984540054  # ~95%\n",
    "    denom = 1 + z**2/n\n",
    "    center = (p + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * math.sqrt(p*(1-p)/n + z*z/(4*n*n))\n",
    "    return float(max(0.0, center - half)), float(min(1.0, center + half))\n",
    "\n",
    "def lr_ci(lr, tp, fn, tn, fp, alpha=ALPHA):\n",
    "    \"\"\"\n",
    "    Log-normal CI for LR+ and LR- using standard approximation on log scale.\n",
    "    Returns (lo, hi); if lr is inf/0 or cells invalid, returns (nan, nan).\n",
    "    \"\"\"\n",
    "    if not _HAVE_SCIPY:\n",
    "        # simple guard if SciPy missing\n",
    "        z = 1.959963984540054\n",
    "    else:\n",
    "        from scipy.stats import norm\n",
    "        z = norm.ppf(1 - alpha/2)\n",
    "\n",
    "    if lr is None or lr<=0 or not np.isfinite(lr):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Per Gart & Nam-style approximation:\n",
    "    # Var(log LR+) â‰ˆ 1/TP - 1/(TP+FN) + 1/FP - 1/(FP+TN)\n",
    "    # Var(log LR-) â‰ˆ 1/FN - 1/(TP+FN) + 1/TN - 1/(FP+TN)\n",
    "    # Caller decides which to use.\n",
    "    return z\n",
    "\n",
    "def lr_plus_ci(tp, fn, tn, fp, alpha=ALPHA):\n",
    "    P = tp + fn\n",
    "    N = tn + fp\n",
    "    if P==0 or N==0 or fp==0 or tp==0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    se = tp / P\n",
    "    sp = tn / N\n",
    "    lr_plus = se / (1 - sp)\n",
    "    var_log = (1/max(tp,1)) - (1/max(P,1)) + (1/max(fp,1)) - (1/max(N,1))\n",
    "    se_log = math.sqrt(max(var_log, 0.0))\n",
    "    z = 1.959963984540054 if not _HAVE_SCIPY else norm.ppf(1 - alpha/2)\n",
    "    lo = math.exp(math.log(lr_plus) - z*se_log)\n",
    "    hi = math.exp(math.log(lr_plus) + z*se_log)\n",
    "    return float(lr_plus), float(lo), float(hi)\n",
    "\n",
    "def lr_minus_ci(tp, fn, tn, fp, alpha=ALPHA):\n",
    "    P = tp + fn\n",
    "    N = tn + fp\n",
    "    if P==0 or N==0 or tn==0 or fn==0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    se = tp / P\n",
    "    sp = tn / N\n",
    "    lr_minus = (1 - se) / sp\n",
    "    var_log = (1/max(fn,1)) - (1/max(P,1)) + (1/max(tn,1)) - (1/max(N,1))\n",
    "    se_log = math.sqrt(max(var_log, 0.0))\n",
    "    z = 1.959963984540054 if not _HAVE_SCIPY else norm.ppf(1 - alpha/2)\n",
    "    lo = math.exp(math.log(lr_minus) - z*se_log)\n",
    "    hi = math.exp(math.log(lr_minus) + z*se_log)\n",
    "    return float(lr_minus), float(lo), float(hi)\n",
    "\n",
    "def choose_threshold(window, y, p):\n",
    "    # 1) prefer external_validation/models/threshold_w{w}.csv\n",
    "    for base in [MDIR, ROOT / \"outputs\" / \"models\"]:\n",
    "        thr_csv = base / f\"threshold_w{window}.csv\"\n",
    "        if thr_csv.exists():\n",
    "            try:\n",
    "                t = float(pd.read_csv(thr_csv).iloc[0][\"threshold\"])\n",
    "                return float(t), f\"{thr_csv}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    # 2) else compute Youden on these preds\n",
    "    t = _youden_threshold(y, p)\n",
    "    return float(t), \"computed_Youden\"\n",
    "\n",
    "def one_window(window):\n",
    "    # load preds\n",
    "    pth = RES / f\"eicu_preds_{window}h.csv\"\n",
    "    if not pth.exists():\n",
    "        print(f\"[{window}h] âš ï¸ missing {pth}; skipping\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(pth)\n",
    "    y_col, p_col = _detect_cols(df)\n",
    "    y = df[y_col].astype(int).values\n",
    "    p = df[p_col].astype(float).values\n",
    "\n",
    "    thr, src = choose_threshold(window, y, p)\n",
    "    pred = (p >= thr).astype(int)\n",
    "\n",
    "    TP = int(((pred==1) & (y==1)).sum())\n",
    "    FP = int(((pred==1) & (y==0)).sum())\n",
    "    TN = int(((pred==0) & (y==0)).sum())\n",
    "    FN = int(((pred==0) & (y==1)).sum())\n",
    "\n",
    "    N = TP + FP + TN + FN\n",
    "    P = TP + FN\n",
    "    NN = TN + FP\n",
    "    PP = TP + FP\n",
    "    PN = TN + FN\n",
    "\n",
    "    # point estimates\n",
    "    prevalence = P / N if N else np.nan\n",
    "    sensitivity = TP / P if P else np.nan\n",
    "    specificity = TN / NN if NN else np.nan\n",
    "    ppv = TP / PP if PP else np.nan\n",
    "    npv = TN / PN if PN else np.nan\n",
    "    accuracy = (TP + TN) / N if N else np.nan\n",
    "\n",
    "    # exact (or Wilson) CIs\n",
    "    prev_lo, prev_hi = prop_ci(P, N)\n",
    "    sens_lo, sens_hi = prop_ci(TP, P) if P else (np.nan, np.nan)\n",
    "    spec_lo, spec_hi = prop_ci(TN, NN) if NN else (np.nan, np.nan)\n",
    "    ppv_lo, ppv_hi   = prop_ci(TP, PP) if PP else (np.nan, np.nan)\n",
    "    npv_lo, npv_hi   = prop_ci(TN, PN) if PN else (np.nan, np.nan)\n",
    "    acc_lo, acc_hi   = prop_ci(TP+TN, N) if N else (np.nan, np.nan)\n",
    "\n",
    "    # LR+ / LR- with log-normal CI\n",
    "    lr_p, lr_p_lo, lr_p_hi = lr_plus_ci(TP, FN, TN, FP)\n",
    "    lr_m, lr_m_lo, lr_m_hi = lr_minus_ci(TP, FN, TN, FP)\n",
    "\n",
    "    out = pd.DataFrame([{\n",
    "        \"window_h\": window,\n",
    "        \"threshold\": thr,\n",
    "        \"threshold_source\": src,\n",
    "        \"N\": N, \"Positives\": P, \"Negatives\": NN,\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"prevalence\": prevalence, \"prevalence_lo\": prev_lo, \"prevalence_hi\": prev_hi,\n",
    "        \"sensitivity\": sensitivity, \"sensitivity_lo\": sens_lo, \"sensitivity_hi\": sens_hi,\n",
    "        \"specificity\": specificity, \"specificity_lo\": spec_lo, \"specificity_hi\": spec_hi,\n",
    "        \"ppv\": ppv, \"ppv_lo\": ppv_lo, \"ppv_hi\": ppv_hi,\n",
    "        \"npv\": npv, \"npv_lo\": npv_lo, \"npv_hi\": npv_hi,\n",
    "        \"accuracy\": accuracy, \"accuracy_lo\": acc_lo, \"accuracy_hi\": acc_hi,\n",
    "        \"lr_plus\": lr_p, \"lr_plus_lo\": lr_p_lo, \"lr_plus_hi\": lr_p_hi,\n",
    "        \"lr_minus\": lr_m, \"lr_minus_lo\": lr_m_lo, \"lr_minus_hi\": lr_m_hi\n",
    "    }])\n",
    "\n",
    "    # write per-window file\n",
    "    out_path = TABS / f\"operating_point_ci_w{window}h.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"[{window}h] âœ“ wrote: {out_path}  (thr={thr:.3f}, from {src})\")\n",
    "    return out\n",
    "\n",
    "# ---- run for all windows & write combined ----\n",
    "rows = []\n",
    "for w in WINDOWS:\n",
    "    res = one_window(w)\n",
    "    if res is not None:\n",
    "        rows.append(res)\n",
    "\n",
    "if rows:\n",
    "    comb = pd.concat(rows, ignore_index=True)\n",
    "    comb_path = TABS / \"operating_point_ci_all_windows.csv\"\n",
    "    comb.to_csv(comb_path, index=False)\n",
    "    # pretty print\n",
    "    disp = comb[[\"window_h\",\"threshold\",\"N\",\"prevalence\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"accuracy\"]].copy()\n",
    "    print(\"\\nSummary (point estimates):\")\n",
    "    print(disp.round(3).to_string(index=False))\n",
    "    print(f\"\\nâœ“ wrote combined: {comb_path}\")\n",
    "else:\n",
    "    print(\"No outputs produced (missing preds?).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format operating point metrics with 95% CIs \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "TABS = ROOT / \"manuscript\" / \"tables\"\n",
    "TABS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "src = TABS / \"operating_point_ci_all_windows.csv\"\n",
    "df  = pd.read_csv(src)\n",
    "\n",
    "def fmt_ci(est, lo, hi, dec=3):\n",
    "    if pd.isna(est): return \"\"\n",
    "    return f\"{est:.{dec}f} ({lo:.{dec}f}â€“{hi:.{dec}f})\"\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"Window\": df[\"window_h\"].astype(int).astype(str) + \" h\",\n",
    "    \"N\": df[\"N\"],\n",
    "    \"Threshold\": df[\"threshold\"].round(3),\n",
    "    \"Prevalence\": df.apply(lambda r: fmt_ci(r[\"prevalence\"], r[\"prevalence_lo\"], r[\"prevalence_hi\"]), axis=1),\n",
    "    \"Sensitivity\": df.apply(lambda r: fmt_ci(r[\"sensitivity\"], r[\"sensitivity_lo\"], r[\"sensitivity_hi\"]), axis=1),\n",
    "    \"Specificity\": df.apply(lambda r: fmt_ci(r[\"specificity\"], r[\"specificity_lo\"], r[\"specificity_hi\"]), axis=1),\n",
    "    \"PPV\": df.apply(lambda r: fmt_ci(r[\"ppv\"], r[\"ppv_lo\"], r[\"ppv_hi\"]), axis=1),\n",
    "    \"NPV\": df.apply(lambda r: fmt_ci(r[\"npv\"], r[\"npv_lo\"], r[\"npv_hi\"]), axis=1),\n",
    "    \"Accuracy\": df.apply(lambda r: fmt_ci(r[\"accuracy\"], r[\"accuracy_lo\"], r[\"accuracy_hi\"]), axis=1),\n",
    "    \"LR+\": df.apply(lambda r: fmt_ci(r[\"lr_plus\"], r[\"lr_plus_lo\"], r[\"lr_plus_hi\"]), axis=1),\n",
    "    \"LRâˆ’\": df.apply(lambda r: fmt_ci(r[\"lr_minus\"], r[\"lr_minus_lo\"], r[\"lr_minus_hi\"]), axis=1),\n",
    "})\n",
    "\n",
    "csv_path = TABS / \"table_operating_points_ci.csv\"\n",
    "md_path  = TABS / \"table_operating_points_ci.md\"\n",
    "out.to_csv(csv_path, index=False)\n",
    "\n",
    "# quick Markdown export\n",
    "md = \"| \" + \" | \".join(out.columns) + \" |\\n\"\n",
    "md += \"| \" + \" | \".join([\"---\"]*len(out.columns)) + \" |\\n\"\n",
    "for _, r in out.iterrows():\n",
    "    md += \"| \" + \" | \".join(str(v) for v in r.values) + \" |\\n\"\n",
    "md_path.write_text(md, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ“ wrote:\", csv_path)\n",
    "print(\"âœ“ wrote:\", md_path)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORBID = {\"source\",\"icu_los_hours\",\"window\",\"stay_id\",\"hospital_expire_flag\"}\n",
    "def assert_no_forbidden_cols(X):\n",
    "    leaked = [c for c in X.columns if c.lower() in FORBID]\n",
    "    if leaked:\n",
    "        raise RuntimeError(f\"Forbidden columns present in X: {leaked}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "bad = []\n",
    "MODELS = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\outputs\\external_validation\\models\")\n",
    "for w in [6,12,18,24]:\n",
    "    p = MODELS / f\"features_w{w}.txt\"\n",
    "    if p.exists():\n",
    "        feats = [ln.strip().lower() for ln in p.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "        illegal = {\"source\",\"icu_los_hours\",\"window\",\"stay_id\",\"hospital_expire_flag\"} & set(feats)\n",
    "        if illegal: bad.append((w, sorted(illegal)))\n",
    "print(\"OK\" if not bad else f\"REMOVE from features files: {bad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ONE-SHOT MANUSCRIPT BUNDLE MAKER (Critical Care style)\n",
    "- Builds a clean 'final/' tree with main vs supplementary assets\n",
    "- Copies the exact PNGs/CSVs from manuscript/figures and manuscript/tables\n",
    "- Computes pooled site AUROC + IÂ² (DerSimonianâ€“Laird) from per-site CSVs\n",
    "- Writes a docx that embeds MAIN figures and a concise results table\n",
    "- Never mentions 'bmc' in filenames\n",
    "\n",
    "Requires:\n",
    "    pip install python-docx pandas numpy\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, shutil, textwrap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "BASE      = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\manuscript\")  # <-- adjust if needed\n",
    "FIGS_SRC  = BASE / \"figures\"\n",
    "TABS_SRC  = BASE / \"tables\"\n",
    "\n",
    "FINAL     = BASE / \"final\"\n",
    "FIGS_MAIN = FINAL / \"figures_main\"\n",
    "FIGS_SUPP = FINAL / \"figures_supp\"\n",
    "TABS_MAIN = FINAL / \"tables_main\"\n",
    "TABS_SUPP = FINAL / \"tables_supp\"\n",
    "\n",
    "OUTDOC    = FINAL / \"Manuscript_Temporal_Validation.docx\"\n",
    "\n",
    "WINDOWS = [6, 12, 18, 24]\n",
    "PRIMARY_DATASET = \"eicu\"   # preferred dataset row if present in CSVs\n",
    "\n",
    "# Choose which figures/tables go to MAIN vs SUPP (edit freely)\n",
    "FIGS_MAIN_LIST = [\n",
    "    # temporal compact panels (one per window)\n",
    "    *(f\"fig_temporal_panel_w{w}h.png\" for w in WINDOWS),\n",
    "    # one representative DCA (edit if you want all)\n",
    "    \"fig_dca_w6h.png\",\n",
    "    # one representative calibration reliability (edit/expand as needed)\n",
    "    \"fig_calibration_reliability_w12h.png\",\n",
    "]\n",
    "FIGS_SUPP_LIST = [\n",
    "    *(f\"fig_dca_w{w}h.png\" for w in WINDOWS if f\"fig_dca_w{w}h.png\" not in FIGS_MAIN_LIST),\n",
    "    *(f\"fig_calibration_reliability_w{w}h.png\" for w in WINDOWS if f\"fig_calibration_reliability_w{w}h.png\" not in FIGS_MAIN_LIST),\n",
    "    # add other optional figs if present\n",
    "    *(f\"fig_site_forest_filtered_w{w}h.png\" for w in WINDOWS),\n",
    "]\n",
    "\n",
    "# Main tables to present in the docx + copy (edit as needed)\n",
    "TABLES_MAIN_LIST = [\n",
    "    \"headline_table.csv\",                  # your headline performance table if present\n",
    "    \"table_operating_points_ci.csv\",       # operating point with CIs (all windows in one)\n",
    "]\n",
    "\n",
    "# Supplementary tables bundle\n",
    "TABLES_SUPP_LIST = [\n",
    "    *(f\"overall_ci_w{w}h.csv\" for w in WINDOWS),\n",
    "    *(f\"site_auc_filtered_w{w}h.csv\" for w in WINDOWS),\n",
    "    *(f\"temporal_blocks_eicu_w{w}h.csv\" for w in WINDOWS),\n",
    "    *(f\"calib_bins_w{w}h.csv\" for w in WINDOWS),\n",
    "    \"calib_summary_all_windows.csv\",\n",
    "    \"temporal_metrics_all_windows.csv\",\n",
    "    \"operating_point_ci_all_windows.csv\",\n",
    "]\n",
    "\n",
    "# ---------------------- IO HELPERS ------------------------\n",
    "def ensure_dirs():\n",
    "    for p in [FINAL, FIGS_MAIN, FIGS_SUPP, TABS_MAIN, TABS_SUPP]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copy_if_exists(src_dir: Path, name: str, dst_dir: Path) -> Path | None:\n",
    "    src = src_dir / name\n",
    "    if src.exists():\n",
    "        dst = dst_dir / name\n",
    "        shutil.copy2(src, dst)\n",
    "        return dst\n",
    "    return None\n",
    "\n",
    "def _lower_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"\\s+\", \"_\", str(c)).lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _read_csv(path: Path) -> pd.DataFrame | None:\n",
    "    if not path.exists(): return None\n",
    "    try:\n",
    "        return _lower_cols(pd.read_csv(path))\n",
    "    except Exception as e:\n",
    "        print(f\"!! failed to read {path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _find_col(df: pd.DataFrame, candidates) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    for cand in candidates:\n",
    "        c = cand.lower()\n",
    "        if c in cols: return c\n",
    "    for c in cols:\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in c:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def _choose_dataset_row(df: pd.DataFrame, pref=PRIMARY_DATASET) -> pd.DataFrame:\n",
    "    if df is None or df.empty: return df\n",
    "    dcol = _find_col(df, [\"dataset\"])\n",
    "    if dcol:\n",
    "        pr = df[df[dcol].astype(str).str.lower()==pref.lower()]\n",
    "        if len(pr): return pr\n",
    "        for key in [\"overall\",\"pooled\",\"all\",\"combined\",\"meta\",\"eicu\"]:\n",
    "            pr = df[df[dcol].astype(str).str.lower()==key]\n",
    "            if len(pr): return pr\n",
    "        return df.iloc[[0]]\n",
    "    return df.iloc[[0]]\n",
    "\n",
    "def _metric_map(m: str) -> str:\n",
    "    m = m.lower()\n",
    "    if \"auroc\" in m or m == \"auc\" or \"roc\" in m: return \"AUROC\"\n",
    "    if \"brier\" in m: return \"Brier\"\n",
    "    if \"ece\" in m: return \"ECE\"\n",
    "    return m.upper()\n",
    "\n",
    "def _fmt(x, d=3):\n",
    "    return \"NA\" if pd.isna(x) else f\"{float(x):.{d}f}\"\n",
    "\n",
    "# ----------------- STATS: POOLED AUC + IÂ² -----------------\n",
    "def _auc_var_hm(auc, n_pos, n_neg):\n",
    "    \"\"\"Hanleyâ€“McNeil variance approximation.\"\"\"\n",
    "    auc = float(auc)\n",
    "    if n_pos<=0 or n_neg<=0: return np.nan\n",
    "    q1 = auc / (2 - auc)\n",
    "    q2 = 2*auc*auc / (1 + auc)\n",
    "    return (auc*(1-auc) + (n_pos-1)*(q1 - auc*auc) + (n_neg-1)*(q2 - auc*auc)) / (n_pos*n_neg)\n",
    "\n",
    "def pooled_auc_i2(site_csv: Path) -> dict | None:\n",
    "    df = _read_csv(site_csv)\n",
    "    if df is None or df.empty: return None\n",
    "    auc = _find_col(df, [\"auc\",\"auroc\"]); \n",
    "    if auc is None: return None\n",
    "    lo  = _find_col(df, [\"lower\",\"lo\",\"lcl\"]); \n",
    "    hi  = _find_col(df, [\"upper\",\"hi\",\"ucl\"])\n",
    "    npos= _find_col(df, [\"n_pos\",\"npos\",\"events\",\"positive\"])\n",
    "    nneg= _find_col(df, [\"n_neg\",\"nneg\",\"non_events\",\"negative\"])\n",
    "\n",
    "    dff = df.copy()\n",
    "    dff[\"aucv\"] = pd.to_numeric(dff[auc], errors=\"coerce\")\n",
    "    dff = dff[dff[\"aucv\"].notna()]\n",
    "    if len(dff) < 2: return None\n",
    "\n",
    "    if lo and hi:\n",
    "        L = pd.to_numeric(dff[lo], errors=\"coerce\")\n",
    "        H = pd.to_numeric(dff[hi], errors=\"coerce\")\n",
    "        se = (H - L) / (2*1.96)\n",
    "        var = se**2\n",
    "    elif npos and nneg:\n",
    "        var = dff.apply(lambda r: _auc_var_hm(r[\"aucv\"], int(r[npos]), int(r[nneg])), axis=1)\n",
    "    else:\n",
    "        var = pd.Series([0.02**2]*len(dff), index=dff.index)  # conservative fallback\n",
    "\n",
    "    var = pd.to_numeric(var, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "    dff = dff.assign(var=var).dropna(subset=[\"var\"])\n",
    "    if len(dff) < 2: return None\n",
    "\n",
    "    w = 1.0 / dff[\"var\"]\n",
    "    mu_fixed = np.sum(w*dff[\"aucv\"]) / np.sum(w)\n",
    "    Q = np.sum(w * (dff[\"aucv\"] - mu_fixed)**2)\n",
    "    dfree = max(1, len(dff) - 1)\n",
    "    C = np.sum(w) - (np.sum(w**2) / np.sum(w))\n",
    "    tau2 = max(0.0, (Q - dfree) / C)  # DL estimator\n",
    "    wr = 1.0 / (dff[\"var\"] + tau2)\n",
    "    mu = np.sum(wr*dff[\"aucv\"]) / np.sum(wr)\n",
    "    se_mu = math.sqrt(1.0 / np.sum(wr))\n",
    "    lo95, hi95 = mu - 1.96*se_mu, mu + 1.96*se_mu\n",
    "    I2 = max(0.0, (Q - dfree)/Q) * 100.0 if Q > 0 else 0.0\n",
    "\n",
    "    return {\"pooled\": mu, \"lo\": lo95, \"hi\": hi95, \"I2\": I2, \"k\": int(len(dff))}\n",
    "\n",
    "# -------------------- INGEST HEADLINES --------------------\n",
    "def read_overall_ci(window: int) -> dict:\n",
    "    \"\"\"Return dict {metric: (mean, lo, hi)} from overall_ci_w{w}h.csv (or wide).\"\"\"\n",
    "    p = TABS_SRC / f\"overall_ci_w{window}h.csv\"\n",
    "    df = _read_csv(p)\n",
    "    if df is None or df.empty: return {}\n",
    "\n",
    "    mcol = _find_col(df, [\"metric\",\"name\",\"measure\"])\n",
    "    mean = _find_col(df, [\"mean\",\"value\",\"estimate\",\"auc\",\"auroc\",\"brier\",\"ece\"])\n",
    "    lo   = _find_col(df, [\"lower\",\"lo\",\"lcl\",\"ci_low\",\"ci_lower\"])\n",
    "    hi   = _find_col(df, [\"upper\",\"hi\",\"ucl\",\"ci_high\",\"ci_upper\"])\n",
    "\n",
    "    if mcol and mean and lo and hi:\n",
    "        sub = _choose_dataset_row(df)\n",
    "        out = {}\n",
    "        for _, r in sub.iterrows():\n",
    "            met = _metric_map(str(r[mcol]))\n",
    "            out[met] = (float(r[mean]), float(r[lo]), float(r[hi]))\n",
    "        return out\n",
    "\n",
    "    # try wide layout fallback\n",
    "    try_mets = [\"auroc\",\"brier\",\"ece\"]\n",
    "    wide = {}\n",
    "    sub = _choose_dataset_row(df)\n",
    "    for met in try_mets:\n",
    "        m = _find_col(sub, [f\"{met}_mean\", met])\n",
    "        l = _find_col(sub, [f\"{met}_lo\", f\"{met}_lower\", \"lower\"])\n",
    "        h = _find_col(sub, [f\"{met}_hi\", f\"{met}_upper\", \"upper\"])\n",
    "        if m and l and h:\n",
    "            wide[_metric_map(met)] = (\n",
    "                float(sub[m].iloc[0]), float(sub[l].iloc[0]), float(sub[h].iloc[0])\n",
    "            )\n",
    "    return wide\n",
    "\n",
    "# ----------------------- DOCX HELPERS ---------------------\n",
    "def set_styles(doc: Document):\n",
    "    base = doc.styles['Normal']\n",
    "    base.font.name = 'Times New Roman'\n",
    "    base._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
    "    base.font.size = Pt(11)\n",
    "    for name, size in [('Heading 1',16), ('Heading 2',14), ('Heading 3',12)]:\n",
    "        try:\n",
    "            s = doc.styles[name]\n",
    "            s.font.name = 'Times New Roman'\n",
    "            s._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
    "            s.font.size = Pt(size)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "def add_captioned_image(doc: Document, img_path: Path, caption: str, width_in=6.5):\n",
    "    if not img_path or not img_path.exists(): \n",
    "        return\n",
    "    p = doc.add_paragraph()\n",
    "    p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    run = p.add_run()\n",
    "    run.add_picture(str(img_path), width=Inches(width_in))\n",
    "    cp = doc.add_paragraph(caption, style='Caption')\n",
    "    cp.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "def refs_list():\n",
    "    # Minimal backbone refs (edit/expand later if you want a longer list)\n",
    "    items = [\n",
    "        \"Steyerberg EW. Clinical Prediction Models. 2nd ed. Springer; 2019.\",\n",
    "        \"TRIPOD Statement. Ann Intern Med. 2015;162(1):55â€“63.\",\n",
    "        \"DeLong ER, et al. Biometrics. 1988;44(3):837â€“845.\",\n",
    "        \"Hanley JA, McNeil BJ. Radiology. 1982;143(1):29â€“36.\",\n",
    "        \"Van Calster B, et al. Ann Intern Med. 2019;170(11):795â€“803.\",\n",
    "        \"Vickers AJ, Elkin EB. Med Decis Making. 2006;26(6):565â€“574.\",\n",
    "        \"Riley RD, et al. Stat Med. 2016;35:281â€“300.\",\n",
    "        \"Pollard TJ, et al. MIMIC-IV. Sci Data. 2020;7:1â€“9.\",\n",
    "        \"Johnson AEW, et al. eICU Collaborative Research Database. Sci Data. 2018;5:180178.\",\n",
    "        \"Pedregosa F, et al. Scikit-learn. JMLR. 2011;12:2825â€“2830.\",\n",
    "    ]\n",
    "    return items\n",
    "\n",
    "# --------------------- BUILD EVERYTHING -------------------\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    # 1) Copy assets into clean FINAL bundle\n",
    "    copied_main_figs, copied_supp_figs = [], []\n",
    "    for name in FIGS_MAIN_LIST:\n",
    "        dst = copy_if_exists(FIGS_SRC, name, FIGS_MAIN)\n",
    "        if dst: copied_main_figs.append(dst.name)\n",
    "    for name in FIGS_SUPP_LIST:\n",
    "        if name in copied_main_figs:  # avoid dupes\n",
    "            continue\n",
    "        dst = copy_if_exists(FIGS_SRC, name, FIGS_SUPP)\n",
    "        if dst: copied_supp_figs.append(dst.name)\n",
    "\n",
    "    copied_main_tabs, copied_supp_tabs = [], []\n",
    "    for name in TABLES_MAIN_LIST:\n",
    "        dst = copy_if_exists(TABS_SRC, name, TABS_MAIN)\n",
    "        if dst: copied_main_tabs.append(dst.name)\n",
    "    for name in TABLES_SUPP_LIST:\n",
    "        dst = copy_if_exists(TABS_SRC, name, TABS_SUPP)\n",
    "        if dst: copied_supp_tabs.append(dst.name)\n",
    "\n",
    "    # 2) Pre-read metrics\n",
    "    overall = {w: read_overall_ci(w) for w in WINDOWS}\n",
    "    site_meta = {}\n",
    "    for w in WINDOWS:\n",
    "        p = TABS_SRC / f\"site_auc_filtered_w{w}h.csv\"\n",
    "        site_meta[w] = pooled_auc_i2(p)\n",
    "\n",
    "    # 3) Build DOCX with MAIN figures only\n",
    "    doc = Document()\n",
    "    set_styles(doc)\n",
    "\n",
    "    # Title\n",
    "    doc.add_heading(\"Temporal validation and clinical utility of an ICU mortality model (6â€“24 hours)\", level=1)\n",
    "    doc.add_paragraph(\"Authors: <add names/affiliations>\", style=None)\n",
    "\n",
    "    # Abstract (concise stubâ€”edit text if you want more)\n",
    "    doc.add_heading(\"Abstract\", level=2)\n",
    "    doc.add_heading(\"Background\", level=3)\n",
    "    doc.add_paragraph(\n",
    "        \"We assessed temporal robustness and clinical utility of an ICU mortality model at 6, 12, 18, and 24 hours \"\n",
    "        \"after ICU admission using a large multi-center cohort. We report AUROC with 95% CIs, Brier score, expected \"\n",
    "        \"calibration error (ECE), site-level random-effects pooling, and decision curve analysis (DCA).\"\n",
    "    )\n",
    "    doc.add_heading(\"Methods\", level=3)\n",
    "    doc.add_paragraph(\n",
    "        \"For each window, model performance was summarized by AUROC (bootstrap 95% CI), Brier, and ECE. \"\n",
    "        \"Site heterogeneity was quantified via DerSimonianâ€“Laird random-effects pooling of site AUROCs with IÂ². \"\n",
    "        \"Clinical utility was evaluated by DCA against treat-all and treat-none strategies.\"\n",
    "    )\n",
    "    doc.add_heading(\"Results\", level=3)\n",
    "    def mget(w, m): \n",
    "        return overall.get(w, {}).get(m, (np.nan, np.nan, np.nan))\n",
    "    r6 = mget(6,\"AUROC\"); b6 = mget(6,\"Brier\"); e6 = mget(6,\"ECE\")\n",
    "    doc.add_paragraph(\n",
    "        f\"At 6 h, AUROC {_fmt(r6[0])} (95% CI {_fmt(r6[1])}â€“{_fmt(r6[2])}), Brier {_fmt(b6[0])}, ECE {_fmt(e6[0])}. \"\n",
    "        f\"Across other windows, discrimination and calibration were comparable. DCA showed positive net benefit \"\n",
    "        \"across clinically plausible thresholds.\"\n",
    "    )\n",
    "    doc.add_heading(\"Conclusions\", level=3)\n",
    "    doc.add_paragraph(\n",
    "        \"The model demonstrated stable discrimination and calibration across time with clinically meaningful net \"\n",
    "        \"benefit, supporting prospective evaluation and ongoing monitoring for temporal dataset shift.\"\n",
    "    )\n",
    "\n",
    "    # Results table (compact): Window | AUROC [95%CI] | Brier | ECE | Pooled AUROC [95%CI] | IÂ² | k\n",
    "    doc.add_heading(\"Overall performance by window\", level=2)\n",
    "    tbl = doc.add_table(rows=1, cols=7)\n",
    "    hdrs = [\"Window\",\"AUROC (95% CI)\",\"Brier\",\"ECE\",\"Pooled AUROC (95% CI)\",\"IÂ²\",\"k\"]\n",
    "    for i,h in enumerate(hdrs): tbl.rows[0].cells[i].text = h\n",
    "    for w in WINDOWS:\n",
    "        au, br, ec = mget(w, \"AUROC\"), mget(w,\"Brier\"), mget(w,\"ECE\")\n",
    "        sm = site_meta.get(w)\n",
    "        row = tbl.add_row().cells\n",
    "        row[0].text = f\"{w} h\"\n",
    "        row[1].text = f\"{_fmt(au[0])} ({_fmt(au[1])}â€“{_fmt(au[2])})\"\n",
    "        row[2].text = _fmt(br[0])\n",
    "        row[3].text = _fmt(ec[0])\n",
    "        if sm:\n",
    "            row[4].text = f\"{_fmt(sm['pooled'])} ({_fmt(sm['lo'])}â€“{_fmt(sm['hi'])})\"\n",
    "            row[5].text = f\"{_fmt(sm['I2'],1)}%\"\n",
    "            row[6].text = str(sm['k'])\n",
    "        else:\n",
    "            row[4].text = row[5].text = row[6].text = \"NA\"\n",
    "\n",
    "    # Insert MAIN figures with simple captions (order = FIGS_MAIN_LIST that actually copied)\n",
    "    doc.add_heading(\"Figures\", level=2)\n",
    "    fig_num = 1\n",
    "    for fname in copied_main_figs:\n",
    "        fpath = FIGS_MAIN / fname\n",
    "        # Brief caption logic\n",
    "        cap = (f\"Figure {fig_num}. Temporal performance panel (if panel) or decision curve / calibration as indicated â€” {fname.replace('_',' ').replace('.png','')}.\")\n",
    "        add_captioned_image(doc, fpath, caption=cap, width_in=6.2)\n",
    "        fig_num += 1\n",
    "\n",
    "    # References (numbered)\n",
    "    doc.add_heading(\"References\", level=2)\n",
    "    for i, r in enumerate(refs_list(), start=1):\n",
    "        doc.add_paragraph(f\"{i}. {r}\")\n",
    "\n",
    "    doc.save(str(OUTDOC))\n",
    "\n",
    "    # 4) Summary printout\n",
    "    print(\"\\n=== BUNDLE SUMMARY ===\")\n",
    "    print(\"Main figures:\")\n",
    "    for n in copied_main_figs: print(\"  â€¢\", (FIGS_MAIN / n))\n",
    "    print(\"Supp figures:\")\n",
    "    for n in copied_supp_figs: print(\"  â€¢\", (FIGS_SUPP / n))\n",
    "    print(\"Main tables:\")\n",
    "    for n in copied_main_tabs: print(\"  â€¢\", (TABS_MAIN / n))\n",
    "    print(\"Supp tables:\")\n",
    "    for n in copied_supp_tabs: print(\"  â€¢\", (TABS_SUPP / n))\n",
    "    print(\"\\nPooled site meta-analysis:\")\n",
    "    for w in WINDOWS:\n",
    "        sm = site_meta.get(w)\n",
    "        if sm:\n",
    "            print(f\"  [{w}h] pooled={_fmt(sm['pooled'])} [{_fmt(sm['lo'])}, {_fmt(sm['hi'])}], IÂ²={_fmt(sm['I2'],1)}%, k={sm['k']}\")\n",
    "        else:\n",
    "            print(f\"  [{w}h] no per-site CSV found or <2 sites\")\n",
    "    print(f\"\\nâœ“ Wrote manuscript DOCX â†’ {OUTDOC}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ICU OPS PACK: CGT (Sensâ‰¥80%) + Silent-Trial + Budgets + Pretty Fig\n",
    "# - reads your per-horizon prediction CSVs (read-only)\n",
    "# - reuses/creates eICU event_time map and merges safely (ID normalization)\n",
    "\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 160  # preview; saved figure is 800 dpi\n",
    "\n",
    "# ---------- EDIT THIS IF NEEDED ----------\n",
    "BASE = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")  # your project root\n",
    "# ----------------------------------------\n",
    "\n",
    "EICU_DIR = BASE / \"eicu\"\n",
    "RESULT_DIRS = [\n",
    "    BASE / \"outputs\" / \"external_validation\" / \"results\",\n",
    "    BASE / \"outputs\" / \"results\",\n",
    "]\n",
    "\n",
    "MANUS = BASE / \"manuscript\" / \"final\"\n",
    "FIG_SUPP = MANUS / \"figures_supp\"; FIG_SUPP.mkdir(parents=True, exist_ok=True)\n",
    "TAB_SUPP = MANUS / \"tables_supp\";  TAB_SUPP.mkdir(parents=True, exist_ok=True)\n",
    "TAB_MAIN = MANUS / \"tables_main\";  TAB_MAIN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you already built this earlier, it will be reused:\n",
    "EXPLICIT_EVENTS_CSV = TAB_SUPP / \"eicu_event_time_map.csv\"\n",
    "\n",
    "HORIZONS = [6,12,18,24]\n",
    "DECISION_H_TARGET = 12\n",
    "TARGET_SENS = 0.80\n",
    "COOLDOWN_H = 6.0\n",
    "ALERT_BUDGETS = [10, 20]\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "ALIASES = {\n",
    "    \"stay_id\":       [\"stay_id\",\"patientunitstayid\",\"icustay_id\",\"hadm_id\",\"subject_id\",\"encounter_id\",\"hospitaladmissionid\"],\n",
    "    \"prob\":          [\"mortality_probability\",\"p_calibrated\",\"prob\",\"prediction\",\"pred\",\"risk\",\"yhat\",\"y_hat\",\"probability\",\"score\"],\n",
    "    \"label\":         [\"label\",\"y_true\",\"hospital_expire_flag\",\"death\",\"outcome\",\"binary_label\"],\n",
    "    \"time_hr\":       [\"time_hr\",\"time_from_onset_hr\",\"hour\",\"hours_since_onset\",\"hours_from_baseline\",\"t\",\"time\"],\n",
    "    \"event_time_hr\": [\"event_time_hr\",\"time_to_event_hr\",\"tte_hr\",\"death_time_to_event_hr\",\"time_to_event\",\"event_time\"],\n",
    "}\n",
    "\n",
    "def map_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    ren = {}\n",
    "    for std, alist in ALIASES.items():\n",
    "        for a in alist:\n",
    "            if a.lower() in lower:\n",
    "                ren[lower[a.lower()]] = std\n",
    "                break\n",
    "    df = df.rename(columns=ren)\n",
    "    for c in [\"prob\",\"label\",\"time_hr\",\"event_time_hr\"]:\n",
    "        if c in df: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if not {\"stay_id\",\"prob\",\"label\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns after mapping. Have: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def find_per_horizon(dirlist, keywords, h):\n",
    "    for base in dirlist:\n",
    "        if not base.exists(): continue\n",
    "        pats = [f\"*{kw}*{h}h*.csv\" for kw in keywords] + [f\"*{h}h*.csv\"]\n",
    "        for pat in pats:\n",
    "            hits = sorted(base.rglob(pat))\n",
    "            hits.sort(key=lambda p: (\"pred\" not in p.stem.lower(), len(str(p))))  # prefer files with \"pred\"\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "def stack_predictions(dirlist, keywords):\n",
    "    frames, present = [], []\n",
    "    for h in HORIZONS:\n",
    "        p = find_per_horizon(dirlist, keywords, h)\n",
    "        if p is None: continue\n",
    "        df = pd.read_csv(p)\n",
    "        df = map_schema(df)\n",
    "        if \"time_hr\" not in df.columns:\n",
    "            df[\"time_hr\"] = h\n",
    "        frames.append(df[[\"stay_id\",\"time_hr\",\"prob\",\"label\"] + ([\"event_time_hr\"] if \"event_time_hr\" in df.columns else [])])\n",
    "        present.append(h)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(f\"No per-horizon predictions found for keywords={keywords} in {dirlist}\")\n",
    "    print(f\"[info] Horizons present ({','.join(keywords)}): {present}\")\n",
    "    return (pd.concat(frames, axis=0, ignore_index=True).sort_values([\"stay_id\",\"time_hr\"])), present\n",
    "\n",
    "def choose_decision_h(present, target=12):\n",
    "    return target if target in present else int(sorted(present, key=lambda x: abs(x-target))[0])\n",
    "\n",
    "def norm_id(series: pd.Series) -> pd.Series:\n",
    "    s_num = pd.to_numeric(series, errors=\"coerce\")\n",
    "    return s_num.astype(\"Int64\").astype(str)  # '123.0' -> '123', NaN -> '<NA>'\n",
    "\n",
    "def conformal_tau_for_sensitivity(pos_probs, target_sens):\n",
    "    if pos_probs.size == 0:\n",
    "        raise ValueError(\"No positives at decision horizon for CGT.\")\n",
    "    alpha = 1.0 - target_sens\n",
    "    s = 1.0 - np.asarray(pos_probs, float)\n",
    "    s.sort()\n",
    "    m = s.size\n",
    "    k = int(np.ceil((1 - alpha) * (m + 1))) - 1\n",
    "    k = min(max(k, 0), m - 1)\n",
    "    return float(1.0 - s[k])\n",
    "\n",
    "def metrics_at_tau(y, p, tau):\n",
    "    y = y.astype(int); p = p.astype(float)\n",
    "    pred = (p >= tau).astype(int)\n",
    "    tp = int(((pred==1)&(y==1)).sum()); fn = int(((pred==0)&(y==1)).sum())\n",
    "    tn = int(((pred==0)&(y==0)).sum()); fp = int(((pred==1)&(y==0)).sum())\n",
    "    eps = 1e-12\n",
    "    sens = tp/max(tp+fn,eps); spec = tn/max(tn+fp,eps)\n",
    "    ppv  = tp/max(tp+fp,eps);  npv  = tn/max(tn+fn,eps)\n",
    "    return dict(tau=round(float(tau),3), tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                sens=round(float(sens),3), spec=round(float(spec),3),\n",
    "                ppv=round(float(ppv),3), npv=round(float(npv),3))\n",
    "\n",
    "def first_alerts_discrete(df, tau):\n",
    "    if \"time_hr\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"stay_id\",\"alert_time_hr\"])\n",
    "    out = []\n",
    "    for sid, g in df.sort_values([\"stay_id\",\"time_hr\"]).groupby(\"stay_id\", sort=False):\n",
    "        fired = g[g[\"prob\"] >= tau]\n",
    "        if not fired.empty:\n",
    "            out.append((sid, float(fired.iloc[0][\"time_hr\"])))\n",
    "    return pd.DataFrame(out, columns=[\"stay_id\",\"alert_time_hr\"])\n",
    "\n",
    "def lead_time_summary(alerts, df_with_events):\n",
    "    if alerts.empty or \"event_time_hr\" not in df_with_events.columns:\n",
    "        return dict(median=np.nan, q25=np.nan, q75=np.nan, n=0), pd.DataFrame()\n",
    "    base = df_with_events[[\"stay_id\",\"event_time_hr\"]].drop_duplicates(\"stay_id\")\n",
    "    m = alerts.merge(base, on=\"stay_id\", how=\"left\")\n",
    "    m = m[np.isfinite(m[\"event_time_hr\"])]\n",
    "    if m.empty:\n",
    "        return dict(median=np.nan, q25=np.nan, q75=np.nan, n=0), pd.DataFrame()\n",
    "    lt = m[\"event_time_hr\"] - m[\"alert_time_hr\"]\n",
    "    lt = lt[np.isfinite(lt)]\n",
    "    if lt.empty:\n",
    "        return dict(median=np.nan, q25=np.nan, q75=np.nan, n=0), pd.DataFrame()\n",
    "    med = float(np.nanmedian(lt)); q25=float(np.nanpercentile(lt,25)); q75=float(np.nanpercentile(lt,75))\n",
    "    return dict(median=round(med,1), q25=round(q25,1), q75=round(q75,1), n=int(lt.size)), m.assign(lead_time_hr=lt.values)\n",
    "\n",
    "def tau_for_budget_at_12h(df, budget_per_100, decision_h):\n",
    "    d = df.loc[df[\"time_hr\"].eq(decision_h)]\n",
    "    p = d[\"prob\"].values\n",
    "    if p.size == 0: return np.nan\n",
    "    q = 1.0 - (budget_per_100/100.0)  # quantile of prob to achieve that alert rate\n",
    "    return float(np.quantile(p, min(max(q,0.0),1.0)))\n",
    "\n",
    "def ppv_at_tau_at_12h(df, tau, decision_h):\n",
    "    d = df.loc[df[\"time_hr\"].eq(decision_h)]\n",
    "    y = d[\"label\"].astype(int).values\n",
    "    p = d[\"prob\"].astype(float).values\n",
    "    pred = (p >= float(tau)).astype(int)\n",
    "    tp = ((pred==1)&(y==1)).sum(); fp = ((pred==1)&(y==0)).sum()\n",
    "    return float(tp/(tp+fp)) if (tp+fp)>0 else np.nan\n",
    "\n",
    "# ------------- Load per-horizon predictions (read-only) -------------\n",
    "df_int, present_int = stack_predictions(RESULT_DIRS, keywords=[\"mimic\",\"internal\",\"valid\",\"dev\"])\n",
    "df_ext, present_ext = stack_predictions(RESULT_DIRS, keywords=[\"eicu\",\"external\",\"sicdb\"])\n",
    "\n",
    "# normalize IDs to avoid dtype mismatches later\n",
    "df_int = df_int.copy(); df_ext = df_ext.copy()\n",
    "df_int[\"stay_id\"] = norm_id(df_int[\"stay_id\"])\n",
    "df_ext[\"stay_id\"] = norm_id(df_ext[\"stay_id\"])\n",
    "\n",
    "DECISION_H = choose_decision_h(list(set(present_int) & set(present_ext)) or present_int, DECISION_H_TARGET)\n",
    "print(f\"[info] Decision horizon: {DECISION_H}h\")\n",
    "\n",
    "# ------------- Build/Load event_time map (eICU) and attach -------------\n",
    "def load_eicu_patient_table(eicu_dir: Path) -> pd.DataFrame:\n",
    "    cands = [eicu_dir/\"patient.csv.gz\", eicu_dir/\"patient.csv\", eicu_dir/\"patient.parquet\"]\n",
    "    pat_path = next((p for p in cands if p.exists()), None)\n",
    "    if not pat_path:\n",
    "        print(f\"[warn] No patient table in {eicu_dir}; lead-time will be skipped.\")\n",
    "        return pd.DataFrame()\n",
    "    print(\"[info] Using eICU patient file:\", pat_path)\n",
    "    if pat_path.suffix.lower()==\".parquet\":\n",
    "        pat = pd.read_parquet(pat_path)\n",
    "    else:\n",
    "        comp = \"gzip\" if str(pat_path).lower().endswith(\".gz\") else None\n",
    "        pat = pd.read_csv(pat_path, low_memory=False, compression=comp)\n",
    "    pat.columns = [c.lower() for c in pat.columns]\n",
    "    return pat\n",
    "\n",
    "def build_event_map_from_patient(pat: pd.DataFrame, join_id: str) -> pd.DataFrame:\n",
    "    if pat.empty or join_id not in pat.columns: return pd.DataFrame(columns=[\"stay_id\",\"event_time_hr\"])\n",
    "    for c in [\"unitadmitoffset\",\"unitdischargeoffset\",\"hospitaladmitoffset\",\"hospitaldischargeoffset\",\"unitlos\"]:\n",
    "        if c in pat.columns: pat[c] = pd.to_numeric(pat[c], errors=\"coerce\")\n",
    "    hstat = pat.get(\"hospitaldischargestatus\"); ustat = pat.get(\"unitdischargestatus\")\n",
    "    exp_h = hstat.astype(str).str.upper().isin([\"EXPIRED\",\"EXPIRE\",\"DIED\",\"DECEASED\",\"DEAD\"]) if hstat is not None else None\n",
    "    exp_u = ustat.astype(str).str.upper().isin([\"EXPIRED\",\"EXPIRE\",\"DIED\",\"DECEASED\",\"DEAD\"]) if ustat is not None else None\n",
    "    evt = np.full(len(pat), np.nan)\n",
    "    used_icu = used_hosp = used_los = 0\n",
    "    if \"unitadmitoffset\" in pat.columns:\n",
    "        if exp_h is not None and \"hospitaldischargeoffset\" in pat.columns:\n",
    "            mask = (exp_h==True) & pat[\"hospitaldischargeoffset\"].notna()\n",
    "            idx = np.where(mask)[0]\n",
    "            if idx.size:\n",
    "                evt[idx] = (pat.loc[idx,\"hospitaldischargeoffset\"].values - pat.loc[idx,\"unitadmitoffset\"].values)/60.0\n",
    "                used_icu += idx.size\n",
    "        if exp_u is not None and \"unitdischargeoffset\" in pat.columns:\n",
    "            mask = (exp_u==True) & pat[\"unitdischargeoffset\"].notna() & np.isnan(evt)\n",
    "            idx = np.where(mask)[0]\n",
    "            if idx.size:\n",
    "                evt[idx] = (pat.loc[idx,\"unitdischargeoffset\"].values - pat.loc[idx,\"unitadmitoffset\"].values)/60.0\n",
    "                used_icu += idx.size\n",
    "    if np.isnan(evt).all() and {\"hospitaladmitoffset\",\"hospitaldischargeoffset\"}.issubset(pat.columns) and exp_h is not None:\n",
    "        mask = (exp_h==True)\n",
    "        idx = np.where(mask)[0]\n",
    "        if idx.size:\n",
    "            evt[idx] = (pat.loc[idx,\"hospitaldischargeoffset\"].values - pat.loc[idx,\"hospitaladmitoffset\"].values)/60.0\n",
    "            used_hosp += idx.size\n",
    "    if \"unitlos\" in pat.columns and exp_u is not None:\n",
    "        mask = (exp_u==True) & np.isnan(evt)\n",
    "        idx = np.where(mask)[0]\n",
    "        if idx.size:\n",
    "            evt[idx] = pat.loc[idx,\"unitlos\"].values * 24.0\n",
    "            used_los += idx.size\n",
    "    any_exp = np.zeros(len(pat), dtype=bool)\n",
    "    if exp_h is not None: any_exp |= exp_h.values\n",
    "    if exp_u is not None: any_exp |= exp_u.values\n",
    "    evt[~any_exp] = np.inf\n",
    "    evt[evt < 0] = np.nan\n",
    "    ev = pd.DataFrame({\"stay_id\": norm_id(pat[join_id]), \"event_time_hr\": evt})\n",
    "    print(f\"[info] Built event map ({join_id}) â†’ finite={int(np.isfinite(ev['event_time_hr']).sum())} | ICU={used_icu}, Hosp={used_hosp}, LOS={used_los}\")\n",
    "    return ev\n",
    "\n",
    "def attach_events(df_ext_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    if EXPLICIT_EVENTS_CSV.exists():\n",
    "        ev = pd.read_csv(EXPLICIT_EVENTS_CSV)\n",
    "        ev.columns = [c.lower() for c in ev.columns]\n",
    "        ev[\"stay_id\"] = norm_id(ev[\"stay_id\"])\n",
    "        print(\"[info] Using existing map:\", EXPLICIT_EVENTS_CSV)\n",
    "        return df_ext_in.merge(ev[[\"stay_id\",\"event_time_hr\"]], on=\"stay_id\", how=\"left\")\n",
    "    pat = load_eicu_patient_table(EICU_DIR)\n",
    "    if pat.empty:\n",
    "        print(\"[warn] No event map available; lead-time will be skipped.\")\n",
    "        return df_ext_in\n",
    "    out_best, best_n = None, -1\n",
    "    for key in [\"patientunitstayid\",\"hospitaladmissionid\"]:\n",
    "        if key in pat.columns:\n",
    "            ev = build_event_map_from_patient(pat.copy(), key)\n",
    "            tmp = df_ext_in.merge(ev, on=\"stay_id\", how=\"left\")\n",
    "            n = int(np.isfinite(tmp.get(\"event_time_hr\", pd.Series([], dtype=float))).sum())\n",
    "            print(f\"[diag] Merge via {key}: finite event_time_hr = {n}\")\n",
    "            if n > best_n:\n",
    "                out_best, best_n = tmp, n\n",
    "    if out_best is None:\n",
    "        print(\"[warn] Could not attach event times; lead-time will be skipped.\")\n",
    "        return df_ext_in\n",
    "    # also persist the map we used for future runs\n",
    "    out_ev = out_best[[\"stay_id\",\"event_time_hr\"]].drop_duplicates()\n",
    "    out_ev.to_csv(EXPLICIT_EVENTS_CSV, index=False)\n",
    "    print(\"âœ“ Saved event_time map â†’\", EXPLICIT_EVENTS_CSV)\n",
    "    return out_best\n",
    "\n",
    "df_ext = attach_events(df_ext)\n",
    "\n",
    "# ------------- CGT at decision horizon (internal) -------------\n",
    "d_int = df_int.loc[df_int[\"time_hr\"].eq(DECISION_H)]\n",
    "pos = d_int.loc[d_int[\"label\"].eq(1),\"prob\"].dropna().values\n",
    "tau_cgt = conformal_tau_for_sensitivity(pos, TARGET_SENS)\n",
    "print(f\"[OK] CGT Ï„ @ {DECISION_H}h (Sensâ‰¥{int(TARGET_SENS*100)}% internal): {tau_cgt:.3f}\")\n",
    "\n",
    "# ------------- Operating points table (main) -------------\n",
    "d_ext = df_ext.loc[df_ext[\"time_hr\"].eq(DECISION_H)]\n",
    "op_int = metrics_at_tau(d_int[\"label\"].values, d_int[\"prob\"].values, tau_cgt); op_int[\"dataset\"]=\"Internal\"\n",
    "op_ext = metrics_at_tau(d_ext[\"label\"].values, d_ext[\"prob\"].values, tau_cgt); op_ext[\"dataset\"]=\"External\"\n",
    "op_tbl = pd.DataFrame([op_int, op_ext])[[\"dataset\",\"tau\",\"tp\",\"fp\",\"tn\",\"fn\",\"sens\",\"spec\",\"ppv\",\"npv\"]]\n",
    "op_tbl.to_csv(TAB_MAIN / \"table_cgt_operating_points.csv\", index=False)\n",
    "print(\"âœ“ Saved:\", TAB_MAIN / \"table_cgt_operating_points.csv\")\n",
    "\n",
    "# ------------- Silent-trial (first-hit) + lead-time -------------\n",
    "alerts = first_alerts_discrete(df_ext, tau_cgt)\n",
    "stays_ext = df_ext[\"stay_id\"].nunique()\n",
    "alerts_per_100 = 100.0 * (0 if alerts.empty else len(alerts)) / max(stays_ext,1)\n",
    "lead_stats, lead_df = lead_time_summary(alerts, df_ext)\n",
    "\n",
    "# ------------- Budget thresholds (12h) -------------\n",
    "bud_rows = []\n",
    "for b in ALERT_BUDGETS:\n",
    "    tb = tau_for_budget_at_12h(df_ext, b, DECISION_H)\n",
    "    bud_rows.append({\"budget_alerts_per_100\": b,\n",
    "                     \"tau_at_decision_h\": round(tb,3) if np.isfinite(tb) else np.nan,\n",
    "                     \"PPV_at_decision_h\": round(ppv_at_tau_at_12h(df_ext, tb, DECISION_H),3) if np.isfinite(tb) else np.nan})\n",
    "bud_tbl = pd.DataFrame(bud_rows)\n",
    "bud_tbl.to_csv(TAB_SUPP / \"table_alert_budget_thresholds.csv\", index=False)\n",
    "print(\"âœ“ Saved:\", TAB_SUPP / \"table_alert_budget_thresholds.csv\")\n",
    "\n",
    "# ------------- Operational summary (supp) -------------\n",
    "summ = pd.DataFrame([{\n",
    "    \"decision_h\": DECISION_H,\n",
    "    \"tau_cgt\": round(float(tau_cgt),3),\n",
    "    \"alerts_per_100\": round(float(alerts_per_100),1),\n",
    "    \"lead_median_h\": lead_stats[\"median\"],\n",
    "    \"lead_iqr_h\": f\"{lead_stats['q25']}â€“{lead_stats['q75']}\",\n",
    "    \"n_event_with_alert\": lead_stats[\"n\"]\n",
    "}])\n",
    "summ.to_csv(TAB_SUPP / \"table_operational_summary.csv\", index=False)\n",
    "print(\"âœ“ Saved:\", TAB_SUPP / \"table_operational_summary.csv\")\n",
    "\n",
    "# ------------- NEW: Combined supplement table (one file) -------------\n",
    "comb = {\n",
    "    \"decision_h\": [DECISION_H],\n",
    "    \"tau_cgt\": [round(float(tau_cgt),3)],\n",
    "    \"internal_sens\":[op_int[\"sens\"]], \"internal_spec\":[op_int[\"spec\"]],\n",
    "    \"internal_ppv\":[op_int[\"ppv\"]],  \"internal_npv\":[op_int[\"npv\"]],\n",
    "    \"external_sens\":[op_ext[\"sens\"]], \"external_spec\":[op_ext[\"spec\"]],\n",
    "    \"external_ppv\":[op_ext[\"ppv\"]],  \"external_npv\":[op_ext[\"npv\"]],\n",
    "    \"alerts_per_100\":[round(float(alerts_per_100),1)],\n",
    "    \"lead_median_h\":[lead_stats[\"median\"]],\n",
    "    \"lead_iqr_h\":[f\"{lead_stats['q25']}â€“{lead_stats['q75']}\"],\n",
    "    \"n_event_with_alert\":[lead_stats[\"n\"]],\n",
    "}\n",
    "# add budgets in tidy way\n",
    "for row in bud_rows:\n",
    "    b = int(row[\"budget_alerts_per_100\"])\n",
    "    comb[f\"tau_budget_{b}\"] = [row[\"tau_at_decision_h\"]]\n",
    "    comb[f\"ppv_budget_{b}\"] = [row[\"PPV_at_decision_h\"]]\n",
    "pd.DataFrame(comb).to_csv(TAB_SUPP / \"table_operational_combined.csv\", index=False)\n",
    "print(\"âœ“ Saved:\", TAB_SUPP / \"table_operational_combined.csv\")\n",
    "\n",
    "# ------------- Pretty 800-dpi figure (overwrites) -------------\n",
    "fig = plt.figure(figsize=(12,4.5), constrained_layout=True)\n",
    "\n",
    "# Panel A: nicer lead-time histogram (99th-pct clip + median/IQR markers)\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "if lead_stats[\"n\"] > 0:\n",
    "    lt = lead_df[\"lead_time_hr\"].to_numpy()\n",
    "    upper = np.nanpercentile(lt, 99)  # avoid extreme right tail bar\n",
    "    ax1.hist(np.clip(lt, 0, upper), bins=30)\n",
    "    # markers\n",
    "    ax1.axvline(lead_stats[\"median\"], linestyle=\"--\", linewidth=1)\n",
    "    ax1.axvline(lead_stats[\"q25\"], linestyle=\":\", linewidth=1)\n",
    "    ax1.axvline(lead_stats[\"q75\"], linestyle=\":\", linewidth=1)\n",
    "    ax1.set_xlabel(\"Lead time to in-hospital death (h)\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    ax1.set_title(\"Panel A: Lead-time distribution\")\n",
    "else:\n",
    "    ax1.text(0.5,0.5,\"No events with alerts\", ha=\"center\", va=\"center\", transform=ax1.transAxes)\n",
    "    ax1.set_axis_off()\n",
    "\n",
    "# Panel B: PPV vs alerts/100 at decision_h (with budget markers)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ths = np.linspace(0.01, 0.99, 60)\n",
    "d_dec = df_ext.loc[df_ext[\"time_hr\"].eq(DECISION_H)]\n",
    "rates, ppvs = [], []\n",
    "for t in ths:\n",
    "    pred = (d_dec[\"prob\"].values >= t).astype(int)\n",
    "    tp = ((pred==1)&(d_dec[\"label\"].values==1)).sum()\n",
    "    fp = ((pred==1)&(d_dec[\"label\"].values==0)).sum()\n",
    "    ppv = (tp/(tp+fp)) if (tp+fp)>0 else np.nan\n",
    "    ppvs.append(ppv); rates.append(100.0 * pred.mean())\n",
    "ax2.plot(rates, ppvs, marker='o', lw=1)\n",
    "for r in bud_rows:\n",
    "    ax2.scatter([r[\"budget_alerts_per_100\"]], [r[\"PPV_at_decision_h\"]], s=36)\n",
    "    ax2.annotate(f\"{int(r['budget_alerts_per_100'])}/100\",\n",
    "                 (r[\"budget_alerts_per_100\"], r[\"PPV_at_decision_h\"]),\n",
    "                 textcoords=\"offset points\", xytext=(4,4), fontsize=8)\n",
    "ax2.set_xlabel(f\"Alerts per 100 admissions ({DECISION_H}h)\")\n",
    "ax2.set_ylabel(\"PPV\")\n",
    "ax2.set_title(f\"Panel B: Workloadâ€“utility ({DECISION_H}h)\")\n",
    "\n",
    "# Panel C: first-alert timing bars\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "bins = [0, 6, 12, 24, 1e9]; labels = [\"0â€“6h\",\"6â€“12h\",\"12â€“24h\",\">24h\"]\n",
    "if alerts.empty:\n",
    "    counts = pd.Series([0,0,0,0], index=labels)\n",
    "else:\n",
    "    counts = pd.cut(alerts[\"alert_time_hr\"], bins=bins, labels=labels, right=True)\\\n",
    "               .value_counts().reindex(labels).fillna(0)\n",
    "ax3.bar(counts.index, counts.values.astype(float))\n",
    "ax3.set_xlabel(\"First-alert timing\")\n",
    "ax3.set_ylabel(\"Alerts (count)\")\n",
    "ax3.set_title(\"Panel C: First alert timing\")\n",
    "\n",
    "out_fig = FIG_SUPP / \"fig_operational_cgt_silent_trial.png\"\n",
    "fig.savefig(out_fig, dpi=800, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"âœ“ Saved figure:\", out_fig)\n",
    "\n",
    "# ------------- Final diags -------------\n",
    "print(\"\\n[diag] Internal positives @h=\", DECISION_H, \":\", int(d_int['label'].sum()))\n",
    "print(\"[diag] External positives @h=\", DECISION_H, \":\", int(d_ext['label'].sum()))\n",
    "print(\"[diag] Alerts fired (first-hit across horizons):\", 0 if alerts.empty else len(alerts))\n",
    "print(\"[diag] Finite event_time_hr in external:\", int(np.isfinite(df_ext.get('event_time_hr', pd.Series([], dtype=float))).sum()))\n",
    "print(\"[diag] Lead-time n (alerts with events):\", lead_stats[\"n\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, pathlib as pl\n",
    "root = pl.Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\manuscript\\final\")\n",
    "print(pd.read_csv(root/\"tables_main/table_cgt_operating_points.csv\"))\n",
    "print(pd.read_csv(root/\"tables_supp/table_alert_budget_thresholds.csv\"))\n",
    "print(pd.read_csv(root/\"tables_supp/table_operational_summary.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- PATHS ----------------\n",
    "BASE = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES  = BASE / r\"outputs\\external_validation\\results\"\n",
    "MAN  = BASE / r\"manuscript\\final\"\n",
    "TAB_SUPP = MAN / \"tables_supp\"; TAB_SUPP.mkdir(parents=True, exist_ok=True)\n",
    "FIG_SUPP = MAN / \"figures_supp\"; FIG_SUPP.mkdir(parents=True, exist_ok=True)\n",
    "OP_TBL   = MAN / r\"tables_main\\table_cgt_operating_points.csv\"\n",
    "\n",
    "# ---------------- SAFE WRITE ----------------\n",
    "def safe_write_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "        return str(path)\n",
    "    except PermissionError:\n",
    "        alt = path.with_name(path.stem + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + path.suffix)\n",
    "        df.to_csv(alt, index=False)\n",
    "        print(f\"[warn] {path.name} was locked (probably open in Excel). Wrote â†’ {alt}\")\n",
    "        return str(alt)\n",
    "\n",
    "# ---------------- EXPLICIT COLUMN MAP (eICU) ----------------\n",
    "# Your eICU files use: stay id + label + mortality_probability\n",
    "MAP = {\n",
    "    6:  (\"eicu_preds_6h\",  \"stay_id\", \"mortality_probability\", \"label\"),\n",
    "    12: (\"eicu_preds_12h\", \"stay_id\", \"mortality_probability\", \"label\"),\n",
    "    18: (\"eicu_preds_18h\", \"stay_id\", \"mortality_probability\", \"label\"),\n",
    "    24: (\"eicu_preds_24h\", \"stay_id\", \"mortality_probability\", \"label\"),\n",
    "}\n",
    "\n",
    "def read_preds(path_base: Path):\n",
    "    # try CSV then XLSX\n",
    "    if path_base.with_suffix(\".csv\").exists():\n",
    "        return pd.read_csv(path_base.with_suffix(\".csv\"))\n",
    "    elif path_base.with_suffix(\".xlsx\").exists():\n",
    "        return pd.read_excel(path_base.with_suffix(\".xlsx\"))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ---------------- GATHER & LOG ----------------\n",
    "rows = []\n",
    "maplog = []\n",
    "for H, (stem, idc, pc, lc) in MAP.items():\n",
    "    df = read_preds(RES / stem)\n",
    "    if df.empty:\n",
    "        maplog.append({\"horizon_h\": H, \"file\": str((RES / stem)), \"prob\": None, \"label\": None, \"n_valid\": 0, \"note\": \"missing\"})\n",
    "        continue\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    if pc.lower() not in cols or lc.lower() not in cols:\n",
    "        maplog.append({\"horizon_h\": H, \"file\": str((RES / stem)), \"prob\": None, \"label\": None, \"n_valid\": 0, \"note\": \"expected columns not found\"})\n",
    "        continue\n",
    "    prob = pd.to_numeric(df[cols[pc.lower()]], errors=\"coerce\")\n",
    "    lab  = pd.to_numeric(df[cols[lc.lower()]], errors=\"coerce\").fillna(0).astype(int)\n",
    "    msk = prob.notna()\n",
    "    rows.append(pd.DataFrame({\"horizon_h\": H, \"prob\": prob[msk].to_numpy(), \"label\": lab[msk].to_numpy()}))\n",
    "    maplog.append({\"horizon_h\": H, \"file\": str((RES / stem)), \"prob\": cols[pc.lower()], \"label\": cols[lc.lower()], \"n_valid\": int(msk.sum()), \"note\": \"ok\"})\n",
    "\n",
    "diag_path = Path(safe_write_csv(pd.DataFrame(maplog), TAB_SUPP / \"diag_pred_column_mapping.csv\"))\n",
    "print(f\"[info] Column mapping log â†’ {diag_path}\")\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"[RESULT] No usable eICU prediction files (after explicit mapping).\")\n",
    "\n",
    "preds = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# ---------------- Ï„ (from your CGT table) ----------------\n",
    "try:\n",
    "    tau = float(pd.read_csv(OP_TBL)[\"tau\"].iloc[0])\n",
    "    print(f\"[info] Ï„ (CGT): {tau:.3f}\")\n",
    "except Exception:\n",
    "    tau = np.nan\n",
    "    print(\"[warn] Could not read Ï„; NMB curve will still plot but star will be hidden.\")\n",
    "\n",
    "# ---------------- WORKLOADâ€“UTILITY (12h) ----------------\n",
    "p12 = preds[preds[\"horizon_h\"]==12].copy()\n",
    "if p12.empty:\n",
    "    raise SystemExit(\"[RESULT] No 12h predictions found (check eicu_preds_12h.*)\")\n",
    "\n",
    "# make a smooth curve of PPV vs alerts per 100 by sweeping threshold\n",
    "q = np.linspace(0, 1, 21)\n",
    "ths = np.quantile(p12[\"prob\"], q)\n",
    "pts = []\n",
    "for t in ths:\n",
    "    pred = (p12[\"prob\"] >= t).astype(int)\n",
    "    alerts_per_100 = pred.mean() * 100\n",
    "    tp = int(((pred==1) & (p12[\"label\"]==1)).sum())\n",
    "    fp = int(((pred==1) & (p12[\"label\"]==0)).sum())\n",
    "    ppv = tp / max(tp+fp, 1)\n",
    "    pts.append((alerts_per_100, ppv))\n",
    "pts = pd.DataFrame(pts, columns=[\"alerts_per_100\",\"ppv\"]).sort_values(\"alerts_per_100\")\n",
    "\n",
    "# save figure (no display)\n",
    "fig = plt.figure(figsize=(9,5), constrained_layout=True)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(pts[\"alerts_per_100\"], pts[\"ppv\"], \"-o\", markersize=4)\n",
    "ax.set_xlabel(\"Alerts per 100 admissions\"); ax.set_ylabel(\"PPV\"); ax.set_title(\"Workloadâ€“utility (12h)\")\n",
    "fig.savefig(FIG_SUPP / \"fig_workload_utility_12h.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# ---------------- NMB curve (simple, same costs as before) ----------------\n",
    "# WTP=500 per TP; follow-up c=2.50 per minute; mean evaluation time ~8 min -> 20 per alert\n",
    "WTP = 500.0\n",
    "cost_per_alert = 2.50 * 8.0\n",
    "pts2 = []\n",
    "for t in ths:\n",
    "    pred = (p12[\"prob\"] >= t).astype(int)\n",
    "    tp = int(((pred==1) & (p12[\"label\"]==1)).sum())\n",
    "    alerts = pred.sum()\n",
    "    n = len(p12)\n",
    "    nmb = (WTP * tp) - (cost_per_alert * alerts)\n",
    "    pts2.append((pred.mean()*100, nmb * 100.0 / n))  # scale per 100 admissions\n",
    "pts2 = pd.DataFrame(pts2, columns=[\"alerts_per_100\",\"nmb_per_100\"]).sort_values(\"alerts_per_100\")\n",
    "\n",
    "fig = plt.figure(figsize=(9,5), constrained_layout=True)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(pts2[\"alerts_per_100\"], pts2[\"nmb_per_100\"], \"-o\", markersize=4)\n",
    "if np.isfinite(tau):\n",
    "    # mark Ï„\n",
    "    pred_tau = (p12[\"prob\"] >= tau).astype(int)\n",
    "    ax.scatter([pred_tau.mean()*100], [ (WTP*int(((pred_tau==1)&(p12[\"label\"]==1)).sum()) - cost_per_alert*pred_tau.sum()) * 100.0 / len(p12) ],\n",
    "               s=80, marker=\"*\", zorder=3)\n",
    "    ax.text(pred_tau.mean()*100, ax.get_ylim()[1]*0.95, \"Ï„ (CGT)\", ha=\"center\")\n",
    "ax.set_xlabel(\"Alerts per 100 admissions\"); ax.set_ylabel(\"Net monetary benefit (per 100)\")\n",
    "ax.set_title(\"NMB (12h)\")\n",
    "fig.savefig(FIG_SUPP / \"fig_nmb_12h.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"âœ“ Saved:\", FIG_SUPP / \"fig_workload_utility_12h.png\")\n",
    "print(\"âœ“ Saved:\", FIG_SUPP / \"fig_nmb_12h.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, joblib, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "BASE = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES  = BASE / r\"outputs\\results\"\n",
    "FEAT = BASE / r\"outputs\\features\"\n",
    "MODELS = BASE / r\"outputs\\external_validation\\models\"  # where your pklâ€™s live\n",
    "MANUS  = BASE / r\"manuscript\\final\"\n",
    "TAB_MAIN = MANUS / \"tables_main\"\n",
    "TAB_SUPP = MANUS / \"tables_supp\"\n",
    "for d in (RES, FEAT, MODELS, TAB_MAIN, TAB_SUPP): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OP_TBL = TAB_MAIN / \"table_cgt_operating_points.csv\"  # Ï„ per horizon\n",
    "HORIZONS = [6, 18, 24]  # we already have 12h; extend to these\n",
    "\n",
    "# ------------ HELPERS ------------\n",
    "def load_any(p:Path) -> pd.DataFrame:\n",
    "    if not p.exists(): return pd.DataFrame()\n",
    "    try:\n",
    "        if str(p).lower().endswith(\".parquet\"):\n",
    "            return pd.read_parquet(p)\n",
    "        comp = \"gzip\" if str(p).lower().endswith(\".gz\") else None\n",
    "        return pd.read_csv(p, low_memory=False, compression=comp)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def map_label(df:pd.DataFrame):\n",
    "    lo = {c.lower(): c for c in df.columns}\n",
    "    for k in [\"label\",\"y_true\",\"outcome\",\"hospital_expire_flag\",\"death\",\"binary_label\"]:\n",
    "        if k in lo:\n",
    "            return pd.to_numeric(df[lo[k]], errors=\"coerce\").fillna(0).astype(int)\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "\n",
    "def map_id(df:pd.DataFrame):\n",
    "    lo = {c.lower(): c for c in df.columns}\n",
    "    for k in [\"stay_id\",\"icustay_id\",\"hadm_id\",\"encounter_id\",\"id\",\"stay_id_raw\",\"stay\",\"icustay\"]:\n",
    "        if k in lo: return df[lo[k]].astype(str)\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "\n",
    "def read_taus(op_tbl:Path):\n",
    "    op = load_any(op_tbl)\n",
    "    if op.empty: return {}\n",
    "    op.columns = [c.lower() for c in op.columns]\n",
    "    if \"tau\" not in op.columns: return {}\n",
    "    # guess horizon column\n",
    "    hcol = None\n",
    "    for c in [\"window_h\",\"horizon\",\"hours\"]:\n",
    "        if c in op.columns: hcol = c; break\n",
    "    if hcol is None:\n",
    "        # fallback: single Ï„ â†’ assume for 12h only; others missing\n",
    "        return {12: float(op[\"tau\"].iloc[0])}\n",
    "    out = {}\n",
    "    for _,r in op.iterrows():\n",
    "        try: out[int(r[hcol])] = float(r[\"tau\"])\n",
    "        except: pass\n",
    "    return out\n",
    "\n",
    "def build_preds_if_possible(H:int) -> Path|None:\n",
    "    \"\"\"If features+model+imputer exist for H, build predictions CSV and return its path.\"\"\"\n",
    "    feat_path = FEAT / f\"combined_w{H}.parquet\"\n",
    "    imp_path  = MODELS / f\"imputer_w{H}.pkl\"\n",
    "    mdl_path  = MODELS / f\"xgboost_w{H}.pkl\"\n",
    "    out_csv   = RES / f\"mimic_preds_{H}h.csv\"\n",
    "\n",
    "    missing = [p for p in [feat_path, imp_path, mdl_path] if not p.exists()]\n",
    "    if missing:\n",
    "        print(f\"[miss] {H}h missing:\", \", \".join(str(m) for m in missing))\n",
    "        return None\n",
    "\n",
    "    df = load_any(feat_path)\n",
    "    if df.empty:\n",
    "        print(f\"[skip] {H}h features unreadable:\", feat_path)\n",
    "        return None\n",
    "\n",
    "    # identify label and id (if present)\n",
    "    y = map_label(df)\n",
    "    sid = map_id(df)\n",
    "\n",
    "    # pick candidate feature columns:\n",
    "    # try explicit list file first; then imputer feature_names_in_; else XGB num_features\n",
    "    featlist = FEAT / f\"features_w{H}.txt\"\n",
    "    if featlist.exists():\n",
    "        with open(featlist, \"r\", encoding=\"utf-8\") as f:\n",
    "            cols = [ln.strip() for ln in f if ln.strip()]\n",
    "        X = df[cols].copy()\n",
    "    else:\n",
    "        imputer = joblib.load(imp_path)\n",
    "        if hasattr(imputer, \"feature_names_in_\"):\n",
    "            cols = list(imputer.feature_names_in_)\n",
    "            X = df[cols].copy()\n",
    "        else:\n",
    "            # best effort: drop non-numeric and proceed\n",
    "            num = df.select_dtypes(include=[np.number]).copy()\n",
    "            X = num\n",
    "\n",
    "    # impute & predict\n",
    "    imputer = joblib.load(imp_path)\n",
    "    X_imp = imputer.transform(X)\n",
    "\n",
    "    model = joblib.load(mdl_path)  # XGBClassifier\n",
    "    try:\n",
    "        p = model.predict_proba(X_imp)[:,1]\n",
    "    except Exception:\n",
    "        p = model.predict_proba(pd.DataFrame(X_imp))[:,1]\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"stay_id_raw\": sid.values if sid.notna().any() else np.arange(len(p)),\n",
    "        \"prob\": p,\n",
    "        \"label\": y.values if y.notna().any() else np.nan\n",
    "    })\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ“ Built predictions â†’ {out_csv}\")\n",
    "    return out_csv\n",
    "\n",
    "def metrics_at_tau(y, p, tau):\n",
    "    y = pd.to_numeric(y, errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    p = pd.to_numeric(p, errors=\"coerce\").fillna(0).to_numpy()\n",
    "    pred = (p >= float(tau)).astype(int)\n",
    "    tp = int(((pred==1)&(y==1)).sum()); fp = int(((pred==1)&(y==0)).sum())\n",
    "    fn = int(((pred==0)&(y==1)).sum()); tn = int(((pred==0)&(y==0)).sum())\n",
    "    ppv = tp / max(tp+fp, 1)\n",
    "    return dict(N=len(y), alerts=int(pred.sum()), TP=tp, FP=fp, FN=fn, TN=tn, PPV=round(ppv,3))\n",
    "\n",
    "def write_operational_table(H:int, pred_csv:Path, tau:float):\n",
    "    df = load_any(pred_csv)\n",
    "    if df.empty or not {\"prob\",\"label\"}.issubset({c.lower() for c in df.columns}):\n",
    "        # try to map legacy columns\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "        if not {\"prob\",\"label\"}.issubset(df.columns):\n",
    "            print(f\"[skip] {H}h operational: {pred_csv.name} lacks prob/label\")\n",
    "            return\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    core = metrics_at_tau(df[\"label\"], df[\"prob\"], tau)\n",
    "    alerts_per_100 = core[\"alerts\"]/max(core[\"N\"],1)*100.0\n",
    "    wl_min_5, wl_min_10 = alerts_per_100*5.0, alerts_per_100*10.0\n",
    "    staff_hr_5, staff_hr_10 = wl_min_5/60.0, wl_min_10/60.0\n",
    "\n",
    "    row = dict(\n",
    "        horizon_h=H, tau=round(float(tau),3), N=core[\"N\"],\n",
    "        alerts_per_100=round(alerts_per_100,1),\n",
    "        PPV=core[\"PPV\"],\n",
    "        workload_min_per_100_range=f\"{round(wl_min_5,1)}â€“{round(wl_min_10,1)}\",\n",
    "        staff_hours_per_100_range=f\"{round(staff_hr_5,2)}â€“{round(staff_hr_10,2)}\"\n",
    "    )\n",
    "    out = pd.DataFrame([row])\n",
    "    out.to_csv(TAB_SUPP / f\"table_operational_summary_w{H}h.csv\", index=False)\n",
    "    print(f\"âœ“ Wrote w{H}h â†’\", TAB_SUPP / f\"table_operational_summary_w{H}h.csv\")\n",
    "    return row\n",
    "\n",
    "# ------------ RUN ------------\n",
    "taus = read_taus(OP_TBL)\n",
    "if not taus:\n",
    "    print(f\"[warn] No Ï„ per horizon in {OP_TBL}. Add rows for 6/18/24h to compute tables.\")\n",
    "\n",
    "combined_rows = []\n",
    "for H in HORIZONS:\n",
    "    pred_csv = RES / f\"mimic_preds_{H}h.csv\"\n",
    "    if not pred_csv.exists():\n",
    "        print(f\"[info] {H}h predictions missing â€” attempting to buildâ€¦\")\n",
    "        pred_csv = build_preds_if_possible(H)\n",
    "    if pred_csv and pred_csv.exists():\n",
    "        tau = taus.get(H, np.nan)\n",
    "        if np.isfinite(tau):\n",
    "            r = write_operational_table(H, pred_csv, tau)\n",
    "            if r: combined_rows.append(r)\n",
    "        else:\n",
    "            print(f\"[skip] {H}h: Ï„ not found in {OP_TBL}. Add it and rerun.\")\n",
    "    else:\n",
    "        print(f\"[skip] {H}h: predictions still unavailable.\")\n",
    "\n",
    "if combined_rows:\n",
    "    pd.DataFrame(combined_rows).sort_values(\"horizon_h\").to_csv(\n",
    "        TAB_SUPP / \"table_operational_summary_all_windows.csv\", index=False)\n",
    "    print(\"âœ“ Combined â†’\", TAB_SUPP / \"table_operational_summary_all_windows.csv\")\n",
    "else:\n",
    "    print(\"[RESULT] No new operational tables were written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- paths ----------\n",
    "BASE   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES    = BASE / r\"outputs\\results\"\n",
    "MANUS  = BASE / r\"manuscript\\final\"\n",
    "TAB_MAIN = MANUS / \"tables_main\"\n",
    "TAB_SUPP = MANUS / \"tables_supp\"\n",
    "TAB_MAIN.mkdir(parents=True, exist_ok=True); TAB_SUPP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OP_TBL = TAB_MAIN / \"table_cgt_operating_points.csv\"\n",
    "HORIZONS = [6, 18, 24]  # we already had 12h\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def load_preds(p):\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "    lo = {c.lower(): c for c in df.columns}\n",
    "    prob = next((lo[k] for k in [\"prob\",\"prediction\",\"pred\",\"risk\",\"y_hat\",\"probability\",\"score\"] if k in lo), None)\n",
    "    labl = next((lo[k] for k in [\"label\",\"y_true\",\"outcome\",\"hospital_expire_flag\",\"death\",\"binary_label\"] if k in lo), None)\n",
    "    if prob is None:\n",
    "        raise ValueError(f\"{p.name} has no probability column.\")\n",
    "    return pd.to_numeric(df[prob], errors=\"coerce\"), (pd.to_numeric(df[labl], errors=\"coerce\").fillna(0).astype(int) if labl else None)\n",
    "\n",
    "def youden_tau(y, p):\n",
    "    # robust grid on unique probs (cap to 500 thresholds for speed)\n",
    "    s = pd.DataFrame({\"y\": y, \"p\": p}).dropna()\n",
    "    if s.empty: return np.nan\n",
    "    uniq = np.unique(s[\"p\"])\n",
    "    if len(uniq) > 500:\n",
    "        qs = np.linspace(0, 1, 501)\n",
    "        thr = np.quantile(uniq, qs)\n",
    "    else:\n",
    "        thr = uniq\n",
    "    best = (None, -1.0)\n",
    "    for t in thr:\n",
    "        pred = (s[\"p\"] >= t).astype(int)\n",
    "        tp = ((pred==1)&(s[\"y\"]==1)).sum()\n",
    "        fn = ((pred==0)&(s[\"y\"]==1)).sum()\n",
    "        fp = ((pred==1)&(s[\"y\"]==0)).sum()\n",
    "        tn = ((pred==0)&(s[\"y\"]==0)).sum()\n",
    "        sens = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
    "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
    "        j = sens + spec - 1\n",
    "        if j > best[1]:\n",
    "            best = (t, j)\n",
    "    return float(best[0])\n",
    "\n",
    "def write_operational_summary(H, p, y, tau, outdir:Path):\n",
    "    pred = (p >= tau).astype(int)\n",
    "    N = len(p)\n",
    "    alerts = int(pred.sum())\n",
    "    tp = int(((pred==1)&(y==1)).sum())\n",
    "    fp = int(((pred==1)&(y==0)).sum())\n",
    "    alerts_per_100 = alerts / max(N,1) * 100.0\n",
    "    ppv = tp / max(tp+fp, 1)\n",
    "    wl5 = alerts_per_100 * 5.0; wl10 = alerts_per_100 * 10.0\n",
    "    sh5 = wl5/60.0; sh10 = wl10/60.0\n",
    "    row = dict(\n",
    "        horizon_h=H, tau=round(float(tau),3), N=N,\n",
    "        alerts_per_100=round(alerts_per_100,1),\n",
    "        PPV=round(ppv,3),\n",
    "        workload_min_per_100_range=f\"{round(wl5,1)}â€“{round(wl10,1)}\",\n",
    "        staff_hours_per_100_range=f\"{round(sh5,2)}â€“{round(sh10,2)}\"\n",
    "    )\n",
    "    pd.DataFrame([row]).to_csv(outdir / f\"table_operational_summary_w{H}h.csv\", index=False)\n",
    "    print(f\"âœ“ Wrote w{H}h â†’\", outdir / f\"table_operational_summary_w{H}h.csv\")\n",
    "    return row\n",
    "\n",
    "# ---------- 1) compute taus for 6/18/24 ----------\n",
    "new_rows = []\n",
    "for H in HORIZONS:\n",
    "    pred_path = RES / f\"mimic_preds_{H}h.csv\"\n",
    "    if not pred_path.exists():\n",
    "        print(f\"[skip] {H}h: predictions file missing:\", pred_path)\n",
    "        continue\n",
    "    p, y = load_preds(pred_path)\n",
    "    if y is None or y.isna().all():\n",
    "        print(f\"[skip] {H}h: labels missing â€” cannot compute Ï„ via Youden.\")\n",
    "        continue\n",
    "    tau = youden_tau(y, p)\n",
    "    new_rows.append(dict(window_h=H, tau=float(tau)))\n",
    "    print(f\"[tau] {H}h â†’ Ï„={float(tau):.3f}\")\n",
    "\n",
    "# merge into OP_TBL (preserve existing rows incl. 12h)\n",
    "if new_rows:\n",
    "    if OP_TBL.exists():\n",
    "        op = pd.read_csv(OP_TBL)\n",
    "        op.columns = [c.lower() for c in op.columns]\n",
    "        if \"window_h\" not in op.columns and len(op)==1 and \"tau\" in op.columns:\n",
    "            # old single-row file â†’ assume it's 12h\n",
    "            op[\"window_h\"] = [12]\n",
    "        base = op[[\"window_h\",\"tau\"]] if {\"window_h\",\"tau\"}.issubset(op.columns) else pd.DataFrame(columns=[\"window_h\",\"tau\"])\n",
    "    else:\n",
    "        base = pd.DataFrame(columns=[\"window_h\",\"tau\"])\n",
    "    upd = (pd.concat([base, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "             .drop_duplicates(subset=[\"window_h\"], keep=\"last\")\n",
    "             .sort_values(\"window_h\"))\n",
    "    upd.to_csv(OP_TBL, index=False)\n",
    "    print(\"âœ“ Updated Ï„ table â†’\", OP_TBL)\n",
    "else:\n",
    "    print(\"[note] No Ï„ were computed (missing labels or files).\")\n",
    "\n",
    "# ---------- 2) write the operational tables now that Ï„ exist ----------\n",
    "op_map = {}\n",
    "if OP_TBL.exists():\n",
    "    tmp = pd.read_csv(OP_TBL)\n",
    "    tmp.columns = [c.lower() for c in tmp.columns]\n",
    "    if {\"window_h\",\"tau\"}.issubset(tmp.columns):\n",
    "        op_map = {int(r[\"window_h\"]): float(r[\"tau\"]) for _,r in tmp.iterrows()}\n",
    "\n",
    "rows = []\n",
    "for H in HORIZONS:\n",
    "    pred_path = RES / f\"mimic_preds_{H}h.csv\"\n",
    "    if not (pred_path.exists() and H in op_map):\n",
    "        continue\n",
    "    p, y = load_preds(pred_path)\n",
    "    if y is None or y.isna().all():\n",
    "        print(f\"[skip] {H}h: no labels; cannot write operational summary.\")\n",
    "        continue\n",
    "    r = write_operational_summary(H, p, y, op_map[H], TAB_SUPP)\n",
    "    if r: rows.append(r)\n",
    "\n",
    "if rows:\n",
    "    pd.DataFrame(rows).sort_values(\"horizon_h\").to_csv(\n",
    "        TAB_SUPP / \"table_operational_summary_all_windows.csv\", index=False)\n",
    "    print(\"âœ“ Combined â†’\", TAB_SUPP / \"table_operational_summary_all_windows.csv\")\n",
    "else:\n",
    "    print(\"[RESULT] No new operational tables were written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "BASE     = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES      = BASE / r\"outputs\\results\"\n",
    "MANUS    = BASE / r\"manuscript\\final\"\n",
    "TAB_MAIN = MANUS / \"tables_main\"\n",
    "TAB_SUPP = MANUS / \"tables_supp\"\n",
    "TAB_MAIN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OP_TBL = TAB_MAIN / \"table_cgt_operating_points.csv\"\n",
    "OUT_CSV = TAB_MAIN / \"table3_operational_summary.csv\"\n",
    "OUT_XLSX = TAB_MAIN / \"table3_operational_summary.xlsx\"\n",
    "\n",
    "# ---- helpers ----\n",
    "def load_preds(p):\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "    lo = {c.lower(): c for c in df.columns}\n",
    "    prob = next((lo[k] for k in [\"prob\",\"prediction\",\"pred\",\"risk\",\"y_hat\",\"probability\",\"score\"] if k in lo), None)\n",
    "    labl = next((lo[k] for k in [\"label\",\"y_true\",\"outcome\",\"hospital_expire_flag\",\"death\",\"binary_label\"] if k in lo), None)\n",
    "    if prob is None or labl is None:\n",
    "        raise ValueError(f\"{p.name}: need prob + label columns.\")\n",
    "    p = pd.to_numeric(df[prob], errors=\"coerce\")\n",
    "    y = pd.to_numeric(df[labl], errors=\"coerce\").fillna(0).astype(int)\n",
    "    m = pd.DataFrame({\"p\":p,\"y\":y}).dropna()\n",
    "    return m\n",
    "\n",
    "def summarize_operational(m, tau):\n",
    "    N = len(m)\n",
    "    y = m[\"y\"].to_numpy()\n",
    "    p = m[\"p\"].to_numpy()\n",
    "    pred = (p >= tau).astype(int)\n",
    "    alerts = int(pred.sum())\n",
    "    tp = int(((pred==1)&(y==1)).sum()); fp = int(((pred==1)&(y==0)).sum())\n",
    "    prev = y.mean()*100\n",
    "    alerts_per_100 = alerts/max(N,1)*100\n",
    "    ppv = tp/max(tp+fp,1)\n",
    "    tp_per_1000 = tp/max(N,1)*1000\n",
    "    fp_per_1000 = fp/max(N,1)*1000\n",
    "    wl5 = alerts_per_100*5.0; wl10 = alerts_per_100*10.0      # minutes per 100 admissions\n",
    "    sh5 = wl5/60.0; sh10 = wl10/60.0                          # staff-hours per 100\n",
    "    return dict(\n",
    "        N=N,\n",
    "        prevalence_pct=round(prev,1),\n",
    "        tau=round(float(tau),3),\n",
    "        alerts_per_100=round(alerts_per_100,1),\n",
    "        PPV=round(ppv,3),\n",
    "        TP_per_1000=int(round(tp_per_1000)),\n",
    "        FP_per_1000=int(round(fp_per_1000)),\n",
    "        workload_min_per_100=f\"{round(wl5,1)}â€“{round(wl10,1)}\",\n",
    "        staff_hours_per_100=f\"{round(sh5,2)}â€“{round(sh10,2)}\",\n",
    "    )\n",
    "\n",
    "# ---- read Ï„ table (supports old 12h-only format) ----\n",
    "op = pd.read_csv(OP_TBL)\n",
    "op.columns = [c.lower() for c in op.columns]\n",
    "if \"window_h\" not in op.columns and \"tau\" in op.columns and len(op)==1:\n",
    "    op[\"window_h\"] = [12]\n",
    "taus = {int(r[\"window_h\"]): float(r[\"tau\"]) for _,r in op.iterrows()}\n",
    "\n",
    "rows = []\n",
    "for H in [6,12,18,24]:\n",
    "    pred_path = RES / f\"mimic_preds_{H}h.csv\"\n",
    "    if not (pred_path.exists() and H in taus):\n",
    "        # skip gracefully if any piece missing\n",
    "        continue\n",
    "    m = load_preds(pred_path)\n",
    "    s = summarize_operational(m, taus[H])\n",
    "    s[\"horizon_h\"] = H\n",
    "    rows.append(s)\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"[RESULT] No horizons could be summarized â€” check that predictions and Ï„ exist for 6/12/18/24h.\")\n",
    "\n",
    "df = (pd.DataFrame(rows)\n",
    "        .set_index(\"horizon_h\")\n",
    "        .loc[[h for h in [6,12,18,24] if h in [r[\"horizon_h\"] for r in rows]]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"horizon_h\":\"Window (h)\",\n",
    "                         \"prevalence_pct\":\"Prevalence (%)\",\n",
    "                         \"alerts_per_100\":\"Alerts / 100 adm\",\n",
    "                         \"PPV\":\"PPV\",\n",
    "                         \"TP_per_1000\":\"TP / 1000\",\n",
    "                         \"FP_per_1000\":\"FP / 1000\",\n",
    "                         \"workload_min_per_100\":\"Workload (min) / 100\",\n",
    "                         \"staff_hours_per_100\":\"Staff-hours / 100\"}))\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xw:\n",
    "    df.to_excel(xw, index=False, sheet_name=\"Table3\")\n",
    "print(\"âœ“ Main Table 3 â†’\", OUT_CSV)\n",
    "print(\"âœ“ Main Table 3 (xlsx) â†’\", OUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE   = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "RES    = BASE / r\"outputs\\results\"\n",
    "TAB    = BASE / r\"manuscript\\final\\tables_main\"\n",
    "T3     = TAB / \"table3_operational_summary.csv\"\n",
    "\n",
    "# Inputs for 12h CGT\n",
    "TAU_12 = 0.360\n",
    "ALERTS_PER_100 = 61.4\n",
    "PPV = 0.304\n",
    "\n",
    "# Load predictions to get N and prevalence\n",
    "df = pd.read_csv(RES / \"mimic_preds_12h.csv\")\n",
    "lo = {c.lower(): c for c in df.columns}\n",
    "pcol = next(k for k in lo if k in [\"prob\",\"prediction\",\"pred\",\"risk\",\"probability\",\"score\"])\n",
    "ycol = next(k for k in lo if k in [\"label\",\"y_true\",\"outcome\",\"hospital_expire_flag\",\"death\"])\n",
    "m = pd.DataFrame({\"p\":pd.to_numeric(df[lo[pcol]], errors=\"coerce\"),\n",
    "                  \"y\":pd.to_numeric(df[lo[ycol]], errors=\"coerce\").fillna(0).astype(int)}).dropna()\n",
    "\n",
    "N = len(m)\n",
    "prev_pct = round(m[\"y\"].mean()*100,1)\n",
    "\n",
    "tp_per_1000 = int(round(ALERTS_PER_100*PPV*10))\n",
    "fp_per_1000 = int(round(ALERTS_PER_100*(1-PPV)*10))\n",
    "wl_min = f\"{round(ALERTS_PER_100*5,1)}â€“{round(ALERTS_PER_100*10,1)}\"\n",
    "staff = f\"{round(ALERTS_PER_100*5/60,2)}â€“{round(ALERTS_PER_100*10/60,2)}\"\n",
    "\n",
    "row = pd.DataFrame([{\n",
    "    \"Window (h)\":12, \"N\":N, \"Prevalence (%)\":prev_pct, \"tau\":TAU_12,\n",
    "    \"Alerts / 100 adm\":ALERTS_PER_100, \"PPV\":PPV,\n",
    "    \"TP / 1000\":tp_per_1000, \"FP / 1000\":fp_per_1000,\n",
    "    \"Workload (min) / 100\":wl_min, \"Staff-hours / 100\":staff\n",
    "}])\n",
    "\n",
    "# Append/merge (replace any previous 12h row)\n",
    "out = (pd.read_csv(T3) if T3.exists() else pd.DataFrame())\n",
    "out = pd.concat([out[out[\"Window (h)\"]!=12], row], ignore_index=True)\n",
    "out = out.sort_values(\"Window (h)\")\n",
    "out.to_csv(T3, index=False)\n",
    "print(\"âœ“ Updated Table 3 with 12h CGT row â†’\", T3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN RICH-PAGES SERVER on a NEW PORT (keeps your :8000 API intact)\n",
    "import threading, socket, io, base64, re\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib; matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import uvicorn\n",
    "\n",
    "# --- pick a free port (prefer 8010) ---\n",
    "def _free_port(prefer=8010):\n",
    "    def free(p):\n",
    "        s=socket.socket(); \n",
    "        try:\n",
    "            s.bind((\"127.0.0.1\", p)); \n",
    "            s.close(); \n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "    if free(prefer): return prefer\n",
    "    s=socket.socket(); s.bind((\"127.0.0.1\",0)); p=s.getsockname()[1]; s.close(); return p\n",
    "\n",
    "PORT = _free_port(8010)\n",
    "\n",
    "# --- paths matching your code's config ---\n",
    "RESULTS = Path(\"outputs/external_validation/results\")\n",
    "SHAPDIR = Path(\"outputs/external_validation/shap\")\n",
    "OUTROOT = Path(\"outputs\")\n",
    "ALLOWED_WINDOWS = [6,12,18,24]\n",
    "\n",
    "# --- helpers ---\n",
    "def _png64(fig):\n",
    "    buf = io.BytesIO(); fig.tight_layout(); fig.savefig(buf, format=\"png\", dpi=140); plt.close(fig)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
    "\n",
    "def _page(title, body):\n",
    "    return HTMLResponse(f\"\"\"<!doctype html><html><head>\n",
    "    <meta charset=\"utf-8\"><title>{title}</title>\n",
    "    <style>\n",
    "      body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial;margin:24px}}\n",
    "      .grid{{display:grid;grid-template-columns:repeat(auto-fit,minmax(360px,1fr));gap:24px}}\n",
    "      .card{{border:1px solid #e8e8e8;border-radius:12px;padding:12px;background:#fafafa}}\n",
    "      table{{border-collapse:collapse;width:100%}}\n",
    "      th,td{{border:1px solid #e8e8e8;padding:8px;text-align:left}}\n",
    "      th{{background:#f5f5f5}}\n",
    "      img{{max-width:100%;height:auto;border:1px solid #eee;border-radius:8px;background:#fff}}\n",
    "      .muted{{color:#666;font-size:12px}}\n",
    "      .pill{{display:inline-block;padding:2px 8px;border:1px solid #eee;border-radius:999px;background:#fff}}\n",
    "      a{{ text-decoration:none }}\n",
    "    </style></head><body>\n",
    "    <h2>{title}</h2>\n",
    "    {body}\n",
    "    </body></html>\"\"\")\n",
    "\n",
    "def _discover_figs(root: Path, pattern):\n",
    "    out=[]\n",
    "    if root.exists():\n",
    "        for p in root.rglob(\"*\"):\n",
    "            if p.suffix.lower() in {\".png\",\".svg\"} and re.search(pattern, p.name.lower()):\n",
    "                rel = p.relative_to(Path(\"outputs\")).as_posix()\n",
    "                out.append((f\"/static/{rel}\", p.name))\n",
    "    # de-dup\n",
    "    seen=set(); uniq=[]\n",
    "    for src,lab in out:\n",
    "        if src not in seen:\n",
    "            uniq.append((src,lab)); seen.add(src)\n",
    "    return uniq[:16]\n",
    "\n",
    "# --- new app strictly for rich pages ---\n",
    "rich = FastAPI(title=\"Sentinel-ICU Rich Pages\")\n",
    "try:\n",
    "    rich.mount(\"/static\", StaticFiles(directory=\"outputs\"), name=\"static\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "@rich.get(\"/\", response_class=HTMLResponse)\n",
    "def home():\n",
    "    links = \"\"\"\n",
    "    <ul>\n",
    "      <li>Performance: <a href=\"/performance/6/html\">6h</a> â€¢ <a href=\"/performance/12/html\">12h</a> â€¢ <a href=\"/performance/18/html\">18h</a> â€¢ <a href=\"/performance/24/html\">24h</a></li>\n",
    "      <li>Explainability: <a href=\"/explain/6/html\">6h</a> â€¢ <a href=\"/explain/12/html\">12h</a> â€¢ <a href=\"/explain/18/html\">18h</a> â€¢ <a href=\"/explain/24/html\">24h</a></li>\n",
    "      <li><a href=\"/model-card/html\">Model Card</a></li>\n",
    "    </ul>\n",
    "    <p class=\"muted\">Static pre-rendered figures are served from <code>/static</code> mapped to your <code>outputs/</code> folder.</p>\n",
    "    \"\"\"\n",
    "    return _page(\"Rich Pages\", links)\n",
    "\n",
    "@rich.get(\"/performance/{window}/html\", response_class=HTMLResponse)\n",
    "def performance_html(window: int):\n",
    "    if window not in ALLOWED_WINDOWS:\n",
    "        raise HTTPException(status_code=400, detail=f\"Window must be one of {ALLOWED_WINDOWS}\")\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Table from all_metrics.csv for selected window (if present)\n",
    "    am = RESULTS / \"all_metrics.csv\"\n",
    "    if am.exists():\n",
    "        mdf = pd.read_csv(am)\n",
    "        dfw = mdf[mdf[\"window\"] == window]\n",
    "        if not dfw.empty:\n",
    "            parts.append(f'<div class=\"card\"><h3>{window}h metrics (from all_metrics.csv)</h3>{dfw.to_html(index=False)}</div>')\n",
    "\n",
    "            # AUROC bar for this window\n",
    "            fig = plt.figure(figsize=(5,3.2))\n",
    "            plt.bar(dfw[\"dataset\"], dfw[\"auroc\"].astype(float))\n",
    "            plt.ylabel(\"AUROC\"); plt.title(f\"AUROC by dataset â€¢ {window}h\")\n",
    "            parts.append(f'<div class=\"card\"><img src=\"data:image/png;base64,{_png64(fig)}\"></div>')\n",
    "\n",
    "            # Across-window line chart\n",
    "            fig = plt.figure(figsize=(6,3.2))\n",
    "            for ds in sorted(mdf[\"dataset\"].unique()):\n",
    "                dd = mdf[mdf[\"dataset\"] == ds].sort_values(\"window\")\n",
    "                if not dd.empty:\n",
    "                    plt.plot(dd[\"window\"], dd[\"auroc\"], marker=\"o\", label=ds)\n",
    "            plt.xlabel(\"Window (h)\"); plt.ylabel(\"AUROC\"); plt.title(\"AUROC across windows\"); plt.legend()\n",
    "            parts.append(f'<div class=\"card\"><img src=\"data:image/png;base64,{_png64(fig)}\"></div>')\n",
    "    else:\n",
    "        parts.append('<div class=\"card\">No all_metrics.csv found under outputs/external_validation/results</div>')\n",
    "\n",
    "    # Pre-rendered images auto-discovered\n",
    "    figs = _discover_figs(OUTROOT, pattern=rf\"(perf|performance|auroc|auc|calib).*({window}h|w{window}|_{window}h)?\")\n",
    "    if figs:\n",
    "        gallery = \"\".join([f'<div class=\"card\"><div class=\"pill\">{lab}</div><img src=\"{src}\"></div>' for src,lab in figs])\n",
    "        parts.append(f'<div class=\"grid\">{gallery}</div>')\n",
    "\n",
    "    if not parts:\n",
    "        parts.append('<div class=\"card\">No performance data.</div>')\n",
    "\n",
    "    return _page(f\"Performance â€” {window}h\", '<div class=\"grid\">' + \"\".join(parts) + \"</div>\")\n",
    "\n",
    "@rich.get(\"/explain/{window}/html\", response_class=HTMLResponse)\n",
    "def explain_html(window: int, top_n: int = 15):\n",
    "    fi = SHAPDIR / f\"feature_importance_w{window}.csv\"\n",
    "    if not fi.exists():\n",
    "        alts = list(SHAPDIR.glob(f\"*{window}*.csv\"))\n",
    "        if alts: fi = alts[0]\n",
    "    if not fi.exists():\n",
    "        return _page(\"Explainability\", \"<div class='card'>No feature importance file found.</div>\")\n",
    "\n",
    "    df = pd.read_csv(fi)\n",
    "    cand = [c for c in [\"importance\",\"mean_abs_shap\",\"gain\",\"weight\",\"importance_normalized\"] if c in df.columns] \\\n",
    "           or [c for c in df.columns if c.lower()!=\"feature\" and np.issubdtype(df[c].dtype, np.number)]\n",
    "    imp = cand[0]\n",
    "    if \"feature\" not in df.columns:\n",
    "        feat_col = next((c for c in df.columns if \"feat\" in c.lower() or \"name\" in c.lower()), df.columns[0])\n",
    "        df = df.rename(columns={feat_col:\"feature\"})\n",
    "    df2 = df[[\"feature\", imp]].rename(columns={imp:\"importance\"}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    table_html = df2.head(top_n).to_html(index=False)\n",
    "\n",
    "    # barh\n",
    "    top = df2.head(top_n).iloc[::-1]\n",
    "    fig = plt.figure(figsize=(6, max(3.0, 0.35*len(top))))\n",
    "    plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "    plt.xlabel(\"Importance\"); plt.title(f\"Top {top_n} features â€¢ {window}h\")\n",
    "    bar64 = _png64(fig)\n",
    "\n",
    "    # gallery\n",
    "    figs = _discover_figs(OUTROOT, pattern=rf\"(shap|explain|waterfall).*({window}h|w{window}|_{window}h)?\")\n",
    "    gallery = \"\".join([f'<div class=\"card\"><div class=\"pill\">{lab}</div><img src=\"{src}\"></div>' for src,lab in figs]) or \"<div class='muted'>No extra SHAP/Waterfall images.</div>\"\n",
    "\n",
    "    body = f'''\n",
    "      <div class=\"grid\">\n",
    "        <div class=\"card\"><h3>Top features</h3>{table_html}</div>\n",
    "        <div class=\"card\"><img src=\"data:image/png;base64,{bar64}\"></div>\n",
    "      </div>\n",
    "      <h3>Rendered images from outputs/</h3>\n",
    "      <div class=\"grid\">{gallery}</div>\n",
    "    '''\n",
    "    return _page(f\"Global importance â€” {window}h\", body)\n",
    "\n",
    "@rich.get(\"/model-card/html\", response_class=HTMLResponse)\n",
    "def model_card_html():\n",
    "    # thresholds + optional best external from results\n",
    "    parts = []\n",
    "    # thresholds\n",
    "    thr_rows=[]\n",
    "    for w in ALLOWED_WINDOWS:\n",
    "        p = Path(\"outputs/external_validation/models\")/f\"threshold_w{w}.csv\"\n",
    "        if p.exists():\n",
    "            try:\n",
    "                t = float(pd.read_csv(p)[\"threshold\"].iloc[0]); thr_rows.append((w,t))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if thr_rows:\n",
    "        tdf = pd.DataFrame(thr_rows, columns=[\"Window (h)\",\"Threshold\"])\n",
    "        parts.append(f'<div class=\"card\"><h3>Thresholds</h3>{tdf.to_html(index=False)}</div>')\n",
    "\n",
    "    # best external from all_metrics.csv (if present)\n",
    "    am = RESULTS / \"all_metrics.csv\"\n",
    "    if am.exists():\n",
    "        mdf = pd.read_csv(am)\n",
    "        best = (mdf[mdf[\"dataset\"]==\"eICU\"].sort_values([\"auroc\",\"window\"], ascending=[False,True]).head(1))\n",
    "        if not best.empty:\n",
    "            parts.append(f'<div class=\"card\"><h3>Best external (from all_metrics.csv)</h3>{best.to_html(index=False)}</div>')\n",
    "\n",
    "    # pre-rendered model-card/calibration images\n",
    "    figs = _discover_figs(OUTROOT, pattern=r\"(model[-_ ]?card|calib|calibration)\")\n",
    "    if figs:\n",
    "        gallery = \"\".join([f'<div class=\"card\"><div class=\"pill\">{lab}</div><img src=\"{src}\"></div>' for src,lab in figs])\n",
    "        parts.append(f'<div class=\"grid\">{gallery}</div>')\n",
    "\n",
    "    if not parts:\n",
    "        parts.append('<div class=\"card\">No model-card assets found.</div>')\n",
    "\n",
    "    return _page(\"Model Card\", '<div class=\"grid\">' + \"\".join(parts) + \"</div>\")\n",
    "\n",
    "# --- launch in background ---\n",
    "def _serve():\n",
    "    uvicorn.run(rich, host=\"127.0.0.1\", port=PORT, log_level=\"info\")\n",
    "\n",
    "t = threading.Thread(target=_serve, name=f\"rich-{PORT}\", daemon=True)\n",
    "t.start()\n",
    "\n",
    "print(f\"âœ… Rich pages server running at http://127.0.0.1:{PORT}\")\n",
    "print(\"Open:\")\n",
    "print(f\"  - http://127.0.0.1:{PORT}/performance/12/html\")\n",
    "print(f\"  - http://127.0.0.1:{PORT}/explain/12/html\")\n",
    "print(f\"  - http://127.0.0.1:{PORT}/model-card/html\")\n",
    "print(f\"  - http://127.0.0.1:{PORT}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build manuscript bundle: README + ZIP (one cell)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "MANUS = ROOT / \"manuscript\"\n",
    "TABS  = MANUS / \"tables\"\n",
    "FIGS  = MANUS / \"figures\"\n",
    "\n",
    "MANUS.mkdir(exist_ok=True, parents=True)\n",
    "TABS.mkdir(exist_ok=True, parents=True)\n",
    "FIGS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ---- Load artifacts we already created ----\n",
    "all_metrics_csv = TABS / \"tableA_performance_by_window.csv\"\n",
    "thr_csv         = TABS / \"tableB_thresholds.csv\"\n",
    "sg_csv          = TABS / \"tableC_subgroups_12h.csv\"  # may not exist\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(p) if p.exists() else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "dfA = _read_csv_safe(all_metrics_csv)\n",
    "dfB = _read_csv_safe(thr_csv)\n",
    "dfC = _read_csv_safe(sg_csv)\n",
    "\n",
    "# ---- Build Markdown sections ----\n",
    "lines = []\n",
    "lines.append(\"# Sentinel-ICU Results Bundle\\n\")\n",
    "lines.append(f\"_Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\\n\")\n",
    "\n",
    "# A) Performance by window\n",
    "lines.append(\"## A. Discrimination & Calibration (by window)\")\n",
    "if dfA is not None and not dfA.empty:\n",
    "    try:\n",
    "        mdA = dfA.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdA = dfA.to_csv(index=False)\n",
    "    lines.append(mdA + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table A not found.*\\n\")\n",
    "\n",
    "# B) Decision thresholds\n",
    "lines.append(\"## B. Learned decision thresholds\")\n",
    "if dfB is not None and not dfB.empty:\n",
    "    try:\n",
    "        mdB = dfB.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdB = dfB.to_csv(index=False)\n",
    "    lines.append(mdB + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table B not found.*\\n\")\n",
    "\n",
    "# C) Subgroup snapshot (12h)\n",
    "lines.append(\"## C. Subgroup snapshot (12 h window)\")\n",
    "if dfC is not None and not dfC.empty:\n",
    "    try:\n",
    "        mdC = dfC.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        mdC = dfC.to_csv(index=False)\n",
    "    lines.append(mdC + \"\\n\")\n",
    "else:\n",
    "    lines.append(\"*Table C not found (optional).* \\n\")\n",
    "\n",
    "# D) Figures (paths only; images are already saved)\n",
    "fig1 = FIGS / \"fig1_auroc_by_window.png\"\n",
    "fig2 = FIGS / \"fig2_shap_top10_12h.png\"\n",
    "lines.append(\"## D. Figures\\n\")\n",
    "if fig1.exists():\n",
    "    lines.append(f\"- **Figure 1**: AUROC by window â†’ `manuscript/figures/{fig1.name}`\")\n",
    "if fig2.exists():\n",
    "    lines.append(f\"- **Figure 2**: SHAP top-10 (12 h) â†’ `manuscript/figures/{fig2.name}`\")\n",
    "if not (fig1.exists() or fig2.exists()):\n",
    "    lines.append(\"- (No figures found yet)\")\n",
    "\n",
    "lines.append(\"\")\n",
    "\n",
    "# E) How clinicians can test the model (local)\n",
    "lines.append(\"## E. How clinicians can test the model (local)\")\n",
    "lines.append(\"- **API docs**:  `http://127.0.0.1:8000/docs`\")\n",
    "lines.append(\"- **Health**:    `http://127.0.0.1:8000/health`\")\n",
    "lines.append(\"- **Streamlit UI**:  `http://127.0.0.1:8501`  *(adjust port if you launched on 8502/8503)*\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Example (12 h prediction)**\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8000/predict \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"features\\\": {\\\"age_years\\\": 70, \\\"hr_mean\\\": 95, \\\"hr_max\\\": 105, \\\"hr_min\\\": 85,')\n",
    "lines.append('                 \\\"sbp_mean\\\": 110, \\\"sbp_min\\\": 90, \\\"rr_mean\\\": 22, \\\"rr_max\\\": 26,')\n",
    "lines.append('                 \\\"spo2_mean\\\": 92, \\\"spo2_min\\\": 88, \\\"lactate_max\\\": 3.2,')\n",
    "lines.append('                 \\\"creat_max\\\": 1.8, \\\"troponin_max\\\": 8.5}, \\\"window\\\": 12 }\\'')\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Save README\n",
    "readme_path = MANUS / \"README_results.md\"\n",
    "readme_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"âœ“ Wrote:\", readme_path)\n",
    "\n",
    "# ---- Zip the manuscript folder ----\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "zip_base = ROOT / f\"manuscript_bundle_{ts}\"\n",
    "zip_path = shutil.make_archive(str(zip_base), \"zip\", MANUS)\n",
    "print(\"âœ“ Zipped bundle:\", zip_path)\n",
    "\n",
    "# Show a tiny summary\n",
    "print(\"\\nBundle contents preview:\")\n",
    "for p in sorted(MANUS.rglob(\"*\")):\n",
    "    if p.is_file():\n",
    "        rel = p.relative_to(ROOT)\n",
    "        print(\" -\", rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmb_app_simple.py\n",
    "# FastAPI app for Net Monetary Benefit (NMB) â€” clean UI (no charts), robust error messages, auto-compute on load.\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Tuple, Literal\n",
    "import os, csv, json\n",
    "\n",
    "app = FastAPI(title=\"Sentinel-ICU: NMB Forecaster\", version=\"2.5\")\n",
    "\n",
    "# ==== Manuscript defaults (12 h) ====\n",
    "GUARDRAIL_ALERTS = 61.4\n",
    "GUARDRAIL_PPV    = 0.304\n",
    "DEFAULT_MIN_PER_ALERT = 8.0\n",
    "DEFAULT_TP_VALUE      = 500.0\n",
    "DEFAULT_COST_PER_MIN  = 2.50\n",
    "\n",
    "# Prefilled anchors for PPV vs alerts/100 (edit these or click \"Use my manuscript anchors\")\n",
    "DEFAULT_ANCHORS = [\n",
    "  [15.0,  0.505],\n",
    "  [30.0,  0.420],\n",
    "  [45.0,  0.350],\n",
    "  [61.4,  0.304],   # guardrail\n",
    "  [80.0,  0.280],\n",
    "  [100.0, 0.260],\n",
    "  [120.0, 0.240],\n",
    "]\n",
    "\n",
    "# ==== Core math ====\n",
    "def nmb_from_alerts(alerts_per_100: float, ppv: float,\n",
    "                    tp_value: float, minutes_per_alert: float, cost_per_minute: float):\n",
    "    tp = alerts_per_100 * ppv\n",
    "    minutes = alerts_per_100 * minutes_per_alert\n",
    "    benefit = tp_value * tp\n",
    "    review_cost = cost_per_minute * minutes\n",
    "    nmb = benefit - review_cost\n",
    "    return {\n",
    "        \"alerts_per_100\": round(alerts_per_100, 2),\n",
    "        \"ppv\": round(ppv, 3),\n",
    "        \"tp\": round(tp, 3),\n",
    "        \"review_minutes_total\": round(minutes, 1),\n",
    "        \"review_hours_total\": round(minutes/60.0, 2),\n",
    "        \"benefit_total\": round(benefit, 2),\n",
    "        \"review_cost_total\": round(review_cost, 2),\n",
    "        \"nmb_total\": round(nmb, 2),\n",
    "    }\n",
    "\n",
    "def confusion_from_metrics(sens: float, spec: float, prev: float, admissions: int = 100):\n",
    "    positives = prev * admissions\n",
    "    negatives = (1 - prev) * admissions\n",
    "    tp = sens * positives\n",
    "    tn = spec * negatives\n",
    "    fp = negatives - tn\n",
    "    alerts = tp + fp\n",
    "    ppv = (tp / alerts) if alerts > 0 else 0.0\n",
    "    return {\"alerts_per_100\": alerts*(100.0/admissions), \"ppv\": ppv}\n",
    "\n",
    "def lin_interp(x: float, anchors: List[Tuple[float,float]]) -> float:\n",
    "    pts = sorted(anchors, key=lambda t: t[0])\n",
    "    if not pts: return 0.0\n",
    "    if x <= pts[0][0]: return pts[0][1]\n",
    "    if x >= pts[-1][0]: return pts[-1][1]\n",
    "    for (x0,y0),(x1,y1) in zip(pts[:-1], pts[1:]):\n",
    "        if x0 <= x <= x1:\n",
    "            t = 0.0 if x1==x0 else (x-x0)/(x1-x0)\n",
    "            return y0 + t*(y1-y0)\n",
    "    return pts[-1][1]\n",
    "\n",
    "# ==== Schemas ====\n",
    "class NMBRequest(BaseModel):\n",
    "    mode: Literal[\"direct\",\"metrics\"] = \"direct\"\n",
    "    admissions: int = Field(100, gt=0)\n",
    "    # direct\n",
    "    alerts_per_100: Optional[float] = Field(None, ge=0.0)\n",
    "    ppv: Optional[float]           = Field(None, ge=0.0, le=1.0)\n",
    "    # metrics\n",
    "    sensitivity: Optional[float]   = Field(None, ge=0.0, le=1.0)\n",
    "    specificity: Optional[float]   = Field(None, ge=0.0, le=1.0)\n",
    "    prevalence: Optional[float]    = Field(None, ge=0.0, le=1.0)\n",
    "    # economics\n",
    "    review_minutes_per_alert: float = Field(DEFAULT_MIN_PER_ALERT, ge=0.0)\n",
    "    tp_value: float                 = Field(DEFAULT_TP_VALUE, ge=0.0)\n",
    "    cost_per_minute: float          = Field(DEFAULT_COST_PER_MIN, ge=0.0)\n",
    "\n",
    "class NMBCurveRequest(BaseModel):\n",
    "    start_alerts: float = Field(10.0, ge=0.0)\n",
    "    end_alerts: float   = Field(120.0, ge=0.0)\n",
    "    step: float         = Field(2.0, gt=0.0)\n",
    "    ppv: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    ppv_anchors: Optional[List[Tuple[float,float]]] = None\n",
    "    review_minutes_per_alert: float = Field(DEFAULT_MIN_PER_ALERT, ge=0.0)\n",
    "    tp_value: float = Field(DEFAULT_TP_VALUE, ge=0.0)\n",
    "    cost_per_minute: float = Field(DEFAULT_COST_PER_MIN, ge=0.0)\n",
    "\n",
    "# ==== Optional: auto-load anchors from your bundle ====\n",
    "BUNDLE_ROOT = r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\\ICU_v1_12h_bundle_20250818_0128\"\n",
    "FILENAMES = {\n",
    "    \"tableF_alerts_vs_threshold_12h.csv\",\n",
    "    \"tableE_operating_point_12h.csv\",\n",
    "}\n",
    "RELATIVE_DEFAULTS = [\n",
    "    \"manuscript/tables/tableF_alerts_vs_threshold_12h.csv\",\n",
    "    \"manuscript/tables/tableE_operating_point_12h.csv\",\n",
    "    \"manuscript/final/tables_main/table3_operational_summary.csv\",\n",
    "]\n",
    "X_NAMES = {\"alerts_per_100\",\"alerts_per100\",\"alerts_per_100_adm\",\"alerts\"}\n",
    "Y_NAMES = {\"ppv\",\"PPV\",\"precision\"}\n",
    "\n",
    "def _load_anchors_csv(path: str):\n",
    "    if not os.path.exists(path): return []\n",
    "    out = []\n",
    "    with open(path, newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        rd = csv.DictReader(f)\n",
    "        for r in rd:\n",
    "            x = next((r.get(k) for k in X_NAMES if r.get(k)), None)\n",
    "            y = next((r.get(k) for k in Y_NAMES if r.get(k)), None)\n",
    "            if x is None or y is None: continue\n",
    "            try:\n",
    "                xf, yf = float(str(x).strip()), float(str(y).strip())\n",
    "                if xf >= 0 and 0 <= yf <= 1: out.append((xf, yf))\n",
    "            except:\n",
    "                pass\n",
    "    uniq = {}\n",
    "    for a,p in out: uniq[float(a)] = float(p)\n",
    "    return sorted(uniq.items(), key=lambda t: t[0])\n",
    "\n",
    "def _find_candidate_paths():\n",
    "    candidates = []\n",
    "    if BUNDLE_ROOT and os.path.isdir(BUNDLE_ROOT):\n",
    "        for root, dirs, files in os.walk(BUNDLE_ROOT):\n",
    "            for fn in files:\n",
    "                if fn in FILENAMES:\n",
    "                    candidates.append(os.path.join(root, fn))\n",
    "    for rel in RELATIVE_DEFAULTS:\n",
    "        if os.path.exists(rel):\n",
    "            candidates.append(rel)\n",
    "    for fn in FILENAMES:\n",
    "        if os.path.exists(fn):\n",
    "            candidates.append(fn)\n",
    "    seen=set(); uniq=[]\n",
    "    for p in candidates:\n",
    "        if p not in seen:\n",
    "            seen.add(p); uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "def find_repo_anchors():\n",
    "    best_path, best = None, []\n",
    "    for p in _find_candidate_paths():\n",
    "        try:\n",
    "            anchors = _load_anchors_csv(p)\n",
    "            if len(anchors) > len(best):\n",
    "                best_path, best = p, anchors\n",
    "        except:\n",
    "            pass\n",
    "    return best_path, best\n",
    "\n",
    "# ==== API ====\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"ok\": True, \"version\": app.version}\n",
    "\n",
    "@app.get(\"/defaults\")\n",
    "def defaults():\n",
    "    return {\n",
    "        \"guardrail_alerts\": GUARDRAIL_ALERTS,\n",
    "        \"guardrail_ppv\": GUARDRAIL_PPV,\n",
    "        \"minutes_per_alert\": DEFAULT_MIN_PER_ALERT,\n",
    "        \"tp_value\": DEFAULT_TP_VALUE,\n",
    "        \"cost_per_min\": DEFAULT_COST_PER_MIN,\n",
    "        \"default_anchors\": DEFAULT_ANCHORS,\n",
    "    }\n",
    "\n",
    "@app.get(\"/diagnostics\")\n",
    "def diagnostics():\n",
    "    src, anchors = find_repo_anchors()\n",
    "    return {\n",
    "        \"bundle_root\": BUNDLE_ROOT,\n",
    "        \"searched_files\": _find_candidate_paths(),\n",
    "        \"repo_source\": src,\n",
    "        \"anchors_preview\": anchors[:12],\n",
    "        \"defaults_shown_in_ui\": DEFAULT_ANCHORS\n",
    "    }\n",
    "\n",
    "@app.post(\"/economic/nmb\")\n",
    "def economic_nmb(req: NMBRequest):\n",
    "    if req.mode == \"metrics\":\n",
    "        if None in (req.sensitivity, req.specificity, req.prevalence):\n",
    "            raise HTTPException(400, \"Provide sensitivity, specificity, prevalence for metrics mode\")\n",
    "        cm = confusion_from_metrics(req.sensitivity, req.specificity, req.prevalence, req.admissions)\n",
    "        alerts, ppv = cm[\"alerts_per_100\"], cm[\"ppv\"]\n",
    "    else:\n",
    "        if None in (req.alerts_per_100, req.ppv):\n",
    "            raise HTTPException(400, \"Provide alerts_per_100 and ppv for direct mode\")\n",
    "        alerts, ppv = float(req.alerts_per_100), float(req.ppv)\n",
    "    return JSONResponse(nmb_from_alerts(alerts, ppv, req.tp_value, req.review_minutes_per_alert, req.cost_per_minute))\n",
    "\n",
    "@app.post(\"/economic/nmb-forecast\")\n",
    "def economic_nmb_forecast(req: NMBCurveRequest):\n",
    "    if req.end_alerts < req.start_alerts:\n",
    "        raise HTTPException(400, \"'end_alerts' must be >= 'start_alerts'\")\n",
    "    anchors = [(float(a), float(p)) for a,p in (req.ppv_anchors or [])]\n",
    "    pts = []\n",
    "    a = req.start_alerts\n",
    "    while a <= req.end_alerts + 1e-9:\n",
    "        ppv = lin_interp(a, anchors) if anchors else float(req.ppv or 0.0)\n",
    "        pts.append(nmb_from_alerts(a, ppv, req.tp_value, req.review_minutes_per_alert, req.cost_per_minute))\n",
    "        a += req.step\n",
    "    best_i = max(range(len(pts)), key=lambda i: pts[i][\"nmb_total\"]) if pts else 0\n",
    "    return JSONResponse({\n",
    "        \"points\": pts,\n",
    "        \"best_index\": best_i,\n",
    "        \"best_alerts_per_100\": pts[best_i][\"alerts_per_100\"] if pts else 0,\n",
    "        \"best_nmb_total\": pts[best_i][\"nmb_total\"] if pts else 0\n",
    "    })\n",
    "\n",
    "# ==== UI (KPI tiles; error messages; auto-compute on load) ====\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "def ui():\n",
    "    tpl = \"\"\"<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/>\n",
    "<title>NMB Forecaster</title>\n",
    "<style>\n",
    "  :root{{\n",
    "    --bg:#f7fafc;--text:#0f172a;--muted:#475569;\n",
    "    --card:#ffffff;--line:#e6eef4;--ok:#16a34a;--bad:#ef4444;--brand1:#045d8c;--brand2:#0aa2c0;\n",
    "  }}\n",
    "  body{{max-width:1100px;margin:24px auto;font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;background:var(--bg);color:var(--text);}}\n",
    "  header{{background:linear-gradient(90deg,var(--brand1),var(--brand2));color:#fff;padding:14px 16px;border-radius:14px;margin-bottom:14px;}}\n",
    "  .card{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:14px;margin:14px 0;box-shadow:0 3px 8px rgba(0,0,0,.05);}}\n",
    "  .grid{{display:grid;grid-template-columns:1fr 1fr;gap:12px;}}\n",
    "  input,select,textarea{{width:100%;padding:8px;border:1px solid #cbd5e1;border-radius:8px;background:#fff;}}\n",
    "  .row{{display:flex;gap:8px;align-items:center;flex-wrap:wrap}}\n",
    "  .btn{{display:inline-block;padding:8px 12px;border-radius:8px;background:var(--brand1);color:#fff;border:none;cursor:pointer;}}\n",
    "  .btn:disabled{{opacity:.6;cursor:not-allowed;}}\n",
    "  .nav a{{color:#fff;text-decoration:none;margin-left:12px;}}\n",
    "  .note{{color:var(--muted);font-size:.9rem;margin-left:6px;}}\n",
    "  .err{{color:var(--bad);margin-top:8px}}\n",
    "  .kpis{{display:grid;grid-template-columns:repeat(3,1fr);gap:12px;margin-top:12px}}\n",
    "  .kpi{{border:1px solid var(--line);border-radius:12px;padding:12px;background:#fff}}\n",
    "  .kpi .label{{font-size:.9rem;color:var(--muted)}}\n",
    "  .kpi .val{{font-size:1.6rem;font-weight:700;margin-top:6px}}\n",
    "  .ok .val{{color:var(--ok)}} .bad .val{{color:var(--bad)}}\n",
    "  @media (max-width:840px){{ .grid{{grid-template-columns:1fr}} .kpis{{grid-template-columns:1fr}} }}\n",
    "  details summary{{cursor:pointer;color:var(--muted)}}\n",
    "  textarea{{font-family:ui-monospace,Consolas,Menlo,monospace}}\n",
    "  table{{width:100%;border-collapse:collapse;margin-top:10px;border:1px solid var(--line);background:#fff;border-radius:8px;overflow:hidden}}\n",
    "  th,td{{padding:8px 10px;border-bottom:1px solid var(--line);font-variant-numeric:tabular-nums;}}\n",
    "  th{{text-align:left;background:#f1f5f9}}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<header>\n",
    "  <div style=\"display:flex;justify-content:space-between;align-items:center;\">\n",
    "    <div><strong>Net Monetary Benefit (NMB) Forecaster</strong><div style=\"font-size:.9rem\">v2.5 Â· manuscript defaults</div></div>\n",
    "    <nav class=\"nav\"><a href=\"health\">health</a><a href=\"diagnostics\">diagnostics</a></nav>\n",
    "  </div>\n",
    "</header>\n",
    "\n",
    "<section class=\"card\">\n",
    "  <h3>Single point</h3>\n",
    "  <div class=\"grid\">\n",
    "    <div>\n",
    "      <label>Mode\n",
    "        <select id=\"mode\">\n",
    "          <option value=\"direct\" selected>direct (alerts + PPV)</option>\n",
    "          <option value=\"metrics\">metrics (sensitivity, specificity, prevalence)</option>\n",
    "        </select>\n",
    "      </label>\n",
    "      <div id=\"directFields\">\n",
    "        <label>Alerts per 100<input id=\"alerts\" type=\"number\" step=\"0.1\" value=\"__GA__\"></label>\n",
    "        <label>PPV<input id=\"ppv\" type=\"number\" step=\"0.001\" min=\"0\" max=\"1\" value=\"__GP__\"></label>\n",
    "      </div>\n",
    "      <div id=\"metricFields\" style=\"display:none\">\n",
    "        <div class=\"row\">\n",
    "          <label style=\"flex:1\">Sensitivity<input id=\"sens\" type=\"number\" step=\"0.001\" min=\"0\" max=\"1\" value=\"0.618\"></label>\n",
    "          <label style=\"flex:1\">Specificity<input id=\"spec\" type=\"number\" step=\"0.001\" min=\"0\" max=\"1\" value=\"0.915\"></label>\n",
    "          <label style=\"flex:1\">Prevalence<input id=\"prev\" type=\"number\" step=\"0.001\" min=\"0\" max=\"1\" value=\"0.155\"></label>\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <label>Minutes per alert<input id=\"mins\" type=\"number\" step=\"0.5\" value=\"__MIN__\"></label>\n",
    "      <label>Value per TP ($)<input id=\"valtp\" type=\"number\" step=\"10\" value=\"__TPV__\"></label>\n",
    "      <label>Reviewer cost ($/min)<input id=\"cpm\" type=\"number\" step=\"0.1\" value=\"__CPM__\"></label>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"row\" style=\"margin-top:8px\">\n",
    "    <button id=\"btnPoint\" class=\"btn\" type=\"button\">Compute</button>\n",
    "    <span id=\"endpt1\" class=\"note\"></span>\n",
    "  </div>\n",
    "  <div id=\"err1\" class=\"err\" style=\"display:none\"></div>\n",
    "\n",
    "  <div class=\"kpis\" id=\"kpis\" style=\"display:none\">\n",
    "    <div class=\"kpi\"><div class=\"label\">Alerts / 100</div><div class=\"val\" id=\"kpiAlerts\">â€“</div></div>\n",
    "    <div class=\"kpi\"><div class=\"label\">PPV</div><div class=\"val\" id=\"kpiPPV\">â€“</div></div>\n",
    "    <div class=\"kpi\"><div class=\"label\">True positives</div><div class=\"val\" id=\"kpiTP\">â€“</div></div>\n",
    "    <div class=\"kpi\"><div class=\"label\">Review hours / 100</div><div class=\"val\" id=\"kpiHours\">â€“</div></div>\n",
    "    <div class=\"kpi bad\"><div class=\"label\">Review cost</div><div class=\"val\" id=\"kpiCost\">â€“</div></div>\n",
    "    <div class=\"kpi ok\"><div class=\"label\">Net monetary benefit</div><div class=\"val\" id=\"kpiNMB\">â€“</div></div>\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<section class=\"card\">\n",
    "  <h3>Forecast (optional)</h3>\n",
    "  <div class=\"grid\">\n",
    "    <div>\n",
    "      <label>Alerts range\n",
    "        <div class=\"row\">\n",
    "          <input id=\"startA\" type=\"number\" step=\"1\" value=\"10\"><span>to</span>\n",
    "          <input id=\"endA\" type=\"number\" step=\"1\" value=\"120\"><span>step</span>\n",
    "          <input id=\"stepA\" type=\"number\" step=\"1\" value=\"2\">\n",
    "        </div>\n",
    "      </label>\n",
    "      <details open>\n",
    "        <summary>PPV options</summary>\n",
    "        <label>Constant PPV<input id=\"ppvConst\" type=\"number\" step=\"0.001\" min=\"0\" max=\"1\" value=\"0.30\"></label>\n",
    "        <div class=\"row\"><button id=\"btnUseRepo\" class=\"btn\" type=\"button\">Use anchors from repo</button><span class=\"note\">(auto from CSV if available)</span></div>\n",
    "        <div class=\"row\"><button id=\"btnUseMine\" class=\"btn\" type=\"button\">Use my manuscript anchors</button></div>\n",
    "        <textarea id=\"anchors\" rows=\"6\">__ANCHORS__</textarea>\n",
    "      </details>\n",
    "    </div>\n",
    "    <div>\n",
    "      <label>Minutes per alert<input id=\"minsC\" type=\"number\" step=\"0.5\" value=\"__MIN__\"></label>\n",
    "      <label>Value per TP ($)<input id=\"valtpC\" type=\"number\" step=\"10\" value=\"__TPV__\"></label>\n",
    "      <label>Reviewer cost ($/min)<input id=\"cpmC\" type=\"number\" step=\"0.1\" value=\"__CPM__\"></label>\n",
    "    </div>\n",
    "  </div>\n",
    "  <button id=\"btnCurve\" class=\"btn\" type=\"button\" style=\"margin-top:8px\">Run forecast</button>\n",
    "  <span id=\"endpt2\" class=\"note\"></span>\n",
    "  <div id=\"err2\" class=\"err\" style=\"display:none\"></div>\n",
    "\n",
    "  <div id=\"forecastWrap\" style=\"display:none;margin-top:10px\">\n",
    "    <div class=\"kpis\">\n",
    "      <div class=\"kpi ok\"><div class=\"label\">Best NMB (per 100)</div><div class=\"val\" id=\"bestNMB\">â€“</div></div>\n",
    "      <div class=\"kpi\"><div class=\"label\">Best alerts / 100</div><div class=\"val\" id=\"bestA\">â€“</div></div>\n",
    "      <div class=\"kpi\"><div class=\"label\">Staff-hours at best</div><div class=\"val\" id=\"bestHours\">â€“</div></div>\n",
    "    </div>\n",
    "    <table id=\"tbl\" style=\"display:none\">\n",
    "      <thead><tr><th>Alerts/100</th><th>PPV</th><th>TP</th><th>Hours</th><th>Benefit ($)</th><th>Review cost ($)</th><th>NMB ($)</th></tr></thead>\n",
    "      <tbody id=\"rows\"></tbody>\n",
    "    </table>\n",
    "    <div class=\"note\">Showing first 12 points. Use anchors textbox to adjust PPV vs alerts.</div>\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<script>\n",
    "(function(){{\n",
    "  const path = location.pathname, m=path.match(/^(.*\\/proxy\\/\\\\d+\\\\/)/);\n",
    "  const base = m ? m[1] : (path.endsWith('/')?path: path.substring(0, path.lastIndexOf('/')+1));\n",
    "  window.API = (rel)=> new URL(rel.replace(/^\\\\/+/,''), location.origin+base).toString();\n",
    "}})();\n",
    "\n",
    "const $ = (id)=>document.getElementById(id);\n",
    "const money = (x)=> new Intl.NumberFormat('en-US',{style:'currency',currency:'USD',maximumFractionDigits:0}).format(x);\n",
    "const fixed  = (x,n)=> (Math.round(x*Math.pow(10,n))/Math.pow(10,n)).toFixed(n);\n",
    "\n",
    "function show(elId,txt){ const el=$(elId); el.textContent=txt||\"\"; el.style.display = txt ? \"\" : \"none\"; }\n",
    "function disable(id, yes){ const b=$(id); if (b) b.disabled = !!yes; }\n",
    "\n",
    "function showKPIs(d){\n",
    "  $(\"kpis\").style.display = \"\";\n",
    "  $(\"kpiAlerts\").textContent = fixed(d.alerts_per_100,1);\n",
    "  $(\"kpiPPV\").textContent    = fixed(d.ppv,3);\n",
    "  $(\"kpiTP\").textContent     = fixed(d.tp,3);\n",
    "  $(\"kpiHours\").textContent  = fixed(d.review_hours_total,2);\n",
    "  $(\"kpiCost\").textContent   = money(d.review_cost_total);\n",
    "  $(\"kpiNMB\").textContent    = money(d.nmb_total);\n",
    "}\n",
    "\n",
    "$(\"mode\").addEventListener(\"change\", () => {\n",
    "  const m = $(\"mode\").value;\n",
    "  $(\"directFields\").style.display  = (m===\"direct\")  ? \"\" : \"none\";\n",
    "  $(\"metricFields\").style.display  = (m===\"metrics\") ? \"\" : \"none\";\n",
    "});\n",
    "\n",
    "$(\"btnUseMine\").addEventListener(\"click\", ()=>{ $(\"anchors\").value = JSON.stringify(__ANCHORS_OBJ__); });\n",
    "\n",
    "$(\"btnUseRepo\").addEventListener(\"click\", async ()=>{\n",
    "  try{\n",
    "    const info = await (await fetch(API(\"diagnostics\"))).json();\n",
    "    if (info.anchors_preview && info.anchors_preview.length){\n",
    "      $(\"anchors\").value = JSON.stringify(info.anchors_preview);\n",
    "    } else {\n",
    "      alert(\"No repo anchors found under searched paths.\");\n",
    "    }\n",
    "  }catch(e){ alert(\"Anchor load failed: \"+e); }\n",
    "});\n",
    "\n",
    "$(\"btnPoint\").addEventListener(\"click\", async () => {\n",
    "  show(\"err1\",\"\");\n",
    "  disable(\"btnPoint\", true);\n",
    "  try{\n",
    "    const payload = {\n",
    "      mode: $(\"mode\").value,\n",
    "      review_minutes_per_alert: parseFloat($(\"mins\").value),\n",
    "      tp_value: parseFloat($(\"valtp\").value),\n",
    "      cost_per_minute: parseFloat($(\"cpm\").value)\n",
    "    };\n",
    "    if (payload.mode === \"direct\"){\n",
    "      payload.alerts_per_100 = parseFloat($(\"alerts\").value);\n",
    "      payload.ppv = parseFloat($(\"ppv\").value);\n",
    "    } else {\n",
    "      payload.sensitivity = parseFloat($(\"sens\").value);\n",
    "      payload.specificity = parseFloat($(\"spec\").value);\n",
    "      payload.prevalence  = parseFloat($(\"prev\").value);\n",
    "    }\n",
    "    const url = API(\"economic/nmb\"); $(\"endpt1\").textContent = \"POST \" + url;\n",
    "    const r = await fetch(url, {method:\"POST\", headers:{\"Content-Type\":\"application/json\"}, body:JSON.stringify(payload)});\n",
    "    if (!r.ok){ show(\"err1\", await r.text()); return; }\n",
    "    const d = await r.json();\n",
    "    showKPIs(d);\n",
    "  }catch(e){ show(\"err1\", String(e)); }\n",
    "  finally{ disable(\"btnPoint\", false); }\n",
    "});\n",
    "\n",
    "$(\"btnCurve\").addEventListener(\"click\", async () => {\n",
    "  show(\"err2\",\"\"); $(\"forecastWrap\").style.display=\"none\";\n",
    "  disable(\"btnCurve\", true);\n",
    "  try{\n",
    "    const payload = {\n",
    "      start_alerts: parseFloat($(\"startA\").value),\n",
    "      end_alerts: parseFloat($(\"endA\").value),\n",
    "      step: parseFloat($(\"stepA\").value),\n",
    "      review_minutes_per_alert: parseFloat($(\"minsC\").value),\n",
    "      tp_value: parseFloat($(\"valtpC\").value),\n",
    "      cost_per_minute: parseFloat($(\"cpmC\").value)\n",
    "    };\n",
    "    const txt = $(\"anchors\").value.trim();\n",
    "    if (txt){ try{ payload.ppv_anchors = JSON.parse(txt); } catch{ show(\"err2\",\"Anchors must be valid JSON like [[15,0.5],[60,0.32]]\"); return; } }\n",
    "    else { payload.ppv = parseFloat($(\"ppvConst\").value); }\n",
    "\n",
    "    const url = API(\"economic/nmb-forecast\"); $(\"endpt2\").textContent = \"POST \" + url;\n",
    "    const r = await fetch(url, {method:\"POST\", headers:{\"Content-Type\":\"application/json\"}, body:JSON.stringify(payload)});\n",
    "    if (!r.ok){ show(\"err2\", await r.text()); return; }\n",
    "    const d = await r.json();\n",
    "\n",
    "    $(\"forecastWrap\").style.display = \"\";\n",
    "    const best = d.points[d.best_index] || null;\n",
    "    if (best){\n",
    "      $(\"bestNMB\").textContent   = money(best.nmb_total);\n",
    "      $(\"bestA\").textContent     = fixed(best.alerts_per_100,1);\n",
    "      $(\"bestHours\").textContent = fixed(best.review_hours_total,2);\n",
    "    } else {\n",
    "      $(\"bestNMB\").textContent = $(\"bestA\").textContent = $(\"bestHours\").textContent = \"â€“\";\n",
    "    }\n",
    "\n",
    "    const tbody = $(\"rows\");\n",
    "    tbody.innerHTML = \"\";\n",
    "    const showRows = (d.points||[]).slice(0,12);\n",
    "    if (showRows.length){\n",
    "      $(\"tbl\").style.display = \"\";\n",
    "      for (const p of showRows){\n",
    "        const tr = document.createElement(\"tr\");\n",
    "        tr.innerHTML = `\n",
    "          <td>${fixed(p.alerts_per_100,1)}</td>\n",
    "          <td>${fixed(p.ppv,3)}</td>\n",
    "          <td>${fixed(p.tp,3)}</td>\n",
    "          <td>${fixed(p.review_hours_total,2)}</td>\n",
    "          <td>${money(p.benefit_total)}</td>\n",
    "          <td>${money(p.review_cost_total)}</td>\n",
    "          <td><strong>${money(p.nmb_total)}</strong></td>`;\n",
    "        tbody.appendChild(tr);\n",
    "      }\n",
    "    } else {\n",
    "      $(\"tbl\").style.display = \"none\";\n",
    "    }\n",
    "  }catch(e){ show(\"err2\", String(e)); }\n",
    "  finally{ disable(\"btnCurve\", false); }\n",
    "});\n",
    "\n",
    "/* Auto-compute once on load so you immediately see output */\n",
    "window.addEventListener(\"load\", () => { $(\"btnPoint\").click(); });\n",
    "</script>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "    html = (\n",
    "        tpl\n",
    "        .replace(\"__GA__\", str(GUARDRAIL_ALERTS))\n",
    "        .replace(\"__GP__\", str(GUARDRAIL_PPV))\n",
    "        .replace(\"__MIN__\", str(DEFAULT_MIN_PER_ALERT))\n",
    "        .replace(\"__TPV__\", str(DEFAULT_TP_VALUE))\n",
    "        .replace(\"__CPM__\", str(DEFAULT_COST_PER_MIN))\n",
    "        .replace(\"__ANCHORS__\", json.dumps(DEFAULT_ANCHORS))\n",
    "        .replace(\"__ANCHORS_OBJ__\", json.dumps(DEFAULT_ANCHORS))\n",
    "    )\n",
    "    return HTMLResponse(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Where to save ----\n",
    "preferred = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")\n",
    "OUTDIR = preferred if preferred.exists() else Path.cwd()\n",
    "readme_path = OUTDIR / \"README_NMB_v23.md\"\n",
    "req_path    = OUTDIR / \"requirements_nmb.txt\"\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Sentinel-ICU â€” Net Monetary Benefit (NMB) Forecaster (v2.3)\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"_This README was generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## What is this?\")\n",
    "lines.append(\"A lightweight FastAPI service that converts **alerts per 100 admissions** and **PPV** (or **sensitivity/specificity/prevalence**) into:\")\n",
    "lines.append(\"- **Staff time** (minutes / hours per 100 admissions)\")\n",
    "lines.append(\"- **Net monetary benefit (NMB)**, using manuscript base-case assumptions with one-way parameter changes\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"It exposes two JSON endpoints and a simple browser UI. It mirrors the economic framing in the manuscript so clinicians can explore alert budgets and trade-offs.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Endpoints\")\n",
    "lines.append(\"- `GET  /health` â€” service status (returns version `2.3` when youâ€™re on this build)\")\n",
    "lines.append(\"- `GET  /diagnostics` â€” shows whether PPVâ€“alerts anchors were discovered in your repo/bundle\")\n",
    "lines.append(\"- `POST /economic/nmb` â€” single-point NMB (direct: alerts+PPV, or metrics: sens/spec/prev)\")\n",
    "lines.append(\"- `POST /economic/nmb-forecast` â€” NMB across an alert-budget range; accepts optional **PPV anchors**\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Quick start (Windows)\")\n",
    "lines.append(\"Install the minimal dependencies once:\")\n",
    "lines.append(\"```powershell\")\n",
    "lines.append(\"pip install -r requirements_nmb.txt\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Run the app (choose the line that matches your file name):\")\n",
    "lines.append(\"```powershell\")\n",
    "lines.append(\"# If your file is nmb_app.py\")\n",
    "lines.append(\"python -m uvicorn nmb_app:app --host 0.0.0.0 --port 8899\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"# If your file is nmb_app_simple.py\")\n",
    "lines.append(\"python -m uvicorn nmb_app_simple:app --host 0.0.0.0 --port 8899\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"# Or run from anywhere pointing to your app dir explicitly\")\n",
    "lines.append('python -m uvicorn --app-dir \"D:\\\\ä¸ªäººæ–‡ä»¶å¤¹\\\\Sanwal\\\\ICU\" nmb_app:app --host 0.0.0.0 --port 8899')\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Open:\")\n",
    "lines.append(\"- App UI: `http://127.0.0.1:8899/`\")\n",
    "lines.append(\"- Docs: `http://127.0.0.1:8899/docs`\")\n",
    "lines.append(\"- Health: `http://127.0.0.1:8899/health`  â†’ should return `{\\\"ok\\\": true, \\\"version\\\": \\\"2.3\\\"}`\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"> **Tip:** If a port is already in use, pick a new one (e.g., `--port 8897`).\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Request examples\")\n",
    "lines.append(\"### Single-point NMB (direct mode)\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8899/economic/nmb \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"mode\\\": \\\"direct\\\",')\n",
    "lines.append('    \\\"alerts_per_100\\\": 61.4,')\n",
    "lines.append('    \\\"ppv\\\": 0.304,')\n",
    "lines.append('    \\\"review_minutes_per_alert\\\": 8,')\n",
    "lines.append('    \\\"tp_value\\\": 500,')\n",
    "lines.append('    \\\"cost_per_minute\\\": 2.5')\n",
    "lines.append(\"  }'\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Single-point NMB (metrics mode)\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8899/economic/nmb \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"mode\\\": \\\"metrics\\\",')\n",
    "lines.append('    \\\"sensitivity\\\": 0.618,')\n",
    "lines.append('    \\\"specificity\\\": 0.915,')\n",
    "lines.append('    \\\"prevalence\\\": 0.155,')\n",
    "lines.append('    \\\"review_minutes_per_alert\\\": 8,')\n",
    "lines.append('    \\\"tp_value\\\": 500,')\n",
    "lines.append('    \\\"cost_per_minute\\\": 2.5')\n",
    "lines.append(\"  }'\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Forecast across alert budgets\")\n",
    "lines.append(\"Use a constant PPV **or** provide anchors. Anchors are pairs `[[alerts_per_100, ppv], ...]`.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**With anchors (recommended):**\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8899/economic/nmb-forecast \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"start_alerts\\\": 10,')\n",
    "lines.append('    \\\"end_alerts\\\": 120,')\n",
    "lines.append('    \\\"step\\\": 2,')\n",
    "lines.append('    \\\"ppv_anchors\\\": [[15,0.505],[30,0.420],[45,0.350],[61.4,0.304],[80,0.280],[100,0.260],[120,0.240]],')\n",
    "lines.append('    \\\"review_minutes_per_alert\\\": 8,')\n",
    "lines.append('    \\\"tp_value\\\": 500,')\n",
    "lines.append('    \\\"cost_per_minute\\\": 2.5')\n",
    "lines.append(\"  }'\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**With constant PPV:**\")\n",
    "lines.append(\"```bash\")\n",
    "lines.append(\"curl -X POST http://127.0.0.1:8899/economic/nmb-forecast \\\\\")\n",
    "lines.append(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "lines.append(\"  -d '{\")\n",
    "lines.append('    \\\"start_alerts\\\": 10,')\n",
    "lines.append('    \\\"end_alerts\\\": 120,')\n",
    "lines.append('    \\\"step\\\": 2,')\n",
    "lines.append('    \\\"ppv\\\": 0.30,')\n",
    "lines.append('    \\\"review_minutes_per_alert\\\": 8,')\n",
    "lines.append('    \\\"tp_value\\\": 500,')\n",
    "lines.append('    \\\"cost_per_minute\\\": 2.5')\n",
    "lines.append(\"  }'\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Defaults (manuscript 12-h window)\")\n",
    "lines.append(\"- Guardrail operating point: **61.4 alerts / 100**, **PPV 0.304**\")\n",
    "lines.append(\"- Base-case economics: **8 min / alert**, **$500 / TP**, **$2.50 / minute**\")\n",
    "lines.append(\"- Recommended PPVâ€“alerts anchors (editable):\")\n",
    "lines.append(\"```json\")\n",
    "lines.append('[[15,0.505],[30,0.420],[45,0.350],[61.4,0.304],[80,0.280],[100,0.260],[120,0.240]]')\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Diagnostics & troubleshooting\")\n",
    "lines.append(\"- If the browser shows an old build, youâ€™re likely hitting a **different port/process**. Check `/health` (should say `2.3`).\")\n",
    "lines.append(\"- If a port is busy: use a new `--port`. To find the old PID on Windows:\")\n",
    "lines.append(\"```powershell\")\n",
    "lines.append(\"Get-NetTCPConnection -LocalPort 8899 | Select-Object -Unique OwningProcess\")\n",
    "lines.append(\"Stop-Process -Id <PID> -Force   # replace <PID> with the number shown (no angle brackets)\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"- If the UI buttons do nothing, open **F12 â†’ Console** for errors; the app also exposes `/docs` for manual POST tests.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## How this maps to the manuscript\")\n",
    "lines.append(\"- Converts alert budgets and PPV (or sensitivity/specificity/prevalence) into review workload and **NMB**, using the same base-case values described in Methods.\")\n",
    "lines.append(\"- Lets users sweep alert budgets to find the **NMB-maximising** region, matching our decision-analytic framing.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Minimal requirements\")\n",
    "lines.append(\"```txt\")\n",
    "lines.append(\"fastapi>=0.110\")\n",
    "lines.append(\"uvicorn[standard]>=0.24\")\n",
    "lines.append(\"pydantic>=2.6\")\n",
    "lines.append(\"```\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## License\")\n",
    "lines.append(\"MIT (or your preferred open-source license).\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Citation (example)\")\n",
    "lines.append(\"Sanwal S, et al. **Sentinel-ICU NMB Forecaster (v2.3)**. 2025. Available at: repository URL; archived at DOI if applicable.\")\n",
    "lines.append(\"\")\n",
    "readme_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"âœ“ Wrote:\", readme_path)\n",
    "\n",
    "req_path.write_text(\"fastapi>=0.110\\nuvicorn[standard]>=0.24\\npydantic>=2.6\\n\", encoding=\"utf-8\")\n",
    "print(\"âœ“ Wrote:\", req_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil, json, sys\n",
    "\n",
    "# ---- Configure paths ----\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")  # change if needed\n",
    "OUTDIR = ROOT / \"manuscript\" / \"final\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "TOOL_DIR = OUTDIR / \"Supplementary_Tool_S1_NMB_Forecaster_v23\"\n",
    "TOOL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Pick the app file youâ€™re using for v2.3 ----\n",
    "app_simple = ROOT / \"nmb_app_simple.py\"\n",
    "app_full   = ROOT / \"nmb_app.py\"\n",
    "if app_simple.exists():\n",
    "    shutil.copy2(app_simple, TOOL_DIR / app_simple.name)\n",
    "elif app_full.exists():\n",
    "    shutil.copy2(app_full, TOOL_DIR / app_full.name)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find nmb_app_simple.py or nmb_app.py under ROOT\")\n",
    "\n",
    "# ---- Copy README and requirements (create minimal ones if missing) ----\n",
    "readme = ROOT / \"README_NMB_v23.md\"\n",
    "reqs   = ROOT / \"requirements_nmb.txt\"\n",
    "if readme.exists():\n",
    "    shutil.copy2(readme, TOOL_DIR / \"README.md\")\n",
    "else:\n",
    "    (TOOL_DIR / \"README.md\").write_text(\n",
    "        \"# NMB Forecaster (v2.3)\\n\\nRun:\\n\\n```\\npython -m uvicorn nmb_app_simple:app --host 0.0.0.0 --port 8899\\n```\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "if reqs.exists():\n",
    "    shutil.copy2(reqs, TOOL_DIR / \"requirements.txt\")\n",
    "else:\n",
    "    (TOOL_DIR / \"requirements.txt\").write_text(\n",
    "        \"fastapi>=0.110\\nuvicorn[standard]>=0.24\\npydantic>=2.6\\n\", encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# ---- Add manuscript PPVâ€“alerts anchors (editable) ----\n",
    "anchors = [[15,0.505],[30,0.420],[45,0.350],[61.4,0.304],[80,0.280],[100,0.260],[120,0.240]]\n",
    "(TOOL_DIR / \"ppv_anchors_12h.json\").write_text(json.dumps(anchors, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ---- (Optional) license file ----\n",
    "(TOOL_DIR / \"LICENSE\").write_text(\"MIT\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# ---- Zip it ----\n",
    "zip_path = shutil.make_archive(str(TOOL_DIR), \"zip\", TOOL_DIR)\n",
    "print(\"âœ“ Supplement created:\", zip_path)\n",
    "print(\"Folder:\", TOOL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Supplementary Tool S2: Mortality Inference API (ZIP) â€” brace-safe (no f-strings)\n",
    "from pathlib import Path\n",
    "import shutil, json, textwrap\n",
    "from datetime import date\n",
    "from string import Template\n",
    "\n",
    "# --- CONFIG ---\n",
    "ROOT = Path(r\"D:\\ä¸ªäººæ–‡ä»¶å¤¹\\Sanwal\\ICU\")   # <-- change if your project root is elsewhere\n",
    "OUTDIR = ROOT / \"manuscript\" / \"final\"\n",
    "TOOL_NAME = \"Supplementary_Tool_S2_SentinelICU_Mortality_API_v1\"\n",
    "TOOL_DIR = OUTDIR / TOOL_NAME\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Try to locate your mortality API file; OR set API_FILE explicitly to Path(\".../your_api.py\")\n",
    "CANDIDATES = [\n",
    "    \"icu_api.py\", \"mortality_api.py\", \"api.py\", \"app.py\", \"main.py\", \"server.py\",\n",
    "    \"app_server.py\", \"fastapi_app.py\"\n",
    "]\n",
    "API_FILE = None\n",
    "for nm in CANDIDATES:\n",
    "    p = ROOT / nm\n",
    "    if p.exists():\n",
    "        API_FILE = p\n",
    "        break\n",
    "\n",
    "# If not auto-found, set it here and re-run:\n",
    "# API_FILE = ROOT / \"YOUR_FILE_NAME.py\"\n",
    "\n",
    "if API_FILE is None or not API_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Mortality API file not found under ROOT.\\n\"\n",
    "        f\"Tried: {', '.join(CANDIDATES)} in {ROOT}\\n\"\n",
    "        \"Fix: rename your API to one of the above OR set API_FILE = ROOT / 'your_api.py' above and re-run.\"\n",
    "    )\n",
    "\n",
    "# --- clean target folder ---\n",
    "if TOOL_DIR.exists():\n",
    "    shutil.rmtree(TOOL_DIR)\n",
    "TOOL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- copy API code ---\n",
    "shutil.copy2(API_FILE, TOOL_DIR / API_FILE.name)\n",
    "\n",
    "# --- README.md (Template to avoid brace issues) ---\n",
    "readme_tpl = Template(textwrap.dedent(\"\"\"\\\n",
    "    # Sentinel-ICU â€” Mortality Inference API (v1.0)\n",
    "\n",
    "    This package contains the FastAPI application used to serve the ICU mortality model\n",
    "    (as referenced in the manuscript). It is a **standalone** supplementary tool intended\n",
    "    for reproducibility and demonstration. No patient data are included.\n",
    "\n",
    "    ## Quick start (Windows)\n",
    "\n",
    "    ```powershell\n",
    "    pip install -r requirements_mortality.txt\n",
    "    python -m uvicorn $module:app --host 0.0.0.0 --port 8901\n",
    "    ```\n",
    "\n",
    "    Open:\n",
    "    - UI docs:   http://127.0.0.1:8901/docs\n",
    "    - Health:    http://127.0.0.1:8901/health  (if implemented)\n",
    "\n",
    "    > If 8901 is busy, change the port (e.g., 8902).\n",
    "\n",
    "    ## Example request (12h window)\n",
    "\n",
    "    ```bash\n",
    "    curl -X POST http://127.0.0.1:8901/predict \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\n",
    "        \"features\": {\n",
    "          \"age_years\": 70,\n",
    "          \"hr_mean\": 95,  \"hr_max\": 105, \"hr_min\": 85,\n",
    "          \"sbp_mean\": 110, \"sbp_min\": 90,\n",
    "          \"rr_mean\": 22,  \"rr_max\": 26,\n",
    "          \"spo2_mean\": 92, \"spo2_min\": 88,\n",
    "          \"lactate_max\": 3.2,\n",
    "          \"creat_max\": 1.8,\n",
    "          \"troponin_max\": 8.5\n",
    "        },\n",
    "        \"window\": 12\n",
    "      }'\n",
    "    ```\n",
    "\n",
    "    ## Notes\n",
    "    - Ships **without clinical data** and **without large model weights**.\n",
    "    - If model weights/paths are required, document how to obtain them (or fallbacks).\n",
    "    - Code should run with the included `requirements_mortality.txt`.\n",
    "\n",
    "    ## License\n",
    "    MIT\n",
    "\"\"\"))\n",
    "readme_txt = readme_tpl.substitute(module=API_FILE.stem)\n",
    "(TOOL_DIR / \"README.md\").write_text(readme_txt, encoding=\"utf-8\")\n",
    "\n",
    "# --- requirements (adjust if your API needs more) ---\n",
    "req_txt = textwrap.dedent(\"\"\"\\\n",
    "fastapi>=0.110\n",
    "uvicorn[standard]>=0.24\n",
    "pydantic>=2.6\n",
    "numpy>=1.23\n",
    "pandas>=1.5\n",
    "scikit-learn>=1.1\n",
    "\"\"\")\n",
    "(TOOL_DIR / \"requirements_mortality.txt\").write_text(req_txt, encoding=\"utf-8\")\n",
    "\n",
    "# --- example request JSON ---\n",
    "example_req = {\n",
    "    \"features\": {\n",
    "        \"age_years\": 70,\n",
    "        \"hr_mean\": 95,  \"hr_max\": 105, \"hr_min\": 85,\n",
    "        \"sbp_mean\": 110, \"sbp_min\": 90,\n",
    "        \"rr_mean\": 22,  \"rr_max\": 26,\n",
    "        \"spo2_mean\": 92, \"spo2_min\": 88,\n",
    "        \"lactate_max\": 3.2,\n",
    "        \"creat_max\": 1.8,\n",
    "        \"troponin_max\": 8.5\n",
    "    },\n",
    "    \"window\": 12\n",
    "}\n",
    "(TOOL_DIR / \"example_request_12h.json\").write_text(json.dumps(example_req, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- version, license, citation ---\n",
    "(TOOL_DIR / \"VERSION.txt\").write_text(\"v1.0\\n\", encoding=\"utf-8\")\n",
    "(TOOL_DIR / \"LICENSE\").write_text(\"MIT\\n\", encoding=\"utf-8\")\n",
    "\n",
    "citation_cff = textwrap.dedent(f\"\"\"\\\n",
    "cff-version: 1.2.0\n",
    "title: \"Sentinel-ICU Mortality Inference API\"\n",
    "version: \"v1.0\"\n",
    "message: \"If you use this software, please cite it as below.\"\n",
    "authors:\n",
    "  - family-names: \"Sanwal\"\n",
    "    given-names: \"\"\n",
    "date-released: \"{date.today().isoformat()}\"\n",
    "\"\"\")\n",
    "(TOOL_DIR / \"CITATION.cff\").write_text(citation_cff, encoding=\"utf-8\")\n",
    "\n",
    "# --- zip it ---\n",
    "zip_path = shutil.make_archive(str(TOOL_DIR), \"zip\", TOOL_DIR)\n",
    "print(\"âœ“ Supplement S2 created:\", zip_path)\n",
    "print(\"Folder:\", TOOL_DIR)\n",
    "print(\"Included API file:\", API_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook/Spyder-safe server footer (replace any old uvicorn.run block) ---\n",
    "import socket, threading, os\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "# keep a global reference to the server so we can stop it\n",
    "_SERVER_REF = {\"server\": None}\n",
    "\n",
    "def _port_free(p: int) -> bool:\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.settimeout(0.2)\n",
    "    try:\n",
    "        return s.connect_ex((\"127.0.0.1\", p)) != 0\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "def _pick_port(start=8000, end=8010) -> int:\n",
    "    for p in range(start, end + 1):\n",
    "        if _port_free(p):\n",
    "            return p\n",
    "    return 8000  # fallback\n",
    "\n",
    "def serve_background(app: FastAPI, host=\"0.0.0.0\", port: int | None = None, log_level=\"warning\"):\n",
    "    from uvicorn import Config, Server\n",
    "    if port is None:\n",
    "        port = _pick_port()\n",
    "    cfg = Config(app=app, host=host, port=port, log_level=log_level)\n",
    "    server = Server(cfg)\n",
    "    _SERVER_REF[\"server\"] = server\n",
    "\n",
    "    def _run():\n",
    "        # runs its own asyncio loop in THIS thread; safe inside notebooks\n",
    "        server.run()\n",
    "\n",
    "    t = threading.Thread(target=_run, daemon=True)\n",
    "    t.start()\n",
    "    return t, port\n",
    "\n",
    "# optional: clean shutdown\n",
    "@app.post(\"/shutdown\")\n",
    "def _shutdown():\n",
    "    s = _SERVER_REF.get(\"server\")\n",
    "    if s is not None:\n",
    "        s.should_exit = True\n",
    "        return JSONResponse({\"ok\": True, \"msg\": \"server stopping\"})\n",
    "    return JSONResponse({\"ok\": False, \"msg\": \"server not running\"})\n",
    "\n",
    "def main():\n",
    "    # pick free port & start in background\n",
    "    port_env = os.getenv(\"PORT\")\n",
    "    port = int(port_env) if port_env else _pick_port(8000, 8010)\n",
    "    _, port = serve_background(app, port=port, log_level=\"warning\")\n",
    "\n",
    "    print(f\"=== HRC Overlay server (threaded) ===\")\n",
    "    print(f\"Overlay         : http://127.0.0.1:{port}/overlay\")\n",
    "    print(f\"OBS (transparent): http://127.0.0.1:{port}/overlay_transparent\")\n",
    "    print(f\"Health          : http://127.0.0.1:{port}/health\")\n",
    "    print(f\"Shutdown (POST) : http://127.0.0.1:{port}/shutdown\")\n",
    "    # returns immediately; server keeps running in background\n",
    "\n",
    "# IMPORTANT: call main() even in notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.5.1 (histopathology)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}